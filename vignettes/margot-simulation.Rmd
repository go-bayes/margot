---
title: "Simulating Longitudinal Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simulating Longitudinal Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  fig.width  = 7,
  fig.height = 5
)
```

```{r setup}
library(margot)   # core simulation + estimation tools
library(dplyr)    # tidy manipulation
library(tidyr)    # data reshaping
library(ggplot2)  # graphics for examples
```

## 1. Why simulate longitudinal data?

Synthetic panel data with *known* data-generating mechanisms are invaluable for

- **Teaching**: illustrate causal ideas where the ground truth is visible;
- **Software testing**: ensure estimators recover the truth under controlled violations;
- **Power & design**: gauge sample-size or wave-number requirements;
- **Method development**: iterate quickly without large, confidential datasets.

`margot_simulate()` is our workhorse; the sections below unpack its interface
and demonstrate common scenarios.

## 2. Anatomy of `margot_simulate()`

The simulator draws baseline covariates **B**, time-varying covariates **L**,
exposures **A**, and lead outcomes **Y** across a user-specified number of
waves.  *Figure 1* gives a high-level schematic.

```{r fig.cap = "Figure 1: data-generating process underlying `margot_simulate()`. Solid arrows are structural paths; dashed arrows indicate optional feedback loops and censoring."}
# build a toy DAG with DiagrammeR for illustration only (not evaluated when package is built)
if (requireNamespace("DiagrammeR", quietly = TRUE)) {
  DiagrammeR::grViz(
    "digraph {graph [rankdir = LR, bgcolor = none]
      node [shape = box, fontname = Helvetica]
      subgraph cluster0 {label = \"Wave 0\"; style = dashed; B; t0_L;}
      subgraph cluster1 {label = \"Wave 1\"; style = dashed; t1_A; t1_L; t1_Y;}
      subgraph cluster2 {label = \"Wave 2\"; style = dashed; t2_A; t2_L; t2_Y;}
      B -> t1_L -> t1_A -> t1_Y
      B -> t1_A
      t1_Y -> t2_A [style = dashed]  // optional y_feedback
      t1_A -> t2_L [style = dashed]  // covar_feedback
      t1_A -> censor1 [style = dashed]
      censor1 [shape = circle, label = \"C\"]
    }"
  )
}
```

Key arguments (non-exhaustive):

| Argument            | Purpose                                                           | Typical values |
|---------------------|-------------------------------------------------------------------|----------------|
| `n`                 | number of individuals                                             | 100–10 000     |
| `waves`             | follow-up waves (outcomes measured at `waves + 1`)                | 3–10           |
| `p_covars`          | # of time-varying **L** covariates                                | 0–5            |
| `exposures`         | list describing each **A** variable                               | see §3         |
| `outcomes`          | list describing **Y** variables                                   | rarely needed  |
| `y_feedback`        | strength of *Y → A* path (0 disables)                             | 0–1            |
| `censoring`         | dropout mechanism: `rate`, `exposure_dependence`, …               | list           |
| `item_missing_rate` | MCAR missingness on observed values                               | 0–0.5          |
| `wide`              | return wide format (TRUE recommended; long format under development) | TRUE   |

The default coefficient set is returned by `.default_sim_params()`.  Supply a
named list to the `params` argument to override any subset.

## 3. Worked examples

### 3.1 Baseline: simple continuous outcome
```{r simple-example}
# generate data in wide format (default)
basic_dat <- margot_simulate(n = 500, waves = 3, seed = 2025)

# examine structure
str(basic_dat[, 1:10])

# summarize treatment and outcome
summary(basic_dat[c("t1_A1", "t2_A1", "t3_A1", "t4_Y")])
```

### 3.2 Heterogeneous treatment effects (effect modification)

Suppose the causal effect of exposure `A1` varies with baseline covariate `B2`.
We encode this via the `het` sub-list:

```{r het-sim}
het_dat <- margot_simulate(
  n        = 4000,
  waves    = 3,
  exposures = list(
    A1 = list(
      type = "binary",
      het  = list(modifier = "B2",  # effect modifier
                  coef     = 0.6)    # γ: interaction strength
    )
  ),
  exposure_outcome = 0.4,  # marginal main effect β
  seed = 2027
)
```

Verify the interaction:

```{r het-check}
fit_het <- lm(t4_Y ~ t3_A1 * B2, data = het_dat)
summary(fit_het)$coefficients
```

> **Take-away.** The interaction term (`t3_A1:B2`) recovers  
> $\\beta_{\\text{het}} \\approx 0.6$, matching the data-generating value.

### 3.3 Outcome → Treatment feedback

Realistic behavioural processes often exhibit reflexivity: past outcomes
influence future treatment uptake.  Activate this with `y_feedback`:

```{r feedback-sim}
fb_dat <- margot_simulate(
  n           = 2000,
  waves       = 5,
  y_feedback  = 0.8,   # strong Y → A dependency
  params      = list(exp_L1_coef = 0.3), # optional A → L feedback
  wide        = TRUE,  # use wide format for now
  seed        = 101
)
```

Visualise treatment rates by examining how previous outcomes affect treatment:

```{r feedback-plot, fig.alt = "Treatment probability trends across waves"}
# Extract treatment rates across waves
# The exposure columns follow the pattern t0_A1, t1_A1, t2_A1, etc.
treatment_data <- fb_dat %>%
  select(starts_with("t") & ends_with("_A1")) %>%
  summarise(across(everything(), ~ mean(.x, na.rm = TRUE))) %>%
  tidyr::pivot_longer(
    cols = everything(),
    names_to = "wave_var",
    values_to = "prop_treated"
  ) %>%
  mutate(
    wave = as.numeric(sub("t([0-9]+)_A1", "\\1", wave_var))
  ) %>%
  filter(wave > 0)  # Exclude baseline (t0)

# Plot treatment rates
ggplot(treatment_data, aes(x = wave, y = prop_treated)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_point(size = 3, color = "steelblue") +
  labs(
    title = "Treatment Assignment Rates Across Waves",
    subtitle = "With y_feedback = 0.8, treatment probability increases over time",
    x = "Wave",
    y = "Proportion Treated"
  ) +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 1))
```

The increasing treatment probability across waves demonstrates how the y_feedback
parameter induces time-varying confounding, where past outcomes influence future
treatment decisions.

## 3.4 Positivity diagnostics

Estimators grounded in the g-formula, IPW, or TMLE require sufficient
overlap (positivity) of treatment probabilities across strata of measured
covariates.  Below we inspect estimated propensity scores at the final
observed wave in the heterogeneous-effects example.

```{r positivity-plot, fig.alt="Histogram showing propensity score distributions for treated and control groups with good overlap"}
# wave-3 propensity model: A1 ~ B2
# Filter to complete cases for this analysis
het_complete <- het_dat %>%
  filter(!is.na(t3_A1) & !is.na(B2))

ps_mod <- glm(t3_A1 ~ B2, family = binomial, data = het_complete)
het_complete$ps3 <- predict(ps_mod, type = "response")

ggplot(het_complete, aes(ps3, fill = factor(t3_A1))) +
  geom_histogram(position = "identity", alpha = 0.4, bins = 30) +
  labs(
    x    = "estimated propensity score",
    fill = "A1 at wave 3"
  ) +
  theme_minimal()
```

## 4. Censoring & missingness

Attrition can depend on exposure, a latent frailty shared within individuals,
or both.  For brevity, see the dedicated *Censoring* vignette.

## 5. Power analysis template

Combined heterogeneity and feedback can inflate sample-size requirements.  Use
batched simulations to trace operating characteristics; skeleton:

```{r power-template, eval = FALSE}
power_grid <- tibble::tibble(N = seq(500, 5000, by = 500))

power_grid$power <- vapply(power_grid$N, function(n) {
  mean(replicate(300, {
    dat <- margot_simulate(n = n, waves = 4,
                           y_feedback = 0.7,
                           exposures = list(A1 = list(type = "binary",
                                                      het = list(modifier = "B1", coef = 0.5))),
                           exposure_outcome = 0.3, wide = TRUE))
    t.test(t5_Y ~ t3_A1, data = dat)$p.value < 0.05
  }))
}, numeric(1))

ggplot(power_grid, aes(N, power)) +
  geom_line() +
  geom_hline(yintercept = 0.8, linetype = "dashed") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "Power", x = "Sample size")
```

## 6. Further reading

- *Hernán & Robins (2020)* — *Causal Inference: What If*.
- *Daniel et al. (2013)* — methods for time-varying confounding.

---

Happy simulating — and remember to set a seed!  
