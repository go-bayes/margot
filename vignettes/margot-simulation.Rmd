---
title: "Simulating Longitudinal Data with *margot*"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simulating Longitudinal Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  fig.width  = 7,
  fig.height = 5
)
```

```{r setup}
library(margot)   # core simulation + estimation tools
library(dplyr)    # tidy manipulation
library(ggplot2)  # graphics for examples
```

## 1. Why simulate longitudinal data?

Synthetic panel data with *known* data-generating mechanisms are invaluable for

- **Teaching**: illustrate causal ideas where the ground truth is visible;
- **Software testing**: ensure estimators recover the truth under controlled violations;
- **Power & design**: gauge sample-size or wave-number requirements;
- **Method development**: iterate quickly without large, confidential datasets.

`margot_simulate()` is our workhorse; the sections below unpack its interface
and demonstrate common scenarios.

## 2. Anatomy of `margot_simulate()`

The simulator draws baseline covariates **B**, time-varying covariates **L**,
exposures **A**, and lead outcomes **Y** across a user-specified number of
waves.  *Figure 1* gives a high-level schematic.

```{r fig.cap = "Figure 1: data-generating process underlying `margot_simulate()`. Solid arrows are structural paths; dashed arrows indicate optional feedback loops and censoring."}
# build a toy DAG with DiagrammeR for illustration only (not evaluated when package is built)
if (requireNamespace("DiagrammeR", quietly = TRUE)) {
  DiagrammeR::grViz(
    "digraph {graph [rankdir = LR, bgcolor = none]
      node [shape = box, fontname = Helvetica]
      subgraph cluster0 {label = \"Wave 0\"; style = dashed; B; t0_L;}
      subgraph cluster1 {label = \"Wave 1\"; style = dashed; t1_A; t1_L; t1_Y;}
      subgraph cluster2 {label = \"Wave 2\"; style = dashed; t2_A; t2_L; t2_Y;}
      B -> t1_L -> t1_A -> t1_Y
      B -> t1_A
      t1_Y -> t2_A [style = dashed]  // optional y_feedback
      t1_A -> t2_L [style = dashed]  // covar_feedback
      t1_A -> censor1 [style = dashed]
      censor1 [shape = circle, label = \"C\"]
    }"
  )
}
```

Key arguments (non-exhaustive):

| Argument            | Purpose                                                           | Typical values |
|---------------------|-------------------------------------------------------------------|----------------|
| `n`                 | number of individuals                                             | 100–10 000     |
| `waves`             | follow-up waves (outcomes measured at `waves + 1`)                | 3–10           |
| `p_covars`          | # of time-varying **L** covariates                                | 0–5            |
| `exposures`         | list describing each **A** variable                               | see §3         |
| `outcomes`          | list describing **Y** variables                                   | rarely needed  |
| `y_feedback`        | strength of *Y → A* path (0 disables)                             | 0–1            |
| `censoring`         | dropout mechanism: `rate`, `exposure_dependence`, …               | list           |
| `item_missing_rate` | MCAR missingness on observed values                               | 0–0.5          |
| `wide`              | return wide (TRUE) or long (FALSE) format                         | TRUE / FALSE   |

The default coefficient set is returned by `.default_sim_params()`.  Supply a
named list to the `params` argument to override any subset.

## 3. Worked examples

### 3.1 Baseline: simple continuous outcome
```{r simple-example}
basic_dat <- margot_simulate(n = 500, waves = 3, seed = 2025)
basic_dat %>%
  group_by(wave) %>%
  summarise(
    n_obs        = dplyr::n(),
    prop_treated = mean(a),
    mean_Y       = mean(y, na.rm = TRUE)
  )
```

### 3.2 Heterogeneous treatment effects (effect modification)

Suppose the causal effect of exposure `A1` varies with baseline covariate `B2`.
We encode this via the `het` sub-list:

```{r het-sim}
het_dat <- margot_simulate(
  n        = 4000,
  waves    = 3,
  exposures = list(
    A1 = list(
      type = "binary",
      het  = list(modifier = "B2",  # effect modifier
                  coef     = 0.6)    # γ: interaction strength
    )
  ),
  exposure_outcome = 0.4,  # marginal main effect β
  seed = 2027
)
```

Verify the interaction:

```{r het-check}
fit_het <- lm(t4_Y ~ t3_A1 * B2, data = het_dat)
summary(fit_het)$coefficients
```

> **Take-away.** The interaction term (`t3_A1:B2`) recovers  
> $\\beta_{\\text{het}} \\approx 0.6$, matching the data-generating value.

### 3.3 Outcome → Treatment feedback

Realistic behavioural processes often exhibit reflexivity: past outcomes
influence future treatment uptake.  Activate this with `y_feedback`:

```{r feedback-sim}
fb_dat <- margot_simulate(
  n           = 2000,
  waves       = 5,
  y_feedback  = 0.8,   # strong Y → A dependency
  covar_feedback = 0.3, # optional A → L feedback
  wide        = FALSE,
  seed        = 101
)
```

Visualise treatment rates by quartiles of the previous outcome:

```{r feedback-plot, fig.alt = "Treatment probability across waves stratified by previous-wave outcome quartile"}
fb_plot <- fb_dat %>%
  filter(wave > 1) %>%
  group_by(id) %>%
  mutate(prev_Y = lag(y)) %>%
  ungroup() %>%
  filter(!is.na(prev_Y)) %>%
  mutate(q = ntile(prev_Y, 4L)) %>%
  group_by(wave, q) %>%
  summarise(prop_A = mean(a), .groups = "drop")

ggplot(fb_plot, aes(wave, prop_A, colour = factor(q))) +
  geom_line(linewidth = 1.1) +
  geom_point(size = 2) +
  scale_colour_brewer(name = "Prev. Y quartile", palette = "Dark2") +
  labs(y = "Pr(A = 1)", x = "Wave") +
  theme_minimal()
```

Higher past outcomes (quartiles 3–4) increase the probability of treatment in
later waves, demonstrating the induced time-varying confounding.

## 3.4 Positivity diagnostics

Estimators grounded in the g-formula, IPW, or TMLE require sufficient
overlap (positivity) of treatment probabilities across strata of measured
covariates.  Below we inspect estimated propensity scores at the final
observed wave in the heterogeneous-effects example.

```{r positivity-plot}
# wave-3 propensity model: A1 ~ B2
ps_mod <- glm(t3_A1 ~ B2, family = binomial, data = het_dat)
het_dat$ps3 <- predict(ps_mod, type = "response")

ggplot(het_dat, aes(ps3, fill = factor(t3_A1))) +
  geom_histogram(position = "identity", alpha = 0.4, bins = 30) +
  labs(
    x    = "estimated propensity score",
    fill = "A1 at wave 3"
  ) +
  theme_minimal()
```

## 4. Censoring & missingness

Attrition can depend on exposure, a latent frailty shared within individuals,
or both.  For brevity, see the dedicated *Censoring* vignette.

## 5. Power analysis template

Combined heterogeneity and feedback can inflate sample-size requirements.  Use
batched simulations to trace operating characteristics; skeleton:

```{r power-template, eval = FALSE}
power_grid <- tibble::tibble(N = seq(500, 5000, by = 500))

power_grid$power <- vapply(power_grid$N, function(n) {
  mean(replicate(300, {
    dat <- margot_simulate(n = n, waves = 4,
                           y_feedback = 0.7,
                           exposures = list(A1 = list(type = "binary",
                                                      het = list(modifier = "B1", coef = 0.5))),
                           exposure_outcome = 0.3, wide = TRUE))
    t.test(y ~ t3_A1, data = dat)$p.value < 0.05
  }))
}, numeric(1))

ggplot(power_grid, aes(N, power)) +
  geom_line() +
  geom_hline(yintercept = 0.8, linetype = "dashed") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "Power", x = "Sample size")
```

## 6. Further reading

- *Hernán & Robins (2020)* — *Causal Inference: What If*.
- *Daniel et al. (2013)* — methods for time-varying confounding.

---

Happy simulating — and remember to set a seed!  
