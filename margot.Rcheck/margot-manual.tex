\nonstopmode{}
\documentclass[a4paper]{book}
\usepackage[times,inconsolata,hyper]{Rd}
\usepackage{makeidx}
\makeatletter\@ifl@t@r\fmtversion{2018/04/01}{}{\usepackage[utf8]{inputenc}}\makeatother
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `margot'}}
\par\bigskip{\large \today}
\end{center}
\ifthenelse{\boolean{Rd@use@hyper}}{\hypersetup{pdftitle = {margot: MARGinal Observational Treatment-effects}}}{}
\ifthenelse{\boolean{Rd@use@hyper}}{\hypersetup{pdfauthor = {Joseph A Bulbulia}}}{}
\begin{description}
\raggedright{}
\item[Type]\AsIs{Package}
\item[Title]\AsIs{MARGinal Observational Treatment-effects}
\item[Version]\AsIs{1.0.60}
\item[Description]\AsIs{Functions to obtain MARGinal Observational Treatment-effects from observational data. The package includes simulated data from The New Zealand Attitudes and Values Study, licensed for non-commercial use only to protect privacy. The code is licensed under the GNU General Public License (GPL) v3.0.}
\item[License]\AsIs{CC BY 4.0}
\item[Encoding]\AsIs{UTF-8}
\item[LazyData]\AsIs{true}
\item[Depends]\AsIs{R (>= 4.1)}
\item[Imports]\AsIs{cli, cobalt, crayon, data.table, DiagrammeR, doParallel,
dplyr, EValue, fastDummies, future, flextable, ggokabeito,
ggplot2, ggeffects, gt, gtsummary, grf, glue, here, janitor,
kableExtra, knitr, lifecycle, lubridate, magrittr, MatchIt,
MatchThem, maq, mice, miceadds, naniar, parameters, patchwork,
policytree, progressr, purrr, qs, rlang, report, stats,
SuperLearner, table1, tibble, tidyr, tidyverse, WeightIt,
stringr, zoo, lmtp, clarify, labelled, tools, furrr,
future.apply, vctrs}
\item[RoxygenNote]\AsIs{7.3.2}
\item[Config/testthat/edition]\AsIs{3}
\item[URL]\AsIs{}\url{https://go-bayes.github.io/margot}\AsIs{}
\item[BugReports]\AsIs{}\url{https://github.com/go-bayes/margot/issues}\AsIs{}
\item[NeedsCompilation]\AsIs{no}
\item[Author]\AsIs{Joseph A Bulbulia [aut, cre]}
\item[Maintainer]\AsIs{Joseph A Bulbulia }\email{joseph.bulbulia@gmail.com}\AsIs{}
\end{description}
\Rdcontents{Contents}
\HeaderA{.strict\_exposure\_outcome\_censoring}{Strict All-or-Nothing Censoring for Longitudinal Data}{.strict.Rul.exposure.Rul.outcome.Rul.censoring}
\keyword{internal}{.strict\_exposure\_outcome\_censoring}
%
\begin{Description}
This function processes wide-format longitudinal data with multiple time points:

- For each wave t < final wave:
- If wave t+1 **has exposure columns**, a participant remains "not lost" at wave t
only if *all* exposures at wave t+1 are present (no missing). Otherwise, they are censored at wave t.
- If wave t+1 **has no exposures** (i.e., final wave is purely outcomes),
we require *all* final-wave outcomes to be present. If *any* final-wave outcome is missing,
the participant is censored from wave t onward.

Censoring sets all future waves to `NA`, and once censored, participants remain censored.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
.strict_exposure_outcome_censoring(
  df_wide,
  exposure_vars,
  ordinal_columns = NULL,
  continuous_columns_keep = NULL,
  scale_exposure = FALSE,
  not_lost_in_following_wave = "not_lost_following_wave",
  lost_in_following_wave = "lost_following_wave",
  remove_selected_columns = TRUE,
  time_point_prefixes = NULL,
  time_point_regex = NULL,
  save_observed_y = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df\_wide}] A wide-format dataframe with columns like t0\_X, t1\_X, t2\_X, etc.

\item[\code{exposure\_vars}] Character vector of all exposure names (e.g. c("aaron\_antagonism", "aaron\_disinhibition", ...)).

\item[\code{ordinal\_columns}] Character vector of ordinal (factor) variables to be dummy-coded.

\item[\code{continuous\_columns\_keep}] Numeric columns you do NOT want to scale (e.g. if they must remain in original units).

\item[\code{scale\_exposure}] If FALSE, do not scale exposures; if TRUE, exposures are also scaled.

\item[\code{not\_lost\_in\_following\_wave}] Name for the "not lost" indicator (default "not\_lost\_following\_wave").

\item[\code{lost\_in\_following\_wave}] Name for the "lost" indicator (default "lost\_following\_wave").

\item[\code{remove\_selected\_columns}] If TRUE, remove original columns after dummy-coding ordinal columns.

\item[\code{time\_point\_prefixes}] Optional vector of wave prefixes (like c("t0","t1","t2")); if NULL, we auto-detect via regex.

\item[\code{time\_point\_regex}] Regex used to detect wave prefixes if `time\_point\_prefixes` is NULL.

\item[\code{save\_observed\_y}] If FALSE, set any missing final-wave outcomes to NA. If TRUE, keep partial final-wave outcomes.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
**Core Logic**
For wave t from 0 to T-2 (i.e., up to the penultimate wave):
\begin{alltt}
  needed_exposures <- paste0(t+1, "_", exposure_vars)
  not_lost[t] = 1 if rowSums(!is.na(needed_exposures)) == length(needed_exposures)
               else 0

  if not_lost[t] = 0, set waves t+1..T to NA
\end{alltt}

If wave t+1 is the final wave and it has no exposures, we fallback to the final wave's outcome columns.
Then "not\_lost[t] = 1 if *all* final-wave outcomes are present, else 0".

This is a "strict" approach: if *any* exposure is missing at wave t+1, we censor from wave t onward.
\end{Details}
%
\begin{Value}
A processed dataframe, with strict all-or-nothing censoring on exposures in earlier waves,
and outcome-based censoring for the final wave if it lacks exposures.
\end{Value}
\HeaderA{back\_transform\_logmean}{Back-transform Log-transformed Mean}{back.Rul.transform.Rul.logmean}
%
\begin{Description}
Back-transforms a log-transformed mean (using log(x + 1) transformation) to its original scale.
This utility function is useful for interpreting results when the original data were transformed using log(x + 1)
to handle zero values or to normalize the distribution of the data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
back_transform_logmean(log_mean)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{log\_mean}] The mean on the log scale, where the original data were transformed using log(x + 1).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the mean on the original scale (`mean\_original`).
The standard deviation is not back-transformed by this function due to the complexity introduced by the log(x + 1) transformation.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
log_mean <- 1.098612 # true mean is 2. We add + 1 to the log to handle zero: log(2+1) = log(3)
back_transformed_result <- back_transform_logmean(log_mean)
print(back_transformed_result)

\end{ExampleCode}
\end{Examples}
\HeaderA{back\_transform\_log\_z}{Back Transform Z-Score to Original Log-Transformed Scale}{back.Rul.transform.Rul.log.Rul.z}
%
\begin{Description}
This function takes z-scores and transforms them back to their original values
where the data was originally log-transformed. It performs a two-step transformation:
first converting z-scores back to log values using the log-scale mean and standard deviation,
then exponentiating to return to the original scale.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
back_transform_log_z(z_scores, log_mean, log_sd)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{z\_scores}] A numeric value or vector of z-scores to be transformed back to the original scale.

\item[\code{log\_mean}] The mean of the log-transformed dataset from which the z-scores were calculated.

\item[\code{log\_sd}] The standard deviation of the log-transformed dataset from which the z-scores were calculated.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Returns a numeric value or vector of the original scale values corresponding to the input z-scores.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Given log-transformed data with log_mean = 1.5 and log_sd = 0.5
original_value <- back_transform_log_z(z_scores = 1.2, log_mean = 1.5, log_sd = 0.5)
print(original_value)

# Multiple z-scores can be transformed at once
z_scores <- c(-1, 0, 1, 2)
original_values <- back_transform_log_z(z_scores = z_scores, log_mean = 1.5, log_sd = 0.5)
print(original_values)

# Real-world example: back-transforming household income z-scores
# Get mean and sd from original log-transformed data
log_mean_inc <- mean(original_df$t0_log_household_inc, na.rm = TRUE)
log_sd_inc <- sd(original_df$t0_log_household_inc, na.rm = TRUE)

# Back-transform all z-scores in the dataset
original_data_scale <- back_transform_log_z(
  df_grf$t0_log_household_inc_z,
  log_mean = log_mean_inc,
  log_sd = log_sd_inc
)
head(original_data_scale)

# Interpret key points on the distribution (-1 SD, mean, +1 SD)
z_scores <- c(-1, 0, 1)
scale_values <- back_transform_log_z(
  z_scores,
  log_mean = log_mean_inc,
  log_sd = log_sd_inc
)

# Create a data frame to display the relationship between z-scores and original values
results_df <- data.frame(
  z_score = z_scores,
  data_scale = scale_values
)
print(results_df) # Shows what values on the original scale correspond to each z-score

\end{ExampleCode}
\end{Examples}
\HeaderA{back\_transform\_zscore}{Back Transform Z-Score to Original Scale}{back.Rul.transform.Rul.zscore}
%
\begin{Description}
This function takes a z-score and transforms it back to its original scale
using the specified mean and standard deviation of the original data. Often,
standardization has been applied and the original scale values are needed for
interpretation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
back_transform_zscore(z, mean, sd)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{z}] A numeric value or vector of z-scores to be transformed back to the original scale.

\item[\code{mean}] The mean of the original dataset from which the z-score was calculated.

\item[\code{sd}] The standard deviation of the original dataset from which the z-score was calculated.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Returns a numeric value or vector of the original scale values corresponding to the input z-scores.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Given a dataset with mean = 100 and sd = 15
original_value <- back_transform_zscore(z = 1.5, mean = 100, sd = 15)
print(original_value)

# Multiple z-scores can be transformed at once
z_scores <- c(-1, 0, 1, 2)
original_values <- back_transform_zscore(z = z_scores, mean = 50, sd = 10)
print(original_values)

\end{ExampleCode}
\end{Examples}
\HeaderA{causal\_contrast\_engine}{Compute Causal Contrasts}{causal.Rul.contrast.Rul.engine}
\keyword{internal}{causal\_contrast\_engine}
%
\begin{Description}
Estimates causal contrasts using generalized linear models for different types of treatment effects (ATE, ATT)
and outcomes (RR, RD). Supports handling multiply imputed datasets and allows flexibility in model specification.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
causal_contrast_engine(
  df,
  Y,
  X,
  baseline_vars,
  treat_0,
  treat_1,
  estimand = c("ATE", "ATT"),
  type = c("RR", "RD"),
  nsims = 200,
  cores = parallel::detectCores(),
  family = "gaussian",
  weights = TRUE,
  continuous_X = FALSE,
  splines = FALSE,
  vcov = "HC2",
  verbose = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Data frame or `mids` object containing the data.

\item[\code{Y}] Response variable name as a string.

\item[\code{X}] Treatment or exposure variable name as a string.

\item[\code{baseline\_vars}] Vector of baseline covariate names.

\item[\code{treat\_0}] Reference level of the treatment variable.

\item[\code{treat\_1}] Treatment level of interest for comparison.

\item[\code{estimand}] Type of causal estimand ("ATE", "ATT"); defaults to both.

\item[\code{type}] Type of effect size ("RR" for Risk Ratio, "RD" for Risk Difference); defaults to both.

\item[\code{nsims}] Number of simulations for bootstrap; defaults to 200.

\item[\code{cores}] Number of cores for parallel processing; uses all available cores by default.

\item[\code{family}] Model family as a string or family object; defaults to "gaussian".

\item[\code{weights}] The name of the weights variable in the data frame, or NULL if no weights are to be used.

\item[\code{continuous\_X}] Whether X is a continuous variable; defaults to FALSE.

\item[\code{splines}] Whether to apply spline transformation to X; defaults to FALSE.

\item[\code{vcov}] Type of variance-covariance matrix for standard error estimation; defaults to "HC2".

\item[\code{verbose}] Whether to print detailed output; defaults to FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Depending on the configuration, returns a summary object containing estimated causal contrasts, confidence intervals, and potentially other diagnostics.
\end{Value}
\HeaderA{causal\_contrast\_marginal}{Causal Contrast Marginal Effects Estimation}{causal.Rul.contrast.Rul.marginal}
\keyword{internal}{causal\_contrast\_marginal}
%
\begin{Description}
This function estimates the average treatment effect (ATE) or average treatment effect on the treated (ATT)
using generalized linear models (GLMs). It supports handling of continuous and categorical treatments, optional
use of spline transformations, and adjustments for multiple imputation datasets.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
causal_contrast_marginal(
  df,
  Y,
  X,
  baseline_vars = "1",
  treat_0,
  treat_1,
  estimand = c("ATE", "ATT"),
  type = c("RR", "RD"),
  nsims = 200,
  cores = parallel::detectCores(),
  family = "gaussian",
  weights = NULL,
  continuous_X = FALSE,
  splines = FALSE,
  vcov = "HC2",
  verbose = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Data frame containing the data.

\item[\code{Y}] The response variable in the data frame.

\item[\code{X}] The treatment or exposure variable in the data frame.

\item[\code{baseline\_vars}] A vector of names of baseline covariates to adjust for in the model.

\item[\code{treat\_0}] The reference level of the treatment variable, corresponding to no treatment or control condition.

\item[\code{treat\_1}] The active level of the treatment variable, corresponding to receiving the treatment.

\item[\code{estimand}] A character vector specifying the estimand; "ATE" for Average Treatment Effect or "ATT" for Average Treatment Effect on the Treated.

\item[\code{type}] A character vector specifying the type of effect size; "RD" for Risk Difference or "RR" for Risk Ratio.

\item[\code{nsims}] Number of simulations to perform, relevant when handling multiple imputation datasets.

\item[\code{cores}] Number of cores to use for parallel processing.

\item[\code{family}] The family of the GLM to be used (e.g., "gaussian" for linear models).

\item[\code{weights}] The name of the weights variable in the data frame, or NULL if no weights are to be used.

\item[\code{continuous\_X}] Logical indicating whether the treatment variable X is continuous.

\item[\code{splines}] Logical indicating whether to use spline transformations for the treatment variable X.

\item[\code{vcov}] The method to use for variance-covariance matrix estimation.

\item[\code{verbose}] Logical indicating whether to display detailed output during model fitting.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Depending on the 'type' specified, it returns a summary object containing either risk differences or risk ratios along with additional statistics like confidence intervals.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Assume that df is your dataset with variables 'outcome', 'treatment', 'age', and 'gender'
result <- causal_contrast_marginal(df = df, Y = "outcome", X = "treatment",
                                   baseline_vars = c("age", "gender"),
                                   treat_0 = "control", treat_1 = "exposed",
                                   estimand = "ATE", type = "RD", nsims = 100,
                                   cores = 2, family = "gaussian", weights = "weight_var",
                                   continuous_X = FALSE, splines = FALSE,
                                   vcov = "HC3", verbose = TRUE)

\end{ExampleCode}
\end{Examples}
\HeaderA{coloured\_histogram}{Create a Coloured Histogram Highlighting Specific Ranges (DEPRECATED)}{coloured.Rul.histogram}
\keyword{internal}{coloured\_histogram}
%
\begin{Description}
`r lifecycle::badge("deprecated")`
This function is deprecated. Please use `margot\_plot\_shift()` instead.

This function generates a histogram with specific ranges highlighted to indicate
the highest and/or lowest values within a unit of the specified limits. It allows
customization of bin width, the unit of change for highlighting, and the range to be highlighted. This is useful in the settings of modified treatment policies for
clarifying which part of a distribution is shifted.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
coloured_histogram(
  df,
  col_name,
  binwidth = 1,
  unit_of_change = 1,
  scale_min = NULL,
  scale_max = NULL,
  highlight_range = "highest"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] The dataframe containing the data to be plotted.

\item[\code{col\_name}] The name of the column for which the histogram will be generated.

\item[\code{binwidth}] The width of the bins for the histogram; defaults to 1.

\item[\code{unit\_of\_change}] The unit of change used to define the highlight range.
The subtitle will mention this unit. It also adjusts the calculation of the highlight thresholds
to be slightly less than this unit so that it does not go over the range of the data. Defaults to 1.

\item[\code{scale\_min}] The minimum value to be used for scaling the histogram. If `NULL`,
the minimum value of `col\_name` is used.

\item[\code{scale\_max}] The maximum value to be used for scaling the histogram. If `NULL`,
the maximum value of `col\_name` is used.

\item[\code{highlight\_range}] Specifies which range to highlight: "lowest", "highest", or
"both". Defaults to "highest".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object of the histogram with highlighted ranges as specified.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
 ## Not run: 
# assuming df_19 is your dataframe and contains the column 'forgiveness'
graph <- coloured_histogram(
  df = df_19,
  col_name = "forgiveness",
  scale_min = 1,
  scale_max = 7,
  highlight_range = "highest",
  binwidth = .1, # adjust binwidth as needed
  unit_of_change = 1 # specify the unit of change
)
print(graph)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{coloured\_histogram\_quantiles}{Visualise Distribution with Automatically Calculated Quantile Highlights (DEPRECATED)}{coloured.Rul.histogram.Rul.quantiles}
\keyword{internal}{coloured\_histogram\_quantiles}
%
\begin{Description}
`r lifecycle::badge("deprecated")`
This function is deprecated. Please use `margot\_plot\_categorical()` instead.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
coloured_histogram_quantiles(
  df,
  col_name,
  n_divisions = NULL,
  breaks = NULL,
  binwidth = NULL,
  n_bins = NULL,
  cutpoint_inclusive = "upper",
  ties.method = NULL,
  colours = NULL,
  hist_fill = "lightgray",
  hist_colour = "black",
  line_type = "solid",
  line_width = 0.75,
  title = NULL,
  subtitle = NULL,
  x_lab = NULL,
  y_lab = "Count",
  theme_choice = theme_classic(),
  text_size = 12,
  axis_text_angle = 45,
  add_density = FALSE,
  add_rug = FALSE,
  facet_var = NULL,
  x_scale_transform = NULL,
  y_scale_transform = NULL,
  additional_layers = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] dataframe containing the data to be visualised

\item[\code{col\_name}] name of the column to create a histogram for

\item[\code{n\_divisions}] the number of divisions to create. if null, user must provide breaks

\item[\code{breaks}] optional. a numeric vector specifying custom breakpoints

\item[\code{binwidth}] width of the bins for the histogram

\item[\code{n\_bins}] optional. number of bins for the histogram. overrides binwidth if specified

\item[\code{cutpoint\_inclusive}] a character string specifying whether cutpoints should be included
in the lower or upper category. must be either "lower" or "upper". default is "upper"

\item[\code{ties.method}] a character string specifying how ties should be handled

\item[\code{colours}] optional. a vector of colours for the quantile lines

\item[\code{hist\_fill}] colour for histogram fill. default is "lightgray"

\item[\code{hist\_colour}] colour for histogram outline. default is "black"

\item[\code{line\_type}] line type for quantile lines. default is "solid"

\item[\code{line\_width}] line width for quantile lines. default is 0.75

\item[\code{title}] custom title for the plot. if null, a default title is used

\item[\code{subtitle}] custom subtitle for the plot. if null, a default subtitle is used

\item[\code{x\_lab}] custom x-axis label. if null, the column name is used

\item[\code{y\_lab}] custom y-axis label. default is "count"

\item[\code{theme\_choice}] ggplot2 theme to use. default is theme\_classic()

\item[\code{text\_size}] base text size for the plot. default is 12

\item[\code{axis\_text\_angle}] angle for x-axis text. default is 45

\item[\code{add\_density}] logical. if true, adds a density curve to the plot

\item[\code{add\_rug}] logical. if true, adds a rug plot to the x-axis

\item[\code{facet\_var}] optional. name of variable to use for faceting

\item[\code{x\_scale\_transform}] optional. transformation for x-axis (e.g., "log10")

\item[\code{y\_scale\_transform}] optional. transformation for y-axis (e.g., "log10")

\item[\code{additional\_layers}] optional list of additional ggplot2 layers to add to the plot
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a ggplot object representing the histogram with highlighted quantiles
\end{Value}
\HeaderA{coloured\_histogram\_sd}{Visualize Distribution with Mean and Standard Deviation Highlights}{coloured.Rul.histogram.Rul.sd}
\keyword{internal}{coloured\_histogram\_sd}
%
\begin{Description}
This function creates a histogram for a specified column in a dataframe,
highlighting the mean and one standard deviation above and below the mean.
It draws vertical lines for the mean (in black) and for plus/minus one standard
deviation (in blue and gold, respectively), with arrows from the mean to each
standard deviation marker. The title of the plot includes the capitalized column name,
achieved using \code{tools::toTitleCase()}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
coloured_histogram_sd(df, col_name, binwidth = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Dataframe containing the data to be visualized.

\item[\code{col\_name}] Name of the column to create a histogram for. This column should
contain numeric data.

\item[\code{binwidth}] Width of the bins for the histogram. Can be adjusted for finer or
coarser resolution of the distribution. Default is 1.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object representing the histogram with highlights for the mean and
standard deviations. The plot can be printed or modified further.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
 ## Not run: 
# Assuming `df_nz` is a dataframe with a numeric column 'forgiveness'
# and a factor or integer column 'wave' for subsetting:
df_19 <- dplyr::filter(df_nz, wave == 2019)

graph_density_of_exposure <- coloured_histogram_sd(
  df = df_19,
  col_name = "forgiveness",
  binwidth = 0.5 # Adjust binwidth as needed
)

## End(Not run)
print(graph_density_of_exposure)

\end{ExampleCode}
\end{Examples}
\HeaderA{coloured\_histogram\_shift}{Visualise Shifts in Data Distributions with Highlighted Ranges (DEPRECATED)}{coloured.Rul.histogram.Rul.shift}
\keyword{internal}{coloured\_histogram\_shift}
%
\begin{Description}
`r lifecycle::badge("deprecated")`
This function is deprecated. Please use `margot\_plot\_shift()` instead.

This function creates a histogram that highlights a specified range of values to visualize shifts in data distributions. The highlighted range can indicate areas of interest, such as shifts up or down in the distribution. This visualization is useful for understanding the implications of causal contrasts, such as modified treatment policies. The fill colour of the histogram is dynamically adjusted based on the specified direction of the shift.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
coloured_histogram_shift(
  df,
  col_name,
  binwidth = 1,
  range_highlight = NULL,
  shift = "up",
  show_avg_line = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A dataframe containing the variable of interest.

\item[\code{col\_name}] The name of the column in `df` to be visualized in the histogram. This should be a numeric variable.

\item[\code{binwidth}] The width of the bins for the histogram. Default is 1. Adjust this based on the distribution and scale of your data to create a meaningful visualization.

\item[\code{range\_highlight}] A numeric vector of length 2 specifying the start and end of the range to highlight. If `NULL`, no range is highlighted.

\item[\code{shift}] A character string indicating the direction of the shift, with "up" highlighting in gold and "down" highlighting in dodger blue. The default is "up".

\item[\code{show\_avg\_line}] A logical value indicating whether to display a vertical line representing the average value of the specified column using a red dashed line. Default is `TRUE`.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A `ggplot` object representing the histogram with specified highlights. This object can be printed or further modified using `ggplot2` functions.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming df_nz is your dataframe and it includes a numeric variable 'forgiveness'
# Filter to a specific subset, for example, wave 2019
df_19 <- dplyr::filter(df_nz, wave == 2019)

# Create and print the histogram
graph_density_of_exposure <- coloured_histogram_shift(
  df = df_19,
  shift = "down",
  col_name = "forgiveness",
  binwidth = .5, # Adjust binwidth for your data
  range_highlight = c(3.9, 10)
)
print(graph_density_of_exposure)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{compute\_difference}{Compute Difference in Average Treatment Effects or Relative Risk Ratio Between Two Subgroups}{compute.Rul.difference}
\keyword{internal}{compute\_difference}
%
\begin{Description}
`r lifecycle::badge("deprecated")`
This function is deprecated and will be removed in a future release.
Please use the new `margot\_compare\_groups()` function and associated workflow instead.

This function calculates either the difference in average treatment effects (ATE) or the relative risk ratio (RRR)
between two independent subgroups. Each subgroup is represented as a list that includes the estimated effect (theta) and
the standard error (std.error) of the effect. The result includes both a data frame and an interpretation
string formatted for easy use with `glue::glue` in Quarto documents etc. The subgroups are expected to be
outputs from the `lmtp::lmtp\_contrast()` function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
compute_difference(group1, group2, type = "RD")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{group1}] A list containing the estimated effect and standard error of subgroup 1.
Expected structure: list(vals = data.frame(theta = x, std.error = y)).

\item[\code{group2}] A list containing the estimated effect and standard error of subgroup 2.
Expected structure: list(vals = data.frame(theta = x, std.error = y)).

\item[\code{type}] A character string specifying the type of calculation. "RD" for risk difference (default),
"RR" for relative risk ratio.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing:
- `results`: A data frame with columns `mean\_difference`, `std\_error`, `conf\_low`, and `conf\_high` for type "RD",
or `rrr`, `std\_error\_log`, `conf\_low`, and `conf\_high` for type "RR", each rounded to 4 decimal places.
Suitable for direct use in reporting.
- `interpretation`: A string providing a formatted interpretation of the results.
\end{Value}
\HeaderA{compute\_qini\_curves}{Compute Qini Curves for Multi-Arm and Binary Treatments (Deprecated)}{compute.Rul.qini.Rul.curves}
\keyword{deprecated}{compute\_qini\_curves}
\keyword{internal}{compute\_qini\_curves}
%
\begin{Description}
DEPRECATED: This function is no longer functional and will be removed in a future version.
Please use `compute\_qini\_curves\_binary()` for binary treatments and
`compute\_qini\_curves\_multi\_arm()` for multi-arm treatments instead.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
compute_qini_curves(tau_hat, Y, W = NULL, W_multi = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{tau\_hat}] Matrix or vector of estimated treatment effects.

\item[\code{Y}] Vector of observed outcomes.

\item[\code{W}] Vector of treatment assignments for binary treatment.

\item[\code{W\_multi}] Factor of treatment assignments for multi-arm treatment.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame containing Qini curve data for plotting.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{compute\_qini\_curves\_binary}{compute.Rul.qini.Rul.curves.Rul.binary}} for binary treatments
\code{\LinkA{compute\_qini\_curves\_multi\_arm}{compute.Rul.qini.Rul.curves.Rul.multi.Rul.arm}} for multi-arm treatments
\end{SeeAlso}
\HeaderA{compute\_qini\_curves\_binary}{Compute Qini Curves for Binary Treatments}{compute.Rul.qini.Rul.curves.Rul.binary}
\keyword{internal}{compute\_qini\_curves\_binary}
%
\begin{Description}
Compute Qini Curves for Binary Treatments
\end{Description}
%
\begin{Usage}
\begin{verbatim}
compute_qini_curves_binary(tau_hat, Y, W, verbose = TRUE)
\end{verbatim}
\end{Usage}
\HeaderA{create\_ordered\_variable}{Create Ordered Variable Based on Quantile Breaks or Custom Breaks with Informative Labels}{create.Rul.ordered.Rul.variable}
%
\begin{Description}
This function divides a numeric variable into categories based on either quantile breaks
or custom-specified breaks, and creates an ordered factor variable with informative labels.
It now includes rich CLI reporting for better user feedback.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_ordered_variable(
  df,
  var_name,
  n_divisions = NULL,
  cutpoint_inclusive = "upper",
  ties.method = NULL,
  custom_breaks = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the variable to be divided into categories.

\item[\code{var\_name}] The name of the variable within the data frame to divide into categories.

\item[\code{n\_divisions}] The number of quantile divisions to create. Required if custom\_breaks is not provided.

\item[\code{cutpoint\_inclusive}] A character string specifying whether cutpoints should be included
in the lower or upper category. Must be either "lower" or "upper". Default is "upper".

\item[\code{ties.method}] A character string specifying how ties should be handled when calculating quantiles.
Must be one of "first", "last", "random", "ordered", or "average".
If NULL (default), it will be set to "last" if cutpoint\_inclusive is "upper",
and "first" if cutpoint\_inclusive is "lower".

\item[\code{custom\_breaks}] A numeric vector of break points to use for categorisation. If provided,
this overrides the quantile-based division specified by n\_divisions.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
The input data frame with an additional column representing the ordered factor variable.
The new column name will be the original variable name with "\_binary" appended if there
are 2 divisions, or "\_cat" otherwise.
\end{Value}
\HeaderA{create\_tau\_hat\_plot}{Create Tau Hat Plot}{create.Rul.tau.Rul.hat.Rul.plot}
\keyword{internal}{create\_tau\_hat\_plot}
%
\begin{Description}
Creates a histogram plot of tau hat values for each treatment comparison.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_tau_hat_plot(tau_hat, outcome)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{tau\_hat}] A matrix of estimated treatment effects.

\item[\code{outcome}] A character string specifying the name of the outcome variable.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object representing the distribution of tau hat values.
\end{Value}
\HeaderA{create\_transition\_matrix}{Create transition matrix for state transitions}{create.Rul.transition.Rul.matrix}
\keyword{internal}{create\_transition\_matrix}
%
\begin{Description}
To satisify the positivity assumption of causal inference, we must ensure that the exposure occurs in the data. This function computes a transition matrix for a given state variable across subjects, tracking changes between consecutive observations. The function handles both numeric and factor state variables, excluding NA values in the transition count.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_transition_matrix(data, state_var, id_var)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the observations.

\item[\code{state\_var}] The name of the state variable column in `data` as a string. This variable tracks the state changes to be analyzed.

\item[\code{id\_var}] The name of the identifier variable column in `data` as a string. This variable distinguishes between different subjects or entities.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A matrix indicating the number of transitions between states. The rows represent the initial state ('from'), and the columns represent the subsequent state ('to'). Diagonal entries indicate the number of times the state did not change, while off-diagonal entries indicate transitions from one state to another.
\end{Value}
%
\begin{Note}
This function explicitly excludes NA values from the transition matrix calculation. It treats numeric state variables by converting them to factors, ensuring a consistent analysis approach for both numeric and factor types.
\end{Note}
%
\begin{Examples}
\begin{ExampleCode}
df <- read.table(header=TRUE, text="
id wave year_measured religion_believe_god
3 0 1 0
3 1 1 1
4 0 1 0
4 1 1 1
5 0 1 1
5 1 1 0")

transition_matrix <- create_transition_matrix(df, "religion_believe_god", "id")
print(transition_matrix)

\end{ExampleCode}
\end{Examples}
\HeaderA{determine\_transformation}{works with transform to original scale}{determine.Rul.transformation}
\keyword{internal}{determine\_transformation}
%
\begin{Description}
works with transform to original scale
\end{Description}
%
\begin{Usage}
\begin{verbatim}
determine_transformation(var_name)
\end{verbatim}
\end{Usage}
\HeaderA{df\_nz}{df\_nz: Example Data Frame}{df.Rul.nz}
\keyword{datasets}{df\_nz}
%
\begin{Description}
This dataset contains simulated data from The New Zealand Attitudes and Values Study.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
data(df_nz)
\end{verbatim}
\end{Usage}
%
\begin{Format}
A data frame with X rows and Y variables:
\begin{description}

\item[age] participant age in years
\item[agreeableness] Score on the agreeableness scale. Includes items such as:
\begin{description}

\item[i.] I sympathize with others' feelings.
\item[ii.] I am not interested in other people's problems. (reversed)
\item[iii.] I feel others' emotions.
\item[iv.] I am not really interested in others. (reversed)

\end{description}


\item[...] Other variables

\end{description}

\end{Format}
%
\begin{Details}
The dataset is licensed under a custom license. You may use the data for non-commercial purposes with appropriate credit, but redistribution of the data in any form, including modified versions, is not permitted to protect privacy.

The code in the margot package is licensed under the GNU General Public License (GPL) v3.0. You can redistribute it and/or modify it under the terms of the GPL as published by the Free Software Foundation. See <http://www.gnu.org/licenses/>.
\end{Details}
%
\begin{Source}
Simulated data. Copyright (c) 2024 margot package contributors.
\end{Source}
%
\begin{Examples}
\begin{ExampleCode}
data(df_nz)
head(df_nz)
\end{ExampleCode}
\end{Examples}
\HeaderA{double\_robust\_marginal}{Double Robust Marginal Estimation and Tabulation}{double.Rul.robust.Rul.marginal}
\keyword{internal}{double\_robust\_marginal}
%
\begin{Description}
This function provides a double robust approach for estimating causal effects. It first computes
marginal effects using the `causal\_contrast\_marginal` function, then processes and tabulates these
results using `tab\_engine\_marginal`. The function handles both continuous and categorical variables,
and allows specification of the type of effect measure for both causal estimation and tabulation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
double_robust_marginal(
  df,
  Y,
  X,
  baseline_vars,
  treat_0,
  treat_1,
  nsims,
  cores,
  family,
  weights = TRUE,
  continuous_X = FALSE,
  splines = FALSE,
  vcov = "HC2",
  verbose = FALSE,
  delta = 1,
  sd = 1,
  new_name,
  estimand = c("ATE", "ATT"),
  type_causal = c("RR", "RD"),
  type_tab = c("RR", "RD")
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the dataset for analysis.

\item[\code{Y}] The name of the outcome variable as a string.

\item[\code{X}] The name of the treatment or exposure variable as a string.

\item[\code{baseline\_vars}] A vector of covariate names included in the model.

\item[\code{treat\_0}] The reference level of the treatment variable.

\item[\code{treat\_1}] The treatment level of the treatment variable.

\item[\code{nsims}] The number of simulations to run, used in bootstrap or Monte Carlo methods.

\item[\code{cores}] The number of processor cores to use for parallel computation.

\item[\code{family}] A description of the error distribution and link function to be used in the model.

\item[\code{weights}] The name of the weights variable in the data frame, or NULL if no weights are to be used.

\item[\code{continuous\_X}] Logical, indicating whether the treatment variable X is continuous.

\item[\code{splines}] Logical, indicating whether to use spline functions for continuous variables.

\item[\code{vcov}] The method to use for variance-covariance estimation.

\item[\code{verbose}] Logical, indicating whether to print detailed output during computation.

\item[\code{delta}] The assumed smallest worthwhile effect, used for E-value calculations in tabulation.

\item[\code{sd}] The standard deviation of the effect estimate, used for E-value calculations.

\item[\code{new\_name}] A new name to assign to the tabulated output, typically describing the variable or model.

\item[\code{estimand}] Specifies the target of the causal inference, such as "ATE" (Average Treatment Effect) or "ATT" (Average Treatment on the Treated).

\item[\code{type\_causal}] The type of effect size (e.g., "RR" for Risk Ratio or "RD" for Risk Difference) to be computed in the causal analysis.

\item[\code{type\_tab}] The type of effect size to be used in the tabulation of results.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing two elements: `causal\_results` with the results from the causal analysis, and
`tab\_results` with the tabulated results including E-values and other statistics.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Assuming you have a dataset `df_ate` and necessary variables defined
results <- double_robust_marginal(
  df = df_ate,
  Y = "t2_kessler_latent_anxiety_z",
  X = "treatment_var",
  baseline_vars = c("age", "gender"),
  treat_1 = "intervention",
  treat_0 = "control",
  nsims = 200,
  cores = 4,
  family = "gaussian",
  weights = TRUE,
  continuous_X = FALSE,
  splines = FALSE,
  estimand = "ATE",
  type_causal = "RD",
  type_tab = "RD",
  vcov = "HC2",
  new_name = "Test Model Effect",
  delta = 1,
  sd = 1
)

\end{ExampleCode}
\end{Examples}
\HeaderA{extract\_qini\_data\_binary}{Extract Qini Data for Binary Treatment Plotting}{extract.Rul.qini.Rul.data.Rul.binary}
\keyword{internal}{extract\_qini\_data\_binary}
%
\begin{Description}
Extract Qini Data for Binary Treatment Plotting
\end{Description}
%
\begin{Usage}
\begin{verbatim}
extract_qini_data_binary(qini_obj, name, max_index, verbose = TRUE)
\end{verbatim}
\end{Usage}
\HeaderA{group\_results\_by\_comparison}{Group Results by Comparison}{group.Rul.results.Rul.by.Rul.comparison}
\keyword{internal}{group\_results\_by\_comparison}
%
\begin{Description}
Groups the results of multi-arm causal forest analysis by comparison levels.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
group_results_by_comparison(results_list)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results\_list}] A list of results from margot\_multi\_arm\_causal\_forest.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list of grouped results by comparison levels.
\end{Value}
\HeaderA{group\_tab}{Group and Annotate Treatment Effect Estimates}{group.Rul.tab}
\keyword{internal}{group\_tab}
%
\begin{Description}
This function arranges and annotates a data frame based on specified
types of treatment effect estimates (RR or RD). It supports a variety of sorting
options including alphabetical, magnitude (ascending or descending), E-value bound
(ascending or descending), custom order, and a default alias for backward compatibility.
It also handles original scale estimates when available.

This function arranges and annotates a data frame based on specified
types of treatment effect estimates (RR or RD). It supports a variety of sorting
options including alphabetical, magnitude (ascending or descending), E-value bound
(ascending or descending), custom order, and a default alias for backward compatibility.
It also handles original scale estimates when available.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
group_tab(
  df,
  type = c("RD", "RR"),
  order = c("alphabetical", "magnitude_desc", "magnitude_asc", "evaluebound_desc",
    "evaluebound_asc", "custom", "default"),
  custom_order = NULL
)

group_tab(
  df,
  type = c("RD", "RR"),
  order = c("alphabetical", "magnitude_desc", "magnitude_asc", "evaluebound_desc",
    "evaluebound_asc", "custom", "default"),
  custom_order = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Data frame containing the variables of interest, or a list containing
the results data frame and label mapping from transform\_to\_original\_scale().

\item[\code{type}] Type of treatment effect to analyze. One of 'RR' (Risk Ratio) or
'RD' (Risk Difference). Defaults to 'RD'.

\item[\code{order}] Sorting option for outcomes. Options are:
\begin{itemize}

\item{} 'alphabetical': sort by outcome name (Aâ€“Z)
\item{} 'magnitude\_desc': sort by absolute effect size, descending (default for 'magnitude')
\item{} 'magnitude\_asc': sort by absolute effect size, ascending
\item{} 'evaluebound\_desc': sort by E-value bound, descending
\item{} 'evaluebound\_asc': sort by E-value bound, ascending
\item{} 'custom': user-defined order (requires custom\_order)
\item{} 'default': alias for 'magnitude\_desc' (deprecated)

\end{itemize}

Default is 'default'.

\item[\code{custom\_order}] Character vector specifying a custom outcome ordering,
used when order = 'custom'. Must contain all outcomes exactly once.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function detects whether `df` is a list output from transform\_to\_original\_scale()
and extracts `results\_df` and `label\_mapping` accordingly. It then ensures an `outcome`
column, applies any label mapping, and sorts based on the chosen `order`. New options
'magnitude\_desc' and 'magnitude\_asc' sort by absolute effect size; 'evaluebound\_desc'
and 'evaluebound\_asc' sort by the E-Value bound; 'alphabetical' sorts by outcome
name; 'custom' respects a user-provided vector; 'default' is an alias for 'magnitude\_desc'.

The function detects whether `df` is a list output from transform\_to\_original\_scale()
and extracts `results\_df` and `label\_mapping` accordingly. It then ensures an `outcome`
column, applies any label mapping, and sorts based on the chosen `order`. New options
'magnitude\_desc' and 'magnitude\_asc' sort by absolute effect size; 'evaluebound\_desc'
and 'evaluebound\_asc' sort by the E-Value bound; 'alphabetical' sorts by outcome
name; 'custom' respects a user-provided vector; 'default' is an alias for 'magnitude\_desc'.
\end{Details}
%
\begin{Value}
A data frame arranged according to `order`, annotated with:
\begin{itemize}

\item{} Estimate category (positive, negative, not reliable)
\item{} Formatted label for the effect and confidence interval
\item{} Optional original-scale label if \_original columns are present

\end{itemize}


A data frame arranged according to `order`, annotated with:
\begin{itemize}

\item{} Estimate category (positive, negative, not reliable)
\item{} Formatted label for the effect and confidence interval
\item{} Optional original-scale label if \_original columns are present

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# descending magnitude (default for 'default')
result_df <- group_tab(df = analysis_df, order = 'default')

# ascending magnitude
result_df <- group_tab(df = analysis_df, order = 'magnitude_asc')

# strongest E-value bound first
result_df <- group_tab(df = analysis_df, order = 'evaluebound_desc')

# alphabetical
result_df <- group_tab(df = analysis_df, order = 'alphabetical')

# custom ordering
custom_order <- c('Outcome3','Outcome1','Outcome2')
result_df <- group_tab(df = analysis_df, order = 'custom', custom_order = custom_order)

# descending magnitude (default for 'default')
result_df <- group_tab(df = analysis_df, order = 'default')

# ascending magnitude
result_df <- group_tab(df = analysis_df, order = 'magnitude_asc')

# strongest E-value bound first
result_df <- group_tab(df = analysis_df, order = 'evaluebound_desc')

# alphabetical
result_df <- group_tab(df = analysis_df, order = 'alphabetical')

# custom ordering
custom_order <- c('Outcome3','Outcome1','Outcome2')
result_df <- group_tab(df = analysis_df, order = 'custom', custom_order = custom_order)

\end{ExampleCode}
\end{Examples}
\HeaderA{here\_read}{Read Data Frame or Object from RDS File in a Specified Directory}{here.Rul.read}
%
\begin{Description}
Reads an RDS file specified by `name` from a directory defined by `dir\_path` or `push\_mods`, returning the data frame or object stored within.
If no .rds file is found, it searches for the file without the .rds extension.
This function uses the `here` package to resolve the path, ensuring that file paths are built in a consistent and platform-independent manner.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
here_read(name, dir_path = NULL, quiet = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{name}] Character string specifying the name of the file to be read (with or without the ".rds" extension).

\item[\code{dir\_path}] Character string specifying the directory path from which the file will be read. If NULL (default), uses `push\_mods`.

\item[\code{quiet}] Logical. If TRUE, suppresses console output. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
If `dir\_path` is NULL, the `push\_mods` variable must be defined in the user's environment or within the package, pointing to the directory from where files are to be read.
This function will first try to read an .rds file. If not found, it will attempt to read the file without the .rds extension.
\end{Details}
%
\begin{Value}
The data frame or R object stored in the file.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Assuming `push_mods` is set in your environment to "~/mydata"
# and you have previously saved an RDS file named "my_df.rds" in that directory
my_df <- here_read("my_df")

# Reading from a custom directory
my_df <- here_read("my_df", dir_path = "~/custom_dir")

\end{ExampleCode}
\end{Examples}
\HeaderA{here\_read\_arrow}{Read Data Frame from Parquet File in a Specified Directory (Deprecated)}{here.Rul.read.Rul.arrow}
\keyword{internal}{here\_read\_arrow}
%
\begin{Description}
This function is deprecated and will be removed in future releases.
For reading data frames, consider using the `here\_read\_qs` function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
here_read_arrow(name)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{name}] Character string specifying the name of the Parquet file to be read.
\end{ldescription}
\end{Arguments}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
my_df <- here_read_arrow("my_dataset")

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{here\_read\_qs}{Read Data Frame or Object from qs File in a Specified Directory}{here.Rul.read.Rul.qs}
%
\begin{Description}
Reads a `.qs` file specified by `name` from a directory defined by `dir\_path`, returning the data frame or object stored within.
This function uses the `qs` package to efficiently read `.qs` files and the `here` package to construct the file path in a consistent, platform-independent manner.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
here_read_qs(name, dir_path = NULL, nthreads = 1, quiet = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{name}] Character string specifying the name of the `.qs` file to be read (without the ".qs" extension).

\item[\code{dir\_path}] Character string specifying the directory path from which the file will be read. If NULL (default), uses `push\_mods`.

\item[\code{nthreads}] Integer specifying the number of threads to use for decompression. Default is 1.

\item[\code{quiet}] Logical. If TRUE, suppresses console output. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
If `dir\_path` is NULL, the `push\_mods` variable must be defined in the user's environment or within the package, pointing to the directory from where files are to be read.
This function will throw an error if the specified file does not exist or cannot be read as a `.qs` file.
\end{Details}
%
\begin{Value}
A data frame or object representing the data stored in the specified `.qs` file.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Assuming `push_mods` is set in your environment to "~/mydata"
# and you have previously saved a `.qs` file named "my_dataset.qs" in that directory
my_df <- here_read_qs("my_dataset")

# Reading from a custom directory with multiple threads
my_df <- here_read_qs("my_dataset", dir_path = "~/custom_dir", nthreads = 4)

\end{ExampleCode}
\end{Examples}
\HeaderA{here\_save}{Save Data Frame as RDS File in a Specified Directory}{here.Rul.save}
%
\begin{Description}
Saves the provided data frame as an RDS file using the specified name, within a directory defined by `push\_mods`
This function uses the `here` package to construct the path, ensuring that file paths are built in a consistent and platform-independent manner.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
here_save(df, name, dir_path = NULL, compress = TRUE, quiet = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Data frame or object to be saved. This is the object you want to persist on disk.

\item[\code{name}] Character string specifying the base name of the file. The ".rds" extension will be automatically appended to this name.

\item[\code{dir\_path}] Character string specifying the directory path where the file will be saved. If NULL (default), uses `push\_mods`.

\item[\code{compress}] Logical or character string specifying the type of compression to use. See `?saveRDS` for details. Default is TRUE.

\item[\code{quiet}] Logical. If TRUE, suppresses console output. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
If `dir\_path` is NULL, the `push\_mods` variable should be defined in the user's environment or within the package and should point to the directory where files will be saved.
It is assumed that the specified directory exists. This function does not create directories.
\end{Details}
%
\begin{Value}
Invisibly returns the full path to the saved file.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# assuming `push_mods` is set in your environment to "~/mydata"
my_df <- data.frame(x = 1:5, y = letters[1:5])
here_save(my_df, "my_df")

# specifying a custom directory
here_save(my_df, "my_df", dir_path = "~/custom_dir", compress = "xz")

\end{ExampleCode}
\end{Examples}
\HeaderA{here\_save\_arrow}{Save Data Frame to Parquet File in a Specified Directory (Deprecated)}{here.Rul.save.Rul.arrow}
\keyword{internal}{here\_save\_arrow}
%
\begin{Description}
This function is deprecated and will be removed in future releases.
For saving data frames, consider using the `here\_save\_qs` function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
here_save_arrow(df, name)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Data frame to be saved.

\item[\code{name}] Character string specifying the base name of the file.
\end{ldescription}
\end{Arguments}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
my_df <- data.frame(x = 1:5, y = letters[1:5])
here_save_arrow(my_df, "my_saved_dataframe")

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{here\_save\_qs}{Save Data Frame or Object to qs File in a Specified Directory with Enhanced Compression}{here.Rul.save.Rul.qs}
%
\begin{Description}
Saves the provided data frame or object as a `.qs` file using the specified name, within a directory defined by `dir\_path`.
This function leverages the `qs` package to write data to `.qs` format with enhanced compression for efficient storage and quick access in R.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
here_save_qs(obj, name, dir_path, preset = "high", nthreads = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{obj}] Data frame or object to be saved. This is the object you want to persist to disk in `.qs` format.

\item[\code{name}] Character string specifying the base name of the file (without the ".qs" extension).

\item[\code{dir\_path}] Character string specifying the directory path where the file will be saved.

\item[\code{preset}] Character string specifying the compression preset. Default is "high" for better compression.

\item[\code{nthreads}] Integer specifying the number of threads to use for compression. Default is 1.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The `dir\_path` argument must point to an existing directory. The function does not create directories; it assumes that the specified directory already exists.
The function uses enhanced compression settings by default to minimize file size.
\end{Details}
%
\begin{Value}
Invisible NULL. The primary effect of this function is the side effect of writing a `.qs` file to disk.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
my_df <- data.frame(x = 1:5, y = letters[1:5])
here_save_qs(my_df, "my_saved_dataframe", "~/mydata")

\end{ExampleCode}
\end{Examples}
\HeaderA{impute\_and\_combine}{Perform multiple imputation on a list of data frames and combine the results}{impute.Rul.and.Rul.combine}
%
\begin{Description}
This function takes a list of data frames, performs multiple imputation to fill in missing
values using the 'mice' package, and combines the imputed datasets into a single dataset.
The imputations are performed separately for each data frame in the list, and the results
are combined into a 'mids' object, which is then cleaned and returned.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
impute_and_combine(
  list_df,
  m = 10,
  exclude_vars = c("t0_sample_frame", "id", "t0_sample_origin_names_combined")
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{list\_df}] A list containing data frames on which to perform multiple imputation.

\item[\code{m}] The number of multiple imputations to perform for each data frame.

\item[\code{exclude\_vars}] A vector of variable names to be excluded from the imputation model.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame that combines all imputed datasets, with unnecessary columns removed
and row names reset.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
  # Assuming list_df is a list of data frames with missing values
  imputed_data <- impute_and_combine(list_df, m = 5)
  print(imputed_data)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{lmtp\_evalue\_tab}{Calculate E-values for LMTP Output}{lmtp.Rul.evalue.Rul.tab}
%
\begin{Description}
This function takes the output from `margot\_tab\_lmtp()`, which contains estimates of treatment effects,
and calculates E-values to assess the robustness of the estimates to potential unmeasured confounding.
E-values quantify the minimum strength of association, on the risk ratio scale, that an unmeasured
confounder would need to have with both the treatment and the outcome, to fully explain away the
observed association. The function supports both risk differences (RD) and risk ratios (RR) scales.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmtp_evalue_tab(x, delta = 1, sd = 1, scale = c("RD", "RR"))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A data frame output from `margot\_tab\_lmtp()` containing the estimates of interest.

\item[\code{delta}] The hypothesized increase in outcome, used only when `scale` is "RD". Default is 1.

\item[\code{sd}] The standard deviation of the outcome, used only when `scale` is "RD". Default is 1.

\item[\code{scale}] A character string indicating the scale of the estimate: "RD" for risk difference,
or "RR" for risk ratio. Default is "RD".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame similar to `x`, with additional columns for E-Value and its lower bound, excluding
the 'standard\_error' column.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming 'tab_contrast_hours_charity_z_null' is a data frame output from `margot_lmtp_tab()`
lmtp_evalue_tab(tab_contrast_hours_charity_z_null, scale = "RD")
lmtp_evalue_tab(tab_contrast_hours_charity_z_null, scale = "RR")

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_adjust\_weights}{Adjust Weights for Censoring and Sample Design with Progress Reporting}{margot.Rul.adjust.Rul.weights}
%
\begin{Description}
This function calculates and adjusts weights for censoring, combining them with
sample weights if provided. It also offers options for trimming and normalising
the resulting weights. Progress is reported using the cli package.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_adjust_weights(
  pscore,
  censoring_indicator,
  sample_weights = NULL,
  trim = TRUE,
  normalize = TRUE,
  lower_percentile = 0.01,
  upper_percentile = 0.99,
  na.rm = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{pscore}] Numeric vector of predicted probabilities from a censoring model.
Values must be between 0 and 1.

\item[\code{censoring\_indicator}] Logical vector or 0/1 numeric vector indicating
censoring status (TRUE/1 if censored, FALSE/0 if not).

\item[\code{sample\_weights}] Optional numeric vector of sample weights.

\item[\code{trim}] Logical; whether to trim weights (default is TRUE).

\item[\code{normalize}] Logical; whether to normalise weights (default is TRUE).

\item[\code{lower\_percentile}] Numeric; lower percentile for trimming (default is 0.01).

\item[\code{upper\_percentile}] Numeric; upper percentile for trimming (default is 0.99).

\item[\code{na.rm}] Logical; whether to remove NA values (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing adjusted weights and summary statistics.
\end{Value}
\HeaderA{margot\_amelia\_to\_mice}{convert an amelia object to a mice object}{margot.Rul.amelia.Rul.to.Rul.mice}
\keyword{internal}{margot\_amelia\_to\_mice}
%
\begin{Description}
convert an amelia object to a mice object
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_amelia_to_mice(amelia_obj)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{amelia\_output}] an object of class `amelia`, containing imputed datasets from the Amelia package
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a `mids` object compatible with the `mice` package, structured with the original dataset and imputed values
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# load Amelia package and perform imputation
library(Amelia)
data(africa) # example dataset from Amelia package
amelia_output <- amelia(x = africa, m = 5, idvars = "country") # impute data

# convert amelia object to mice object
mids_obj <- margot_amelia_to_mice(amelia_output)

# verify mids object
print(mids_obj)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_back\_transform\_log\_z}{Create Z-score to Original Scale Mapping for Log-Transformed Data}{margot.Rul.back.Rul.transform.Rul.log.Rul.z}
%
\begin{Description}
This function creates a data frame that maps standard z-scores to their
corresponding values on the original data scale for log-transformed data.
It uses the `back\_transform\_log\_z` function to perform the back-transformation
and presents the results in a tidy data frame.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_back_transform_log_z(
  log_mean,
  log_sd,
  z_scores = c(-2, -1, -0.5, 0, 0.5, 1, 2),
  label = "data_scale"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{log\_mean}] The mean of the log-transformed dataset from which the z-scores were calculated.

\item[\code{log\_sd}] The standard deviation of the log-transformed dataset from which the z-scores were calculated.

\item[\code{z\_scores}] Optional vector of z-scores to transform. Defaults to c(-2, -1, -0.5, 0, 0.5, 1, 2)
representing common points in a normal distribution.

\item[\code{label}] Optional string to label the data scale column. Defaults to "data\_scale".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with two columns: z\_score and the original data scale values.
The name of the second column will be the value of the `label` parameter.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Get mean and sd from original log-transformed income data
log_mean_inc <- mean(original_df$t0_log_household_inc, na.rm = TRUE)
log_sd_inc <- sd(original_df$t0_log_household_inc, na.rm = TRUE)

# Create mapping table with default z-scores
income_mapping <- margot_back_transform_log_z(
  log_mean = log_mean_inc,
  log_sd = log_sd_inc,
  label = "household_income"
)
print(income_mapping)

# Create mapping with custom z-scores
custom_mapping <- margot_back_transform_log_z(
  log_mean = log_mean_inc,
  log_sd = log_sd_inc,
  z_scores = c(-1, 0, 1),
  label = "household_income"
)
print(custom_mapping)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_batch\_policy}{Batch Processing of Policy Trees and Related Visualizations (Deprecated)}{margot.Rul.batch.Rul.policy}
\keyword{internal}{margot\_batch\_policy}
%
\begin{Description}
This function is deprecated as of margot 0.2.1.65. Please use margot\_policy() instead.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_batch_policy(
  result_outcomes,
  policy_tree_args = list(),
  decision_tree_args = list(),
  dpi = 600,
  width = 12,
  height = 12,
  save_plots = TRUE,
  output_dir = here::here(push_mods),
  spend = c(0.2, 0.5),
  label_mapping = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{result\_outcomes}] A list containing the results from margot\_multi\_arm\_causal\_forest().

\item[\code{policy\_tree\_args}] A list of arguments to pass to margot\_plot\_policy\_tree(). Default is list().

\item[\code{decision\_tree\_args}] A list of arguments to pass to margot\_plot\_decision\_tree(). Default is list().

\item[\code{dpi}] The resolution of saved plots in dots per inch. Default is 600.

\item[\code{width}] The width of saved plots in inches. Default is 12.

\item[\code{height}] The height of saved plots in inches. Default is 12.

\item[\code{save\_plots}] Logical indicating whether to save plots to disk. Default is TRUE.

\item[\code{output\_dir}] The directory to save plots in. Default is here::here(push\_mods).

\item[\code{spend}] A vector of spend levels to use for difference gain summaries. Default is c(0.2, 0.5).

\item[\code{label\_mapping}] Optional named list for custom label mappings. Keys should be original variable names
(with or without "model\_" prefix), and values should be the desired display labels. Default is NULL.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list where each element corresponds to a model in the input
`result\_outcomes`. Each element is itself a list containing:
\begin{ldescription}
\item[\code{policy\_tree\_plot}] A ggplot object of the policy tree plot
\item[\code{policy\_tree\_interpretation}] A character string interpreting the policy tree
\item[\code{qini\_plot}] A ggplot object of the Qini plot
\item[\code{decision\_tree\_visualisation}] A ggplot object visualizing the decision tree
\item[\code{policy\_combo\_plot}] A ggplot object of the policy combo plot
\item[\code{diff\_gain\_summaries}] A nested list containing difference gain summaries for each spend level
\end{ldescription}
\end{Value}
\HeaderA{margot\_bind\_models}{Combine multiple batched model outputs (with covariates \& metadata)}{margot.Rul.bind.Rul.models}
%
\begin{Description}
This function combines either "causal forest" or "lmtp" batched model outputs
into a single object, provided they share compatible structures.  It now also
preserves `covariates`, `data`, `weights`, and any flip or rescue metadata
when binding causal forest outputs.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_bind_models(..., quiet = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{...}] One or more batched model output objects.

\item[\code{quiet}] Logical; if TRUE, suppresses CLI feedback messages. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A single combined model output object.
\end{Value}
\HeaderA{margot\_bind\_tables}{Bind and format domain-specific tables or a single table}{margot.Rul.bind.Rul.tables}
%
\begin{Description}
This function binds a named list of domain-specific tables, or handles a single data frame.
It optionally sorts by the raw E\_Val\_bound column, renames columns, highlights rows above a threshold,
and groups rows by domain when provided. Works for latex, html, or markdown output.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_bind_tables(
  tables_list,
  sort_E_val_bound = c("none", "asc", "desc"),
  e_val_bound_threshold = 1.1,
  highlight_color = "yellow",
  bold = TRUE,
  output_format = "markdown",
  rename_cols = TRUE,
  col_renames = list(`E-Value` = "E_Value", `E-Value bound` = "E_Val_bound"),
  rename_ate = FALSE,
  threshold_col = "E_Val_bound",
  kbl_args = list(booktabs = TRUE, caption = "Combined Results", align = NULL)
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{tables\_list}] a named list of data frames, each a domain table, or a single data frame.
For lists, row names are captured as an "Outcome" column; for data frames, same applies.

\item[\code{sort\_E\_val\_bound}] character: one of "none", "asc", or "desc". default "none".

\item[\code{e\_val\_bound\_threshold}] numeric; threshold for highlighting rows. default 1.1.

\item[\code{highlight\_color}] character or NULL; background colour for highlighted rows. default "yellow".

\item[\code{bold}] logical; whether to bold highlighted rows. default TRUE.

\item[\code{output\_format}] character; one of "latex", "html", or "markdown". default "markdown".

\item[\code{rename\_cols}] logical; whether to apply column renaming. default TRUE.

\item[\code{col\_renames}] named list; mapping new\_name = old\_name for renaming.
default list("E-Value" = "E\_Value", "E-Value bound" = "E\_Val\_bound").

\item[\code{rename\_ate}] logical; if TRUE, renames the effect column to "ATE". default FALSE.

\item[\code{threshold\_col}] character; name of the (pre-rename) column to test. default "E\_Val\_bound".

\item[\code{kbl\_args}] list; extra arguments for kableExtra::kbl(). default list(booktabs=TRUE, caption="Combined Results", align=NULL).
\end{ldescription}
\end{Arguments}
\HeaderA{margot\_causal\_forest}{Run Multiple Generalized Random Forest (GRF) Causal Forest Models with Enhanced Qini Cross-Validation}{margot.Rul.causal.Rul.forest}
%
\begin{Description}
This function runs multiple GRF causal forest models with enhanced features. In addition to estimating
causal effects, it can compute the Rank-Weighted Average Treatment Effect (RATE) for each model. It also
gives you the option to train a separate "Qini forest" on a subset of data and compute Qini curves on
held-out data, thereby avoiding in-sample optimism in the Qini plots.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_causal_forest(
  data,
  outcome_vars,
  covariates,
  W,
  weights,
  grf_defaults = list(),
  save_data = FALSE,
  compute_rate = TRUE,
  top_n_vars = 15,
  save_models = TRUE,
  train_proportion = 0.7,
  qini_split = TRUE,
  qini_train_prop = 0.7,
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing all necessary variables.

\item[\code{outcome\_vars}] A character vector of outcome variable names to be modelled.

\item[\code{covariates}] A matrix of covariates to be used in the GRF models.

\item[\code{W}] A vector of binary treatment assignments.

\item[\code{weights}] A vector of weights for the observations.

\item[\code{grf\_defaults}] A list of default parameters for the GRF models.

\item[\code{save\_data}] Logical indicating whether to save data, covariates, and weights. Default is FALSE.

\item[\code{compute\_rate}] Logical indicating whether to compute RATE for each model. Default is TRUE.

\item[\code{top\_n\_vars}] Integer specifying the number of top variables to use for additional computations. Default is 15.

\item[\code{save\_models}] Logical indicating whether to save the full GRF model objects. Default is TRUE.

\item[\code{train\_proportion}] Numeric value between 0 and 1 indicating the proportion of non-missing data to use for
training policy trees. Default is 0.7.

\item[\code{qini\_split}] Logical indicating whether to do a separate train/test split exclusively for the Qini
calculation. Default is TRUE (i.e., Qini is computed out-of-sample).

\item[\code{qini\_train\_prop}] Proportion of data to use for the Qini training set (if \code{qini\_split=TRUE}). Default is 0.7.

\item[\code{verbose}] Logical indicating whether to display detailed messages during execution. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing model results, a combined table, and other relevant information.
\end{Value}
\HeaderA{margot\_causal\_forest\_parallel}{Run Multiple Generalised Random Forest (GRF) Causal Forest Models in Parallel}{margot.Rul.causal.Rul.forest.Rul.parallel}
\keyword{internal}{margot\_causal\_forest\_parallel}
%
\begin{Description}
Parallelised, diagnosticâ€‘rich variant of `margot\_causal\_forest()`.  Each
outcomeâ€‘specific forest is estimated in its own R worker via **future**.  All
the `cli` messages and checks from the sequential original are preserved, so
you still get the same granular reporting (dimension checks, Qini status,
warnings, etc.).  Live progress bars are emitted with **progressr** using a
`cli` handler.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_causal_forest_parallel(
  data,
  outcome_vars,
  covariates,
  W,
  weights,
  grf_defaults = list(),
  save_data = FALSE,
  compute_rate = TRUE,
  top_n_vars = 15,
  save_models = TRUE,
  train_proportion = 0.7,
  qini_split = TRUE,
  qini_train_prop = 0.7,
  n_cores = future::availableCores() - 1,
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing all necessary variables.

\item[\code{outcome\_vars}] A character vector of outcome variable names to be modelled.

\item[\code{covariates}] A matrix of covariates to be used in the GRF models.

\item[\code{W}] A vector of binary treatment assignments.

\item[\code{weights}] A vector of weights for the observations.

\item[\code{grf\_defaults}] A list of default parameters for the GRF models.

\item[\code{save\_data}] Logical indicating whether to save data, covariates, and weights. Default is FALSE.

\item[\code{compute\_rate}] Logical indicating whether to compute RATE for each model. Default is TRUE.

\item[\code{top\_n\_vars}] Integer specifying the number of top variables to use for additional computations. Default is 15.

\item[\code{save\_models}] Logical indicating whether to save the full GRF model objects. Default is TRUE.

\item[\code{train\_proportion}] Numeric value between 0 and 1 indicating the proportion of non-missing data to use for
training policy trees. Default is 0.7.

\item[\code{qini\_split}] Logical indicating whether to do a separate train/test split exclusively for the Qini
calculation. Default is TRUE (i.e., Qini is computed out-of-sample).

\item[\code{qini\_train\_prop}] Proportion of data to use for the Qini training set (if \code{qini\_split=TRUE}). Default is 0.7.

\item[\code{n\_cores}] integer. number of parallel workers (default = all cores âˆ’ 1).

\item[\code{verbose}] Logical indicating whether to display detailed messages during execution. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Messages produced inside workers are captured by **future** and
dispatched to the master session.  Progress bars update in real time.  To
silence progress, call `progressr::handlers("off")` before running.
\end{Details}
%
\begin{Value}
list with elements:
* `results`         â€“ perâ€‘outcome diagnostics and objects
* `combined\_table`  â€“ rbindâ€‘ed eâ€‘value table across outcomes
* `outcome\_vars`    â€“ vector of (successful) outcome names
* `not\_missing`     â€“ indices of completeâ€‘case rows
* (`data`, `covariates`, `weights`) when `save\_data = TRUE`
* `full\_models` when `save\_models = TRUE`
\end{Value}
\HeaderA{margot\_combine\_results}{Combine Multiple Results Tables from margot\_plot into a Single Formatted Table}{margot.Rul.combine.Rul.results}
\keyword{internal}{margot\_combine\_results}
%
\begin{Description}
Takes multiple results tables from `margot\_plot` objects and combines them into a single
formatted table using kableExtra, with optional group headers for each section.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_combine_results(
  results,
  options = NULL,
  format = "latex",
  digits = 2,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A named list of data frames, typically extracted from `margot\_plot` objects
using `\$transformed\_table`. Names will be used as section headers if no options
are provided.

\item[\code{options}] Optional list of options created by `margot\_plot\_create\_options()`. Should
have the same names as the results list. Each option object can include a 'subtitle'
that will be used as the section header.

\item[\code{format}] Output format for kable. Default is "latex".

\item[\code{digits}] Number of decimal places for rounding numeric values. Default is 2.

\item[\code{...}] Additional arguments passed to kable().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A kable object that can be further customized using kableExtra functions.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Suppose we have domain-based results:
results_list <- list(
  Health = list(
    transformed_table = health_religious_vs_secular$transformed_table,
    interpretation = health_religious_vs_secular$interpretation
  ),
  Psychological = list(
    transformed_table = psych_religious_vs_secular$transformed_table,
    interpretation = psych_religious_vs_secular$interpretation
  )
)

# And corresponding options:
options_list <- list(
  Health = margot_plot_create_options(
    subtitle = "Health: Religious vs Secular (baseline)",
  ),
  Psychological = margot_plot_create_options(
    subtitle = "Psychological: Religious vs Secular (baseline)",
  )
)

# Combine the results and print:
combined_table <- margot_combine_results(
  results = results_list,
  options = options_list,
  format = "latex",
  booktabs = TRUE,
  longtable = TRUE,
  digits = 2
)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_compare\_groups}{Compare subgroups from a causal forest model.}{margot.Rul.compare.Rul.groups}
%
\begin{Description}
Compare two sets of treatment-effect estimates and report their differences.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_compare_groups(
  group,
  subgroup,
  type = c("RD", "RR"),
  label_mapping = NULL,
  decimal_places = 3,
  group1_name = "Group 1",
  group2_name = "Group 2"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{group}] data frame with treatment effects and confidence intervals for primary group

\item[\code{subgroup}] data frame with treatment effects and confidence intervals for contrast group

\item[\code{type}] Character; one of `"RD"` or `"RR"` (default: both allowed)

\item[\code{label\_mapping}] Optional named vector for nicer row labels

\item[\code{decimal\_places}] Number of digits for CI formatting (default: 3)

\item[\code{group1\_name}] Character; name to use for primary group (default: "Group 1")

\item[\code{group2\_name}] Character; name to use for contrast group (default: "Group 2")
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with:
- `results`: a data frame with columns `Outcomes` and `Group Differences`
- `interpretation`: a character string summarising reliable differences
\end{Value}
\HeaderA{margot\_compute\_gender\_weights\_by\_wave}{Compute Gender-Based Sample Weights Using Baseline Wave Proportions}{margot.Rul.compute.Rul.gender.Rul.weights.Rul.by.Rul.wave}
%
\begin{Description}
Compute sample weights for gender adjustment based on the baseline wave proportions.

Compute sample weights for gender adjustment based on the baseline wave proportions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_compute_gender_weights_by_wave(
  data,
  male_col = "male",
  wave_col = "wave",
  target_wave,
  target_male_prop = 0.5
)

margot_compute_gender_weights_by_wave(
  data,
  male_col = "male",
  wave_col = "wave",
  target_wave,
  target_male_prop = 0.5
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing gender and wave information.

\item[\code{male\_col}] A character string specifying the column that indicates male gender (1 for male, 0 for female). Default is \code{"male"}.

\item[\code{wave\_col}] A character string specifying the column indicating the wave. Default is \code{"wave"}.

\item[\code{target\_wave}] The value in \code{wave\_col} that identifies the baseline wave.

\item[\code{target\_male\_prop}] A numeric value between 0 and 1 representing the target proportion of males. Default is 0.5.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function calculates the gender proportions in the baseline wave and computes weights so that
the overall sample aligns with the target gender distribution. The same weights are then applied to all rows.


The function computes the sample proportions in the baseline wave and calculates weights by comparing these
proportions with the target proportions. It upweights the underrepresented gender and downweights the overrepresented gender.
The resulting weights are applied to the full dataset.

The function calculates the gender proportions in the baseline wave and computes weights so that
the overall sample aligns with the target gender distribution. The same weights are then applied to all rows.


The function computes the sample proportions in the baseline wave and calculates weights by comparing these
proportions with the target proportions. It upweights the underrepresented gender and downweights the overrepresented gender.
The resulting weights are applied to the full dataset.
\end{Details}
%
\begin{Value}
A numeric vector of sample weights for all rows.

A numeric vector of sample weights for all rows.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
dat <- data.frame(
  id = 1:100,
  male = sample(c(0, 1), 100, replace = TRUE, prob = c(0.7, 0.3)),
  wave = rep(1:2, each = 50)
)
weights <- margot_compute_gender_weights_by_wave(dat, male_col = "male",
                                                 wave_col = "wave",
                                                 target_wave = 1,
                                                 target_male_prop = 0.52)
head(weights)

dat <- data.frame(
  id = 1:100,
  male = sample(c(0, 1), 100, replace = TRUE, prob = c(0.7, 0.3)),
  wave = rep(1:2, each = 50)
)
weights <- margot_compute_gender_weights_by_wave(dat, male_col = "male",
                                                 wave_col = "wave",
                                                 target_wave = 1,
                                                 target_male_prop = 0.52)
head(weights)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_correct\_combined\_table}{Correct a â€œcombined tableâ€ for multiplicity **and** recompute *E*-values}{margot.Rul.correct.Rul.combined.Rul.table}
%
\begin{Description}
`margot\_correct\_combined\_table()` takes the **combined\_table** produced by the
various *margot* models (or by your own code) and
\begin{enumerate}

\item{} widens the confidence interval according to the chosen
familyâ€“wise-error correction, **and**
\item{} recalculates *E*-values (and their lower bounds) so they match the
new interval.

\end{enumerate}

By default it implements the singleâ€“step **Bonferroni** correction at
\eqn{\alpha = 0.05}{} as advocated by VanderWeele \& Mathur (2019).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_correct_combined_table(
  combined_table,
  adjust = c("bonferroni", "holm"),
  alpha = 0.05,
  scale = c("RD", "RR"),
  delta = 1,
  sd = 1
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{combined\_table}] A data frame with *at least* the columns
\begin{itemize}

\item{} `E[Y(1)]-E[Y(0)]` **or** `E[Y(1)]/E[Y(0)]`
\item{} `2.5 

\end{itemize}

Extra columns (e.g. the original *E*-values) are carried through.

\item[\code{adjust}] Multiplicity method: `"bonferroni"` (default) or `"holm"`.
(Those are the two methods with strong FWER control; other
`p.adjust()` flavours are deliberately **not** exposed.)

\item[\code{alpha}] Family-wise error-rate to control. Default `0.05`.

\item[\code{scale}] Scale to use when recomputing the *E*-value.
`"RD"` (risk difference / ATE, **default**) or `"RR"` (risk ratio).

\item[\code{delta}, \code{sd}] Arguments passed to [EValue::evalues.OLS()] when
`scale = "RD"`.  Ignored for `"RR"`.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with the same rows (and order) as `combined\_table`, but
with
\begin{itemize}

\item{} updated `2.5 
\item{} freshly computed `E\_Value` and `E\_Val\_bound`.

\end{itemize}

\end{Value}
%
\begin{Section}{How the correction is applied}

Let \eqn{m}{} be the number of rows (tests).
\begin{itemize}

\item{} **Bonferroni** uses
\deqn{ z^* = \Phi^{-1}\!\bigl(1-\alpha/(2m)\bigr) }{}
and rescales the original half-width.
\item{} **Holm** first step-down adjusts the (two-sided) *p*-value for each
test, then back-calculates a *symmetric* CI whose coverage matches the
adjusted *p*.  Point estimates **never** change.

\end{itemize}

\end{Section}
%
\begin{References}
VanderWeele TJ, Mathur MB (2019).
*Some desirable properties of the Bonferroni correction:
Is the Bonferroni correction really so bad?*
**Am J Epidemiol** 188(3): 617â€“618.
\end{References}
\HeaderA{margot\_count\_dyads}{count dyads in longitudinal data}{margot.Rul.count.Rul.dyads}
%
\begin{Description}
count dyads in longitudinal data
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_count_dyads(
  dat,
  start_wave = 2009,
  end_wave = 2022,
  year_measured_val = 1,
  rel_id_var = "rel_num_l",
  complete_var = "rel_complete",
  prev_wave_counts = c(1, 2, 3, 4)
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dat}] a data frame containing the longitudinal data.

\item[\code{start\_wave}] integer. the first wave to process (default: 2009).

\item[\code{end\_wave}] integer. the last wave to process (default: 2022).

\item[\code{year\_measured\_val}] integer. the value of 'year\_measured' to filter on (default: 1).

\item[\code{rel\_id\_var}] character. the name of the variable indicating relationship dyad (default: "rel\_num\_l").

\item[\code{complete\_var}] character. the name of the variable indicating complete dyad status (default: "rel\_complete").

\item[\code{prev\_wave\_counts}] integer vector. previous wave thresholds to count (default: c(1,2,3,4)).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a tibble with columns for:
- wave: survey wave number
- n\_total: total number of dyads (complete + singleton)
- n\_single: number of singleton dyads
- n\_complete: number of complete dyads
- n\_wave\_1plus: dyads in 1+ previous waves
- n\_wave\_2plus: dyads in 2+ previous waves
- n\_wave\_3plus: dyads in 3+ previous waves
- n\_wave\_4plus: dyads in 4+ previous waves
\end{Value}
\HeaderA{margot\_count\_ids}{count individual participants in longitudinal data}{margot.Rul.count.Rul.ids}
%
\begin{Description}
count individual participants in longitudinal data
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_count_ids(
  dat,
  start_wave = 2009,
  end_wave = 2022,
  prev_wave_counts = c(1, 2, 3, 4),
  opt_in_var = "sample_frame_opt_in",
  opt_in_true = 1,
  opt_in_false = 0
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dat}] a data frame containing the longitudinal data

\item[\code{start\_wave}] integer. the first wave to process (default: 2009)

\item[\code{end\_wave}] integer. the last wave to process (default: 2022)

\item[\code{prev\_wave\_counts}] integer vector. previous wave thresholds to count (default: c(1,2,3,4))

\item[\code{opt\_in\_var}] character. name of the opt-in variable to track (default: "sample\_frame\_opt\_in")

\item[\code{opt\_in\_true}] value indicating opted-in status (default: 1)

\item[\code{opt\_in\_false}] value indicating not opted-in status (default: 0)
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a tibble with columns for:
- wave: survey wave number
- n\_total: cumulative unique participants through current wave
- n\_active: active participants in current wave
- n\_deceased: newly deceased in current wave
- n\_deceased\_total: total deceased through current wave
- n\_returned: participants absent in previous wave but present in earlier waves
- n\_returned\_total: total returnees through current wave
- n\_opt\_in: newly opted-in participants in current wave
- n\_opt\_in\_total: total opted-in participants through current wave
- n\_wave\_1plus: participants in 1+ previous waves
- n\_wave\_2plus: participants in 2+ previous waves
- n\_wave\_3plus: participants in 3+ previous waves
- n\_wave\_4plus: participants in 4+ previous waves
\end{Value}
\HeaderA{margot\_filter}{Filter Data Based on Exposure Variables}{margot.Rul.filter}
%
\begin{Description}
This function filters a dataframe based on the levels of a single factor variable or arranges
the dataframe by identifier if only continuous variables are present.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_filter(dat_wide, exposure_vars, sort_var = "id")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dat\_wide}] Dataframe to filter.

\item[\code{exposure\_vars}] Vector of names of exposure variables to consider.

\item[\code{sort\_var}] Optional; the variable by which to sort the dataframes.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list of dataframes filtered by the levels of the factor variable or arranged by identifier.
\end{Value}
\HeaderA{margot\_flip\_forests}{Flip CATE Estimates and Recalculate Policy Trees for Selected Outcomes}{margot.Rul.flip.Rul.forests}
%
\begin{Description}
This function post-processes the results from margot\_causal\_forest to flip CATE estimates,
RATE results, QINI RATE results, QINI curves, and (by default) recalculates the policy trees
for the specified outcomes. If \code{recalc\_policy = FALSE}, tree regeneration is skipped.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_flip_forests(
  model_results,
  flip_outcomes,
  model_prefix = "model_",
  recalc_policy = TRUE,
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] A list containing the model results from margot\_causal\_forest().

\item[\code{flip\_outcomes}] A character vector of outcome variable names for which CATE estimates should be flipped.

\item[\code{model\_prefix}] A character string indicating the prefix used for model names in the results list. Default is "model\_".

\item[\code{recalc\_policy}] Logical; if TRUE (default) recalculates policy trees for flipped outcomes.

\item[\code{verbose}] Logical indicating whether to display detailed messages during execution. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A modified copy of the model\_results list with flipped CATE estimates and recalculated policy trees.
\end{Value}
\HeaderA{margot\_flip\_forests\_parallel}{Parallel flip of CATE estimates with reproducible RNG and memory guard}{margot.Rul.flip.Rul.forests.Rul.parallel}
\keyword{internal}{margot\_flip\_forests\_parallel}
%
\begin{Description}
Parallel flip of CATE estimates with reproducible RNG and memory guard
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_flip_forests_parallel(
  model_results,
  flip_outcomes,
  model_prefix = "model_",
  recalc_policy = TRUE,
  parallel_policy = FALSE,
  n_policy_cores = future::availableCores() - 1,
  verbose = TRUE,
  n_cores = future::availableCores() - 1,
  max_size_GB = 2,
  seed = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] A list containing the model results from margot\_causal\_forest().

\item[\code{flip\_outcomes}] A character vector of outcome variable names for which CATE estimates should be flipped.

\item[\code{model\_prefix}] A character string indicating the prefix used for model names in the results list. Default is "model\_".

\item[\code{recalc\_policy}] Logical; if TRUE (default) recalculates policy trees for flipped outcomes.

\item[\code{parallel\_policy}] logical. if TRUE, policy-tree refits are parallelised.

\item[\code{n\_policy\_cores}] integer. number of workers for policy-tree refits.

\item[\code{verbose}] Logical indicating whether to display detailed messages during execution. Default is TRUE.

\item[\code{max\_size\_GB}] numeric. globals cap passed to `future.globals.maxSize` (default 2 GB).

\item[\code{seed}] `TRUE` for automatic parallel-safe RNG, or an integer for deterministic streams.
\end{ldescription}
\end{Arguments}
\HeaderA{margot\_get\_labels}{Get display labels for multiple variable names}{margot.Rul.get.Rul.labels}
%
\begin{Description}
Helper to map a vector of variable names to human-readable labels.
Falls back on `transform\_var\_name()` when no explicit mapping is found.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_get_labels(vars, label_map)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{vars}] Character vector of variable names to convert.

\item[\code{label\_map}] Named list mapping variable names to labels (e.g., `label\_mapping\_all`).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
If an entry of `vars` is not present in `label\_map`, this function
calls  `transform\_var\_name()` to auto-generate a label based on naming conventions.
\end{Details}
%
\begin{Value}
Character vector of display labels, in the same order as `vars`.
\end{Value}
\HeaderA{margot\_impute\_carry\_forward}{Impute Missing Values Using Carry Forward in Longitudinal Data}{margot.Rul.impute.Rul.carry.Rul.forward}
%
\begin{Description}
Imputes missing values in longitudinal data by carrying forward previous observations
up to a specified number of time points back. By default, it never imputes data for the final wave
(end-of-study). Optionally, it can create indicator variables for imputed values.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_impute_carry_forward(
  df_wide,
  columns_to_impute,
  max_carry_forward = 1,
  time_point_prefixes = NULL,
  time_point_regex = NULL,
  require_one_observed = TRUE,
  columns_no_future_required = NULL,
  create_na_indicator = TRUE,
  indicator_suffix = "_na",
  indicator_as_suffix = TRUE,
  verbose = TRUE,
  impute_final_wave = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df\_wide}] a wide-format dataframe containing longitudinal data.

\item[\code{columns\_to\_impute}] character vector of base column names to impute (without time prefixes).

\item[\code{max\_carry\_forward}] maximum number of time points to look back for carrying forward values.

\item[\code{time\_point\_prefixes}] optional vector of time point prefixes (e.g., c("t0", "t1", "t2")).

\item[\code{time\_point\_regex}] optional regex pattern to identify time points. Overrides time\_point\_prefixes if provided.

\item[\code{require\_one\_observed}] logical. if TRUE, only impute if at least one value is observed in the present or a following wave.

\item[\code{columns\_no\_future\_required}] character vector of columns that do not require future observations for imputation.
defaults to all columns if require\_one\_observed = FALSE, or none if require\_one\_observed = TRUE.

\item[\code{create\_na\_indicator}] logical. if TRUE, creates indicator variables for imputed values.

\item[\code{indicator\_suffix}] suffix to add to the original column name for the indicator variable (default is "\_na").

\item[\code{indicator\_as\_suffix}] logical. if TRUE, the indicator suffix is added as a suffix; if FALSE, it's added as a prefix.

\item[\code{verbose}] logical. if TRUE, prints progress information.

\item[\code{impute\_final\_wave}] logical. if FALSE (default), the final wave (end-of-study) is never imputed.
if TRUE, the final wave can be imputed like other waves.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a dataframe with imputed values and optional indicator variables.
\end{Value}
\HeaderA{margot\_inspect\_qini}{Inspect qini diagnostics for one or several models}{margot.Rul.inspect.Rul.qini}
%
\begin{Description}
Inspect qini diagnostics for one or several models
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_inspect_qini(
  model_results,
  model_names = NULL,
  test_prop = 0.5,
  propensity_bounds = c(0.05, 0.95),
  seed = 2025
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] list returned by `margot\_causal\_forest()` **with**
`save\_models = TRUE, save\_data = TRUE`.

\item[\code{model\_names}] optional character vector of outcome names
(with or without the `model\_` prefix).  default = *all*.

\item[\code{test\_prop}] fraction of trimmed rows to allocate to the
validation/Qini test set.  default 0.5.

\item[\code{propensity\_bounds}] numeric length-2 vector giving the lower
and upper trimming thresholds for `forest\$W.hat`.  default
c(0.05, 0.95).

\item[\code{seed}] integer for reproducibility.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a tibble of diagnostics (class `"margot\_qini\_diag"`).
\end{Value}
\HeaderA{margot\_interpret\_marginal}{Make Interpretation of ATE Results}{margot.Rul.interpret.Rul.marginal}
\keyword{internal}{margot\_interpret\_marginal}
%
\begin{Description}
helper that assembles a concise markdownâ€‘style interpretation of the results.
when `include\_adjust\_note = FALSE` (the default for a singleâ€‘outcome call
from `margot\_plot()`), statements about multiplicity correction are
suppressed to avoid unnecessary noise.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_interpret_marginal(
  df,
  type = c("RD", "RR"),
  order = c("alphabetical", "magnitude_desc", "magnitude_asc", "evaluebound_desc",
    "evaluebound_asc", "custom", "default"),
  original_df = NULL,
  e_val_bound_threshold = 1,
  adjust = c("none", "bonferroni", "holm"),
  alpha = 0.05,
  include_adjust_note = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{include\_adjust\_note}] logical; if `FALSE`, any reference to adjustment
methods is omitted. default `TRUE`.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
list with one element, `interpretation` (a character string).
\end{Value}
\HeaderA{margot\_interpret\_policy\_batch}{Batch process policy tree interpretations}{margot.Rul.interpret.Rul.policy.Rul.batch}
%
\begin{Description}
This function now accepts a vector of model names to process and produces
a single combined output. The common description is printed once at the top,
followed by each model's specific findings. You can now control whether to
interpret the depth-1 or depth-2 tree via the `max\_depth` argument.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_interpret_policy_batch(
  models,
  model_names = NULL,
  max_depth = 2L,
  save_path = NULL,
  prefix = NULL,
  include_timestamp = FALSE,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{models}] A list containing the results from multi-arm causal forest models.

\item[\code{model\_names}] A character vector of model names to interpret. If NULL, all models are processed.

\item[\code{max\_depth}] Integer, 1 or 2; which saved policy tree to interpret (default 2).

\item[\code{save\_path}] The path where the combined interpretation will be saved. If NULL, nothing is saved.

\item[\code{prefix}] An optional prefix for the filename.

\item[\code{include\_timestamp}] Logical; whether to include a timestamp in the filename (if desired).

\item[\code{...}] Additional arguments to pass to margot\_interpret\_policy\_tree().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A single character string containing the combined markdown output.
\end{Value}
\HeaderA{margot\_interpret\_policy\_tree}{Interpret Policy Tree Results}{margot.Rul.interpret.Rul.policy.Rul.tree}
%
\begin{Description}
This function creates an interpretation of policy tree results from a causal forest or multi-arm causal forest model.
It generates a formatted description of the policy tree, including the main splits and recommended actions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_interpret_policy_tree(
  model,
  model_name,
  max_depth = 2L,
  train_proportion = 0.5,
  custom_action_names = NULL,
  label_mapping = NULL,
  original_df = NULL,
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] A list containing the results from a multi-arm causal forest model.

\item[\code{model\_name}] A string specifying which model's results to interpret.

\item[\code{max\_depth}] Integer, 1 or 2; which stored tree to interpret.

\item[\code{train\_proportion}] Numeric value between 0 and 1 for the proportion of data used for training. Default is 0.5.

\item[\code{custom\_action\_names}] Optional vector of custom names for the actions. Must match the number of actions in the policy tree.

\item[\code{label\_mapping}] Optional list that maps variable names to custom labels.

\item[\code{original\_df}] Optional dataframe with untransformed variables, used to display split values on the data scale.

\item[\code{remove\_tx\_prefix}] Logical indicating whether to remove prefixes like t0\_ from variable names. Default is TRUE.

\item[\code{remove\_z\_suffix}] Logical indicating whether to remove the \_z suffix from variable names. Default is TRUE.

\item[\code{use\_title\_case}] Logical indicating whether to convert variable names to title case. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Invisibly returns a string containing the interpretation; also prints it to the console.
\end{Value}
\HeaderA{margot\_interpret\_qini}{Interpret Qini Results}{margot.Rul.interpret.Rul.qini}
%
\begin{Description}
Interprets Qini results for binary and multi-arm treatments, automatically
detecting treatment type from input data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_interpret_qini(
  multi_batch,
  label_mapping = NULL,
  alpha = 0.05,
  decimal_places = 2,
  model_names = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{multi\_batch}] List from margot\_batch\_policy() with diff\_gain\_summaries

\item[\code{label\_mapping}] Named list mapping model names to readable labels

\item[\code{alpha}] Significance level for confidence intervals (default: 0.05)

\item[\code{decimal\_places}] Decimal places for rounding (default: 2)

\item[\code{model\_names}] Character vector of models to process (optional)
\end{ldescription}
\end{Arguments}
%
\begin{Value}
List with summary\_table, qini\_explanation, reliable\_model\_names, reliable\_model\_ids
\end{Value}
\HeaderA{margot\_interpret\_rate}{Interpret RATE estimates}{margot.Rul.interpret.Rul.rate}
%
\begin{Description}
Produce a compact Markdown summary describing which outcomes show positive,
negative, or inconclusive heterogeneous treatment effects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_interpret_rate(
  rate_df,
  flipped_outcomes = NULL,
  target = "AUTOC",
  adjust_positives_only = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{rate\_df}] A data frame from margot\_rate() or a list containing
rate\_autoc and rate\_qini.

\item[\code{flipped\_outcomes}] Character vector of outcomes inverted during
preprocessing.

\item[\code{target}] Character; either "AUTOC" or "QINI" (ignored when rate\_df
is a list).

\item[\code{adjust\_positives\_only}] Logical; if TRUE, apply multiple testing correction
only to positive RATEs in comparison output. Default FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
If rate\_df is a data frame, a Markdown string. If rate\_df is a list,
returns a list produced by margot\_interpret\_rate\_comparison().
\end{Value}
\HeaderA{margot\_interpret\_rate\_comparison}{Compare and interpret RATE estimates from AUTOC and QINI}{margot.Rul.interpret.Rul.rate.Rul.comparison}
%
\begin{Description}
Compare and interpret RATE estimates from AUTOC and QINI
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_interpret_rate_comparison(
  autoc_df,
  qini_df,
  flipped_outcomes = NULL,
  adjust_positives_only = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{autoc\_df}] Data frame of AUTOC results from margot\_rate().

\item[\code{qini\_df}] Data frame of QINI results from margot\_rate().

\item[\code{flipped\_outcomes}] Character vector of outcomes inverted during preprocessing.

\item[\code{adjust\_positives\_only}] Logical; if TRUE, apply multiple testing correction only to positive RATE estimates (negative outcomes use unadjusted CIs). Default FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with elements:
* comparison: Markdown comparison text
* autoc\_results: Output of margot\_interpret\_rate() for AUTOC
* qini\_results: Output of margot\_interpret\_rate() for QINI
* autoc\_model\_names: Models with positive AUTOC
* qini\_model\_names: Models with positive QINI
* both\_model\_names: Models positive in both
* either\_model\_names: Models positive in either
* not\_excluded\_autoc\_model\_names: AUTOC models not reliably negative
* not\_excluded\_qini\_model\_names: QINI models not reliably negative
* not\_excluded\_both: Models not excluded by both AUTOC and QINI
* not\_excluded\_either: Models not excluded by either AUTOC or QINI
\end{Value}
\HeaderA{margot\_interpret\_table}{Interpret and Describe Causal Effect Estimates Using E-values (Deprecated)}{margot.Rul.interpret.Rul.table}
\keyword{internal}{margot\_interpret\_table}
%
\begin{Description}
`r lifecycle::badge("deprecated")`

This function is deprecated. Please use `margot\_interpret\_marginal()` instead.

This function interprets the output of causal effect analysis, providing textual descriptions
of causal effect estimates. It categorises the strength of evidence for causality based on
E-values and confidence intervals, and generates a detailed interpretation of the effect
estimates according to specified causal scales (i.e., "causal\_difference" or "risk\_ratio")
and estimands. This function supports interpreting results on both the causal difference
and risk ratio scales.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_interpret_table(
  df,
  type = c("RD", "RR"),
  estimand = NULL,
  order = "default",
  original_df = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Data frame containing causal effect estimates, expected to include columns for
outcome names, effect estimates (either differences or ratios), confidence intervals,
E-values, and a summary estimate label. The structure of `df` should align with the specified
`causal\_scale`.

\item[\code{estimand}] Optional character string indicating the type of causal estimand interpreted: "PATE"
(Population Average Treatment Effect), "ATE" (Average Treatment Effect), "ATT" (Average
Treatment Effect in the Treated), "CATE" (Conditional Average Treatment Effect), or "LMTP"
(Longitudinal Modified Treatment Policy). Default is NULL.

\item[\code{order}] Character string specifying the order of results. Default is "default".

\item[\code{causal\_scale}] Character string specifying the causal scale used in the analysis.
Currently supports "causal\_difference" for differences in means or medians, and "risk\_ratio"
for comparing ratios of probabilities or risks.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing two elements:
\begin{ldescription}
\item[\code{estimand\_description}] A character string describing the specified estimand, or NULL if no estimand was provided.
\item[\code{interpretation}] A character string containing a detailed interpretation of each outcome in `df`,
including the causal contrast, E-values, and the strength of evidence for causality.
\end{ldescription}
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming `group_tab_output` is the result from a causal analysis
result <- margot_interpret_table(group_tab_output, "causal_difference", "ATE")
cat(result$estimand_description)
cat(result$interpretation)

# Without specifying an estimand
result <- margot_interpret_table(group_tab_output, "risk_ratio")
cat(result$interpretation)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_lmtp}{Batch Process LMTP Models}{margot.Rul.lmtp}
%
\begin{Description}
This function runs multiple Longitudinal Modified Treatment Policy (LMTP) models for specified outcome variables,
calculates contrasts, creates evaluation tables, and optionally saves the complete output.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_lmtp(
  data,
  outcome_vars,
  trt,
  shift_functions = list(),
  include_null_shift = TRUE,
  lmtp_model_type = lmtp::lmtp_tmle,
  contrast_type = c("pairwise", "null"),
  contrast_scale = c("additive", "rr", "or"),
  lmtp_defaults = list(),
  n_cores = parallel::detectCores() - 1,
  save_output = FALSE,
  save_path = here::here("push_mods"),
  base_filename = "lmtp_output",
  use_timestamp = FALSE,
  prefix = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing all necessary variables.

\item[\code{outcome\_vars}] A character vector of outcome variable names to be modeled.

\item[\code{trt}] A character string specifying the treatment variable.

\item[\code{shift\_functions}] A list of shift functions to be applied. Each function should take `data` and `trt` as arguments.

\item[\code{include\_null\_shift}] Logical, whether to include a null shift. Default is TRUE.

\item[\code{lmtp\_model\_type}] The LMTP model function to use. Default is lmtp\_tmle.

\item[\code{contrast\_type}] Type of contrasts to compute: "pairwise" or "null". Default is "pairwise".

\item[\code{contrast\_scale}] Scale for contrasts: "additive", "rr", or "or". Default is "additive".

\item[\code{lmtp\_defaults}] A list of default parameters for the LMTP models.

\item[\code{n\_cores}] Number of cores to use for parallel processing. Default is detectCores() - 1.

\item[\code{save\_output}] Logical, whether to save the complete output. Default is FALSE.

\item[\code{save\_path}] The directory path to save the output. Default is "push\_mods" in the current working directory.

\item[\code{base\_filename}] The base filename for saving the output. Default is "lmtp\_output".

\item[\code{use\_timestamp}] Logical, whether to include a timestamp in the filename. Default is FALSE.

\item[\code{prefix}] Optional prefix to add to the saved output filename. Default is NULL.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing:
\begin{ldescription}
\item[\code{models}] A list of all LMTP models for each outcome and shift function.
\item[\code{contrasts}] A list of contrasts computed for each outcome.
\item[\code{individual\_tables}] A list of individual tables for each contrast and outcome.
\item[\code{combined\_tables}] A list of combined tables for each contrast type across all outcomes.
\end{ldescription}
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assume we have a dataset 'my_data' with variables 'outcome', 'treatment', and some confounders

# Define shift functions
gain_function <- function(data, trt) {
  data[[trt]] + 1
}

loss_function <- function(data, trt) {
  pmax(data[[trt]] - 1, 0)
}

# Run LMTP analysis
result <- margot_lmtp(
  data = my_data,
  outcome_vars = c("outcome1", "outcome2"),
  trt = "treatment",
  shift_functions = list(gain = gain_function, loss = loss_function),
  lmtp_defaults = list(baseline = c("confounder1", "confounder2"),
                       time_vary = c("time_var1", "time_var2"),
                       outcome_type = "continuous"),
  save_output = TRUE,
  save_path = here::here("output", "lmtp_results"),
  prefix = "my_study"
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_lmtp\_evalue}{Combine LMTP Summary and E-Value Calculation}{margot.Rul.lmtp.Rul.evalue}
%
\begin{Description}
This function first creates a summary table from the output of `lmtp::lmtp\_contrast()` using
`margot\_tab\_lmtp`, specifying the desired scale (RD or RR) and a new name for the row. It then calculates
E-values for the estimates in the table to assess the potential impact of unmeasured confounding,
appending these values to the summary table.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_lmtp_evalue(
  lmtp_output,
  scale = c("RD", "RR"),
  new_name = "character_string",
  delta = 1,
  sd = 1
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{lmtp\_output}] The output from `lmtp::lmtp\_contrast()`, to be summarized and analyzed for E-values.

\item[\code{scale}] Character string specifying the scale of the estimate to be used in the summary table and
E-value calculation. Valid options are "RD" (risk difference) or "RR" (risk ratio). Default is "RD".

\item[\code{new\_name}] Character string to name the row in the output summary table, representing the treatment
contrast. This name will be applied to the first row of the summary table.

\item[\code{delta}] The hypothesized increase in outcome for RD scale calculations. Used only when `scale` is "RD".
Default value is 1.

\item[\code{sd}] The standard deviation of the outcome for RD scale calculations. Used only when `scale` is "RD".
Default value is 1.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with the original estimates and their E-values. The table includes columns for the
estimate (either RD or RR), its E-Value, and the E-Value lower bound, excluding the 'standard\_error' column.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{margot\_tab\_lmtp}{margot.Rul.tab.Rul.lmtp}}, \code{\LinkA{lmtp\_evalue\_tab}{lmtp.Rul.evalue.Rul.tab}} for the underlying functions used.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# assuming `contrast_output` is the result from `lmtp::lmtp_contrast()`
summary_evalues <- margot_lmtp_evalue(
  lmtp_output = contrast_output,
  scale = "RD",
  new_name = "Treatment Effect"
)
print(summary_evalues)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_lmtp\_tab}{Summarise LMTP Output into a Data Frame}{margot.Rul.lmtp.Rul.tab}
\keyword{internal}{margot\_lmtp\_tab}
%
\begin{Description}
This function takes the output from `lmtp::lmtp\_contrast()` and creates a data frame summarising the
estimates. It allows for scaling the estimates as either risk differences (RD) or risk ratios (RR).
The resulting data frame includes the estimate, standard error, and 95
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_lmtp_tab(lmtp_output, scale = c("RD", "RR"), new_name = "")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{lmtp\_output}] The output object from `lmtp::lmtp\_contrast()`.

\item[\code{scale}] A character string specifying the scale of the estimate: "RD" or "RR". Default is "RD".

\item[\code{new\_name}] A character string to name the row of the output data frame, representing the treatment
contrast being summarised.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with four columns: the estimate under the specified scale, its standard error,
and the lower and upper bounds of the 95
\end{Value}
\HeaderA{margot\_log\_transform\_vars}{Log-transform Variables in a Data Frame}{margot.Rul.log.Rul.transform.Rul.vars}
%
\begin{Description}
This function applies a log(x + 1) transformation to specified variables in a data frame.
It handles NA values, warns and drops non-numeric selections, allows for exceptions,
and can be applied to variables with specific prefixes.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_log_transform_vars(
  data,
  vars,
  exceptions = character(0),
  prefix = "log_",
  keep_original = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] a data frame to process.

\item[\code{vars}] a character vector of variable names or a tidyselect helper (e.g., starts\_with("hours\_")).

\item[\code{exceptions}] a character vector of variable names to exclude from transformation.

\item[\code{prefix}] a string to prepend to the names of transformed variables. default is "log\_."

\item[\code{keep\_original}] logical; if true, keeps both original and transformed variables. if false, replaces original. default is true.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a data frame with log-transformed variables.
\end{Value}
\HeaderA{margot\_make\_tables}{Create Summary Tables Using table1 with Custom Formatting}{margot.Rul.make.Rul.tables}
%
\begin{Description}
\code{margot\_make\_tables} is a wrapper for \code{table1::table1()} which simplifies the creation of summary tables.
It provides custom variable labelling, formatting, factor conversion, and additional table options.
This function is optimized for \code{"markdown"}, \code{"latex"}, and \code{"flextable"} outputs, with special support for Quarto documents.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_make_tables(
  data,
  vars,
  by,
  labels = NULL,
  factor_vars = NULL,
  table1_opts = list(),
  format = c("markdown", "latex", "flextable"),
  kable_opts = list(),
  flex_opts = list(),
  quarto_label = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A \code{data.frame} containing the dataset.

\item[\code{vars}] A character vector of variable names to include on the left-hand side of the table.

\item[\code{by}] A character vector of variable names to stratify the table by. Supports multiple variables for interactions.

\item[\code{labels}] A named character vector for custom variable labels. Names should correspond to variable names in \code{vars}.

\item[\code{factor\_vars}] An optional character vector of variable names in \code{vars} to convert to factors for frequency tables.

\item[\code{table1\_opts}] A list of additional options to pass to \code{table1::table1()}. For example, \code{list(overall = FALSE, transpose = TRUE)}.

\item[\code{format}] A character string specifying the output format. Options are \code{"markdown"} (default), \code{"latex"}, or \code{"flextable"}.

\item[\code{kable\_opts}] A list of additional options controlling table styling:
\begin{itemize}

\item{} For \code{format = "latex"}, these are passed to \code{kableExtra::kable\_styling()}.
\item{} For \code{format = "markdown"}, currently only for documentation purposes.

\end{itemize}


\item[\code{flex\_opts}] A list of additional options for flextable formatting:
\begin{itemize}

\item{} \code{font\_size}: Font size for the table (default: 9)
\item{} \code{font\_size\_header}: Font size for headers (default: 10)
\item{} \code{theme}: Theme function to apply (default: "theme\_vanilla")
\item{} \code{autofit}: Whether to autofit columns (default: TRUE)
\item{} \code{width}: Table width (0-1 for proportion of page width, default: 1)

\end{itemize}


\item[\code{quarto\_label}] An optional label for Quarto cross-references (e.g., "tbl-demographics"). When specified for LaTeX output,
this adds a \code{\bsl{}label\{\}} command to enable Quarto's cross-referencing system.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A table object formatted for the specified output:
\begin{itemize}

\item{} For \code{format = "latex"}, a kableExtra-formatted LaTeX table with optional Quarto label
\item{} For \code{format = "markdown"}, a markdown-formatted kable table with bold variable names
\item{} For \code{format = "flextable"}, a flextable object optimized for Word output

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
  # Flextable output for Word
  flex_table <- margot_make_tables(
    data = mydata,
    vars = c("age", "gender", "income"),
    by = "group",
    labels = c("age" = "Age", "gender" = "Gender", "income" = "Income"),
    factor_vars = "gender",
    table1_opts = list(overall = FALSE, transpose = TRUE),
    format = "flextable",
    flex_opts = list(font_size = 8)
  )

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_model\_evalue}{Combine Model Summary and E-Value Calculation for Various Causal Models}{margot.Rul.model.Rul.evalue}
%
\begin{Description}
This function creates a summary table from the output of various causal models,
including `lmtp::lmtp\_contrast()`, `grf::causal\_forest()`, and `grf::multi\_arm\_causal\_forest()`.
It calculates E-values for the estimates to assess the potential impact of unmeasured confounding,
appending these values to the summary table.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_model_evalue(
  model_output,
  scale = c("RD", "RR"),
  new_name = "character_string",
  delta = 1,
  sd = 1,
  subset = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_output}] The output from a supported causal model. Supported types include:
\begin{itemize}

\item{} Output from `lmtp::lmtp\_contrast()`
\item{} Output from `grf::causal\_forest()`
\item{} Output from `grf::multi\_arm\_causal\_forest()`
\item{} A data frame with columns 'estimate' and 'std.err'

\end{itemize}


\item[\code{scale}] Character string specifying the scale of the estimate to be used in the summary table and
E-value calculation. Valid options are "RD" (risk difference) or "RR" (risk ratio). Default is "RD".
This parameter is ignored for causal forest models, which always use "RD".

\item[\code{new\_name}] Character string to name the row(s) in the output summary table, representing the treatment
contrast(s). For multi-arm causal forests, this will be combined with the contrast information.

\item[\code{delta}] The hypothesized increase in outcome for RD scale calculations. Used only when `scale` is "RD".
Default value is 1.

\item[\code{sd}] The standard deviation of the outcome for RD scale calculations. Used only when `scale` is "RD".
Default value is 1.

\item[\code{subset}] An optional logical vector for subsetting the data when the model is a `grf` model. Default is `NULL`.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with the original estimates and their E-values. The table includes columns for the
estimate (either RD or RR), its confidence interval, E-Value, and the E-Value lower bound.
For multi-arm causal forests, multiple rows will be returned, one for each contrast.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# For lmtp_contrast output
summary_evalues <- margot_model_evalue(
  model_output = lmtp_contrast_output,
  scale = "RD",
  new_name = "Treatment Effect"
)

# For causal_forest output
cf_summary <- margot_model_evalue(
  model_output = causal_forest_output,
  new_name = "Causal Forest Effect"
)

# For multi_arm_causal_forest output
macf_summary <- margot_model_evalue(
  model_output = multi_arm_cf_output,
  new_name = "Multi-Arm Effect"
)

# For direct input of estimate and standard error
direct_summary <- margot_model_evalue(
  model_output = data.frame(estimate = 0.5, std.err = 0.1),
  new_name = "Direct Effect"
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_model\_tab}{Summarise LMTP or Causal Forest Output into a Data Frame}{margot.Rul.model.Rul.tab}
\keyword{internal}{margot\_model\_tab}
%
\begin{Description}
This function takes the output from `lmtp::lmtp\_contrast()` or a causal forest model and creates a data frame summarising the
estimates. It allows for scaling the estimates as either risk differences (RD) or risk ratios (RR) for LMTP models.
For causal forest models, the scale is always "RD".
The resulting data frame includes the estimate, standard error, and 95
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_model_tab(
  model_output,
  scale = c("RD", "RR"),
  new_name = "character_string"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_output}] The output object from `lmtp::lmtp\_contrast()` or a causal forest model.

\item[\code{scale}] A character string specifying the scale of the estimate. Valid options are "RD" for risk
difference and "RR" for risk ratio. Default is "RD". This parameter is ignored for causal forest models.

\item[\code{new\_name}] A character string to name the row of the output data frame, representing the treatment
contrast being summarised.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with four columns: the estimate under the specified scale, its standard error, and
the lower and upper bounds of the 95
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming `contrast_hours_charity_z_null` is output from `lmtp::lmtp_contrast()`
tab_contrast_hours_charity_z_null <- margot_model_tab(
  contrast_hours_charity_z_null,
  scale = "RD",
  new_name = "relig service: hours volunteer"
)
print(tab_contrast_hours_charity_z_null)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_omnibus\_hetero\_test}{Omnibus Heterogeneity Test for GRF Models}{margot.Rul.omnibus.Rul.hetero.Rul.test}
%
\begin{Description}
This function performs an omnibus heterogeneity test for specified models
outputted from margot::margot\_run\_models\_grf() and provides interpretations in a readable format.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_omnibus_hetero_test(
  model_results,
  outcome_vars = NULL,
  alpha = 0.05,
  detail_level = "standard",
  label_mapping = NULL,
  format = "table"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] A list of model results from margot::margot\_run\_models\_grf().

\item[\code{outcome\_vars}] Optional. A character vector of outcome variable names. If NULL,
the function will attempt to use the outcome\_vars from the model\_results input.

\item[\code{alpha}] Significance level for hypothesis tests. Default is 0.05.

\item[\code{detail\_level}] Character string specifying the level of detail in the output.
Options are "basic", "standard" (default), or "detailed".

\item[\code{label\_mapping}] Optional. A named list mapping outcome variable names to display labels.
For example: list("t2\_agreeableness\_z" = "Agreeableness").

\item[\code{format}] Output format: "table" (default), "markdown", or "text".
"table" returns a tibble for use with tidyverse tools.
"markdown" returns formatted markdown text for Quarto documents.
"text" returns plain text interpretations.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing:
- summary\_table: A tibble with all test results
- interpretations: Results formatted according to the format parameter
- brief\_interpretation: A concise summary of all results
\end{Value}
\HeaderA{margot\_planned\_subgroups\_batch}{Batch process heterogeneity analyses across multiple outcome domains}{margot.Rul.planned.Rul.subgroups.Rul.batch}
%
\begin{Description}
runs the same planned subgroup contrasts (e.g., wealth, ethnicity,
political orientation) over multiple outcome domains, with optional
correction for multiple comparisons.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_planned_subgroups_batch(
  domain_models,
  X,
  base_defaults,
  label_mapping = NULL,
  subset_types,
  original_df,
  domain_names,
  subtitles,
  adjust = c("none", "bonferroni", "holm"),
  alpha = 0.05,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{domain\_models}] list of model sets; one element per outcome domain

\item[\code{X}] model matrix (or data frame) of predictors used when the models were
fitted

\item[\code{base\_defaults}] named list of default arguments passed to the downstream
plotting / helper functions

\item[\code{label\_mapping}] named list of variable-to-label mappings; passed down to `margot\_plot()`

\item[\code{subset\_types}] named list of subset specifications (e.g., list(wealth = subsets\_standard\_wealth))

\item[\code{original\_df}] the raw data frame containing all variables (needed for
label recovery, plotting on the original scale, etc.)

\item[\code{domain\_names}] character vector naming each element in `domain\_models`

\item[\code{subtitles}] character vector of subtitles used in plot annotations;
must be the same length as `domain\_names`

\item[\code{adjust}] character; correction method for multiple comparisons in plots.
one of "none", "bonferroni", or "holm". default: "none".

\item[\code{alpha}] numeric; significance threshold for correction. default: 0.05.

\item[\code{...}] any additional arguments forwarded directly to [`margot\_subset\_batch()`]
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a nested list. the first level is the domain name; the second level
is the subset type. each leaf contains the full list returned by
`margot\_subset\_batch()`.
\end{Value}
\HeaderA{margot\_plot}{Create a Margot Plot with Interpretation}{margot.Rul.plot}
%
\begin{Description}
Create a margot plot for visualising causal effects with flexible sorting,
embed a compact interpretation, and return a transformed table.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot(
  .data,
  type = c("RD", "RR"),
  order = c("alphabetical", "magnitude_desc", "magnitude_asc", "evaluebound_desc",
    "evaluebound_asc", "custom", "default"),
  custom_order = NULL,
  title_binary = NULL,
  include_coefficients = TRUE,
  standardize_label = c("NZ", "US", "none"),
  e_val_bound_threshold = 1.2,
  adjust = c("none", "bonferroni", "holm"),
  alpha = 0.05,
  ...,
  options = list(),
  label_mapping = NULL,
  save_output = FALSE,
  use_timestamp = FALSE,
  base_filename = "margot_plot_output",
  prefix = NULL,
  save_path = here::here("push_mods"),
  original_df = NULL,
  bold_rows = FALSE,
  rename_cols = FALSE,
  col_renames = list(`E-Value` = "E_Value", `E-Value bound` = "E_Val_bound"),
  rename_ate = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{.data}] data frame containing causal effect estimates with columns for
effect sizes, confidence intervals, E-values and E-value bounds

\item[\code{type}] character. type of effect estimate: "RD" (risk difference) or
"RR" (risk ratio). default "RD"

\item[\code{order}] character. sorting option for outcomes:
\begin{itemize}

\item{} 'alphabetical': sort by outcome name (Aâ€“Z)
\item{} 'magnitude\_desc': sort by absolute effect size, descending
\item{} 'magnitude\_asc': sort by absolute effect size, ascending
\item{} 'evaluebound\_desc': sort by E-value bound, descending
\item{} 'evaluebound\_asc': sort by E-value bound, ascending
\item{} 'custom': user-defined order (requires custom\_order)
\item{} 'default': alias for 'magnitude\_desc' (deprecated)

\end{itemize}


\item[\code{custom\_order}] character vector. custom outcome ordering when order = 'custom'

\item[\code{title\_binary}] character. deprecated parameter, kept for compatibility

\item[\code{include\_coefficients}] logical. whether to add numeric labels to plot points

\item[\code{standardize\_label}] character. label style: "NZ", "US", or "none"

\item[\code{e\_val\_bound\_threshold}] numeric. threshold for reliable causal evidence

\item[\code{adjust}] character. multiplicity correction method: "none", "bonferroni", "holm"

\item[\code{alpha}] numeric. significance level for corrections

\item[\code{...}] additional arguments passed to plot options

\item[\code{options}] list. plot styling options

\item[\code{label\_mapping}] named character vector. outcome label mappings

\item[\code{save\_output}] logical. whether to save results to file

\item[\code{use\_timestamp}] logical. whether to add timestamp to saved filename

\item[\code{base\_filename}] character. base name for saved file

\item[\code{prefix}] character. prefix for saved filename

\item[\code{save\_path}] character. directory path for saved file

\item[\code{original\_df}] data frame. original scale data for back-transformation

\item[\code{bold\_rows}] logical. whether to bold rows exceeding E-value threshold

\item[\code{rename\_cols}] logical. whether to rename E-value columns

\item[\code{col\_renames}] named list. column name mappings for renaming

\item[\code{rename\_ate}] logical. whether to rename effect column to "ATE"
\end{ldescription}
\end{Arguments}
%
\begin{Value}
list containing:
\begin{itemize}

\item{} plot: ggplot object with causal effects visualization
\item{} interpretation: character string with results interpretation
\item{} transformed\_table: data frame with formatted results table

\end{itemize}

\end{Value}
\HeaderA{margot\_plot\_boxplot}{Create panel data Boxplots using ggplot2}{margot.Rul.plot.Rul.boxplot}
%
\begin{Description}
This function creates boxplots for one or more variables across specified panel waves.
It offers various customisation options for the plot appearance and layout.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_boxplot(
  data,
  y_vars,
  waves = NULL,
  id_col = "id",
  title = NULL,
  y_label = NULL,
  x_label = "Wave",
  show_points = FALSE,
  point_alpha = 0.05,
  point_size = 0.5,
  include_timestamp = FALSE,
  save_path = NULL,
  prefix = NULL,
  width = 16,
  height = 8,
  legend_position = "bottom",
  y_limits = NULL,
  facet_scales = "free_y",
  facet_ncol = NULL,
  facet_nrow = NULL,
  coord_flip = FALSE,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables to be plotted.

\item[\code{y\_vars}] A list of variable names to be plotted on the y-axis.

\item[\code{waves}] A vector of wave values to include in the plot (default is NULL, which includes all waves).

\item[\code{id\_col}] Name of the column containing unique identifiers (default is "id").

\item[\code{title}] The title of the plot (optional, auto-generated if NULL).

\item[\code{y\_label}] The label for the y-axis (optional).

\item[\code{x\_label}] The label for the x-axis (optional, defaults to "Wave").

\item[\code{show\_points}] Logical, whether to show individual data points (default is FALSE).

\item[\code{point\_alpha}] Alpha value for data points if shown (default is 0.3).

\item[\code{point\_size}] Size of data points if shown (default is 0.5).

\item[\code{include\_timestamp}] Logical, whether to include timestamp in plot title and filename (default is FALSE).

\item[\code{save\_path}] Path to save the plot (optional).

\item[\code{prefix}] Optional prefix for the saved file name (default is NULL).

\item[\code{width}] Width of the saved plot in inches (default is 16).

\item[\code{height}] Height of the saved plot in inches (default is 8).

\item[\code{legend\_position}] Position of the legend (default is "bottom").

\item[\code{y\_limits}] Y-axis limits (optional).

\item[\code{facet\_scales}] Scales for facet panels (default is "free\_y").

\item[\code{facet\_ncol}] Number of columns for facet\_wrap (optional).

\item[\code{facet\_nrow}] Number of rows for facet\_wrap (optional).

\item[\code{coord\_flip}] Logical, whether to flip the coordinates of the plot (default is FALSE).

\item[\code{...}] Additional arguments passed to geom\_boxplot().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object representing the boxplot.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# define outcome variables
outcome_vars <- c(
  "env_climate_chg_concern",
  "env_climate_chg_cause",
  "env_climate_chg_real",
  "env_sat_nz_environment",
  "envefficacy"
)

# basic usage with all waves
p1 <- margot_plot_boxplot(
  data = your_data,
  y_vars = outcome_vars,
  id_col = "id"
)

# plotting specific waves with points shown and coordinates flipped
p2 <- margot_plot_boxplot(
  data = your_data,
  y_vars = outcome_vars,
  waves = c(2021, 2022),
  show_points = TRUE,
  coord_flip = TRUE,
  id_col = "id"
)

# saving the plot with a custom prefix
margot_plot_boxplot(
  data = your_data,
  y_vars = outcome_vars,
  waves = c(2021, 2022, 2023),
  save_path = "path/to/save",
  prefix = "climate_change",
  include_timestamp = TRUE,
  id_col = "id"
)

# customizing the plot appearance with flipped coordinates
p3 <- margot_plot_boxplot(
  data = your_data,
  y_vars = c("env_climate_chg_concern", "envefficacy"),
  waves = c(2021, 2022),
  title = "Climate Change Concern and Efficacy",
  y_label = "Score",
  legend_position = "right",
  facet_scales = "free",
  coord_flip = TRUE,
  id_col = "id"
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_boxplot\_covariate}{create boxplots with covariates using ggplot2}{margot.Rul.plot.Rul.boxplot.Rul.covariate}
%
\begin{Description}
this function creates boxplots for one outcome variable across specified panel waves,
allowing for different groups as covariates. it combines features from
margot\_plot\_boxplot() and margot\_plot\_slope\_covariate().
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_boxplot_covariate(
  data,
  outcome,
  covariate,
  waves = NULL,
  id_col = "id",
  title = NULL,
  y_label = NULL,
  x_label = "Wave",
  color_label = NULL,
  show_points = FALSE,
  point_alpha = 0.05,
  point_size = 0.5,
  include_timestamp = FALSE,
  save_path = NULL,
  prefix = NULL,
  width = 16,
  height = 8,
  legend_position = "right",
  y_limits = NULL,
  coord_flip = FALSE,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] a data frame containing the variables to be plotted.

\item[\code{outcome}] the name of the outcome variable to be plotted.

\item[\code{covariate}] the name of the covariate variable for grouping.

\item[\code{waves}] a vector of wave values to include in the plot (default is NULL, which includes all waves).

\item[\code{id\_col}] name of the column containing unique identifiers (default is "id").

\item[\code{title}] the title of the plot (optional, auto-generated if NULL).

\item[\code{y\_label}] the label for the y-axis (optional).

\item[\code{x\_label}] the label for the x-axis (optional, defaults to "Wave").

\item[\code{color\_label}] the label for the color legend (optional, defaults to the covariate name).

\item[\code{show\_points}] logical, whether to show individual data points (default is FALSE).

\item[\code{point\_alpha}] alpha value for data points if shown (default is 0.05).

\item[\code{point\_size}] size of data points if shown (default is 0.5).

\item[\code{include\_timestamp}] logical, whether to include timestamp in plot title and filename (default is FALSE).

\item[\code{save\_path}] path to save the plot (optional).

\item[\code{prefix}] optional prefix for the saved file name (default is NULL).

\item[\code{width}] width of the saved plot in inches (default is 12).

\item[\code{height}] height of the saved plot in inches (default is 8).

\item[\code{legend\_position}] position of the legend (default is "right").

\item[\code{y\_limits}] y-axis limits (optional).

\item[\code{coord\_flip}] logical, whether to flip the coordinates of the plot (default is FALSE).

\item[\code{...}] additional arguments passed to geom\_boxplot().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a ggplot object representing the boxplot with covariates.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# example 1: basic usage with all waves
p1 <- margot_plot_boxplot_covariate(
  data = your_data,
  outcome = "env_climate_chg_concern",
  covariate = "education",
  id_col = "id"
)

# example 2: plotting specific waves with custom labels
p2 <- margot_plot_boxplot_covariate(
  data = your_data,
  outcome = "political_orientation",
  covariate = "age_group",
  waves = c(2021, 2022, 2023),
  y_label = "Political Orientation",
  color_label = "Age Group",
  id_col = "id"
)

# example 3: showing individual points and flipping coordinates
p3 <- margot_plot_boxplot_covariate(
  data = your_data,
  outcome = "env_sat_nz_environment",
  covariate = "income_bracket",
  show_points = TRUE,
  coord_flip = TRUE,
  y_label = "Satisfaction with NZ Environment",
  color_label = "Income Bracket",
  id_col = "id"
)

# example 4: customizing plot appearance and saving
p4 <- margot_plot_boxplot_covariate(
  data = your_data,
  outcome = "envefficacy",
  covariate = "gender",
  y_label = "Environmental Efficacy",
  color_label = "Gender",
  legend_position = "bottom",
  y_limits = c(1, 7),
  save_path = "path/to/save",
  prefix = "env_efficacy",
  width = 10,
  height = 6,
  id_col = "id"
)

# example 5: using with categorical outcome and including timestamp
p5 <- margot_plot_boxplot_covariate(
  data = your_data,
  outcome = "env_climate_chg_cause",
  covariate = "political_party",
  y_label = "Perceived Cause of Climate Change",
  color_label = "Political Party",
  include_timestamp = TRUE,
  id_col = "id"
)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_categorical}{Create a coloured histogram with summary lines and optional median}{margot.Rul.plot.Rul.categorical}
%
\begin{Description}
Create a coloured histogram with summary lines and optional median
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_categorical(
  df,
  col_name,
  label_mapping = NULL,
  n_divisions = NULL,
  custom_breaks = NULL,
  cutpoint_inclusive = "upper",
  ties.method = NULL,
  colour_palette = NULL,
  hist_colour = NA,
  line_type = "solid",
  line_width = 0.75,
  title = NULL,
  subtitle = NULL,
  x_lab = NULL,
  y_lab = "Count",
  theme_choice = theme_classic(),
  text_size = 12,
  axis_text_angle = 45,
  x_scale_transform = NULL,
  y_scale_transform = NULL,
  additional_layers = NULL,
  binwidth = NULL,
  legend_position = "right",
  show_mean = FALSE,
  show_sd = FALSE,
  sd_multipliers = c(-2, 1),
  show_median = FALSE,
  print_summary = TRUE,
  save_path = NULL,
  width = 16,
  height = 10,
  include_timestamp = FALSE,
  file_prefix = ""
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{label\_mapping}] named vector; optional remapping of variable names for labels.

\item[\code{show\_mean}] logical; draw vertical line at mean. default FALSE.

\item[\code{show\_sd}] logical; draw dashed lines at mean Â± sd\_multipliers. default FALSE.

\item[\code{sd\_multipliers}] numeric(2); multipliers for sd bands, c(lower, upper). default c(-2,1).

\item[\code{show\_median}] logical; draw vertical line at median. default FALSE.

\item[\code{print\_summary}] logical; annotate mean/median values. default TRUE.
\end{ldescription}
\end{Arguments}
\HeaderA{margot\_plot\_create\_options}{Create Plot Options for Margot Plot}{margot.Rul.plot.Rul.create.Rul.options}
%
\begin{Description}
This function creates a list of options for use with the `margot\_plot` function.
It allows for easy customization of plot settings while maintaining a set of default values.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_create_options(
  subtitle,
  base_defaults = NULL,
  title = NULL,
  filename_prefix = NULL,
  label_mapping = NULL,
  save_output = FALSE,
  use_timestamp = FALSE,
  base_filename = "margot_plot_output",
  save_path = NULL,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{subtitle}] Character string. The subtitle for the plot.

\item[\code{base\_defaults}] List. The base default options to use. If not provided, uses a pre-defined set of defaults.

\item[\code{title}] Character string. The title for the plot. If NULL, uses the title from base\_defaults.

\item[\code{filename\_prefix}] Character string. Prefix for the filename. If NULL, uses a default prefix.

\item[\code{label\_mapping}] Named list. Mapping of original outcome labels to new labels.

\item[\code{save\_output}] Logical. Whether to save the complete output. Default is FALSE.

\item[\code{use\_timestamp}] Logical. Whether to add a timestamp to the output filename. Default is FALSE.

\item[\code{base\_filename}] Character string. Base name for the output file. Default is "margot\_plot\_output".

\item[\code{save\_path}] Character string. Path where the output should be saved. Default is NULL.

\item[\code{...}] Additional named arguments to override or add to the default options.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function allows customization of various plot parameters, including color schemes,
text sizes, label transformations, and output saving options. The `label\_mapping` parameter
enables custom renaming of specific outcomes without affecting the default transformations
for other labels.
\end{Details}
%
\begin{Value}
A list of plot options to be used with `margot\_plot`.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Basic usage
health_options <- margot_plot_create_options("Health Outcomes")

# Custom title, filename prefix, and output saving
education_options <- margot_plot_create_options(
  "Education Outcomes",
  title = "Custom Title",
  filename_prefix = "edu_outcomes",
  save_output = TRUE,
  use_timestamp = TRUE,
  base_filename = "education_analysis",
  save_path = "path/to/save"
)

# Using label_mapping for custom outcome labels
trust_science_options <- margot_plot_create_options(
  subtitle = "Trust in Science Outcomes",
  title = "Science Trust Analysis",
  filename_prefix = "science_trust",
  label_mapping = list(
    "t2_trust_science_our_society_places_too_much_emphasis_reversed_z" = "Science Overemphasis"
  ),
  colors = c(
    "positive" = "#4CAF50",
    "not reliable" = "#FFC107",
    "negative" = "#F44336"
  ),
  base_size = 16,
  point_size = 4,
  save_output = TRUE
)

# Use the created options in margot_plot
result <- margot_plot(your_data, options = trust_science_options)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_decision\_tree}{Plot a Decision Tree from Margot Causal-Forest Results}{margot.Rul.plot.Rul.decision.Rul.tree}
\keyword{(robust}{margot\_plot\_decision\_tree}
\keyword{Causal-Forest}{margot\_plot\_decision\_tree}
\keyword{Decision}{margot\_plot\_decision\_tree}
\keyword{Margot}{margot\_plot\_decision\_tree}
\keyword{Plot}{margot\_plot\_decision\_tree}
\keyword{Results}{margot\_plot\_decision\_tree}
\keyword{Tree}{margot\_plot\_decision\_tree}
\keyword{a}{margot\_plot\_decision\_tree}
\keyword{from}{margot\_plot\_decision\_tree}
\keyword{internal}{margot\_plot\_decision\_tree}
\keyword{labelling)}{margot\_plot\_decision\_tree}
%
\begin{Description}
Creates a visualisation of a policy tree showing the decision rules
and assigned treatments from causal forest results.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_decision_tree(
  result_object,
  model_name = NULL,
  max_depth = 2L,
  original_df = NULL,
  x_padding = 0.12,
  y_padding = 0.25,
  border_size = 0.5,
  text_size = 4,
  edge_label_offset = 0.025,
  span_ratio = 0.4,
  non_leaf_fill = "lightyellow",
  title = NULL,
  plot_margin = grid::unit(c(1, 1, 1, 1), "cm"),
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE,
  remove_underscores = TRUE,
  remove_action_label = TRUE,
  label_mapping = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{result\_object}] A list returned by `margot\_causal\_forest()`

\item[\code{model\_name}] Name of the model in the results to visualise

\item[\code{max\_depth}] Maximum depth of the tree (1L or 2L)

\item[\code{original\_df}] Optional dataframe with original data for showing untransformed values

\item[\code{x\_padding}] Horizontal padding for the plot (proportion)

\item[\code{y\_padding}] Vertical padding for the plot (proportion)

\item[\code{border\_size}] Size of node borders in lines

\item[\code{text\_size}] Size of text in plot elements

\item[\code{edge\_label\_offset}] Offset for edge labels from connecting lines

\item[\code{span\_ratio}] Controls the aspect ratio of the plot

\item[\code{non\_leaf\_fill}] Colour for non-leaf nodes (decision nodes)

\item[\code{title}] Optional custom title for the plot

\item[\code{plot\_margin}] Margins around the plot

\item[\code{remove\_tx\_prefix}] Whether to remove treatment prefixes from variable names

\item[\code{remove\_z\_suffix}] Whether to remove z-suffixes from variable names

\item[\code{use\_title\_case}] Whether to use title case for variable names

\item[\code{remove\_underscores}] Whether to replace underscores with spaces in variable names

\item[\code{remove\_action\_label}] Whether to remove "Action:" prefix from leaf node labels

\item[\code{label\_mapping}] Optional list for renaming variables in the display
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object with the decision tree visualisation
\end{Value}
\HeaderA{margot\_plot\_discontinuity}{Create a Discontinuity Plot for Multiple Events}{margot.Rul.plot.Rul.discontinuity}
%
\begin{Description}
This function creates a ggplot2 visualisation to show discontinuities in data across multiple events.
It's particularly useful for visualising changes in trends before and after significant events.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_discontinuity(
  data,
  y_var,
  event_dates,
  event_names = NULL,
  start_date = NULL,
  end_date = NULL,
  title = NULL,
  y_label = NULL,
  x_label = NULL,
  smoothing_method = "gam",
  gam_k = 4,
  data_fraction = 1,
  seed = NULL,
  point_alpha = 0.03,
  jitter_width = 1,
  base_date = as.Date("2009-06-30"),
  save_path = NULL,
  width = 12,
  height = 8,
  event_line_color = "darkred",
  event_line_alpha = 0.7,
  event_line_type = "dashed",
  event_line_width = 0.5,
  event_label_size = 3,
  event_label_color = "darkred",
  legend_position = "bottom",
  use_title_case = TRUE,
  remove_underscores = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables to be plotted.

\item[\code{y\_var}] The name of the y-axis variable in the data frame.

\item[\code{event\_dates}] A vector of dates representing the events.

\item[\code{event\_names}] An optional vector of names for the events. If NULL, events will be labeled "Event 1", "Event 2", etc.

\item[\code{start\_date}] An optional start date for the x-axis.

\item[\code{end\_date}] An optional end date for the x-axis.

\item[\code{title}] An optional title for the plot.

\item[\code{y\_label}] An optional label for the y-axis.

\item[\code{x\_label}] An optional label for the x-axis.

\item[\code{smoothing\_method}] The method used for smoothing. Default is "gam".

\item[\code{gam\_k}] The number of knots to use if smoothing\_method is "gam". Default is 4.

\item[\code{data\_fraction}] The fraction of data to use. Default is 1 (use all data).

\item[\code{seed}] An optional seed for reproducibility when sampling data.

\item[\code{point\_alpha}] The alpha (transparency) of the data points. Default is 0.03.

\item[\code{jitter\_width}] The width of the jitter for the data points. Default is 1.

\item[\code{base\_date}] The base date for the timeline. Default is "2009-06-30".

\item[\code{save\_path}] An optional path to save the plot.

\item[\code{width}] The width of the saved plot in inches. Default is 12.

\item[\code{height}] The height of the saved plot in inches. Default is 8.

\item[\code{event\_line\_color}] The color of the event lines. Default is "darkred".

\item[\code{event\_line\_alpha}] The alpha of the event lines. Default is 0.7.

\item[\code{event\_line\_type}] The type of the event lines. Default is "dashed".

\item[\code{event\_line\_width}] The width of the event lines. Default is 0.5.

\item[\code{event\_label\_size}] The size of the event labels. Default is 3.

\item[\code{event\_label\_color}] The color of the event labels. Default is "darkred".

\item[\code{legend\_position}] The position of the legend. Default is "bottom".

\item[\code{use\_title\_case}] Logical, whether to use title case for labels. Default is TRUE.

\item[\code{remove\_underscores}] Logical, whether to remove underscores from labels. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the discontinuity plot.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
library(dplyr)
library(ggplot2)
library(margot)

# Assume that 'dat' is your dataset and that 'path_talk' is defined
muslim_discontinuity_warmth_plot <- margot_plot_discontinuity(
  data = dat,
  y_var = "warm_muslims",
  event_dates = c("2019-03-15", "2020-03-26"),
  event_names = c("Christchurch Attack", "COVID-19 Lockdown"),
  start_date = "2012-06-06",
  title = "Discontinuity at multiple events (GAM)",
  y_label = "Muslim Warmth",
  x_label = "NZAVS Time 4 - 14 Cohort (2012-2023)",
  point_alpha = 0.05,
  smoothing_method = "gam",
  gam_k = 4,
  data_fraction = .1,
  seed = 123,
  save_path = here::here(path_talk)
)

# Display the plot
print(muslim_discontinuity_warmth_plot)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_exposure}{Create a separate exposure plot}{margot.Rul.plot.Rul.exposure}
%
\begin{Description}
Create a separate exposure plot
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_exposure(data, name_exposure, exposure_waves, baseline_wave)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the exposure data

\item[\code{name\_exposure}] The name of the exposure variable

\item[\code{exposure\_waves}] A vector of wave numbers for exposure measurements

\item[\code{baseline\_wave}] The wave number for baseline measurements
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object representing the exposure plot
\end{Value}
\HeaderA{margot\_plot\_hist}{Create a Coloured Histogram with Quantile or Custom Breaks (DEPRECATED)}{margot.Rul.plot.Rul.hist}
\keyword{internal}{margot\_plot\_hist}
%
\begin{Description}
`r lifecycle::badge("deprecated")`
This function is deprecated. Please use `margot\_plot\_histogram()` instead.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_hist(
  df,
  col_name,
  n_divisions = NULL,
  custom_breaks = NULL,
  cutpoint_inclusive = "upper",
  ties.method = NULL,
  colour_palette = NULL,
  hist_colour = "black",
  line_type = "solid",
  line_width = 0.75,
  title = NULL,
  subtitle = NULL,
  x_lab = NULL,
  y_lab = "Count",
  theme_choice = theme_classic(),
  text_size = 12,
  axis_text_angle = 45,
  add_density = FALSE,
  add_rug = FALSE,
  facet_var = NULL,
  x_scale_transform = NULL,
  y_scale_transform = NULL,
  additional_layers = NULL,
  binwidth = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the variable to be plotted.

\item[\code{col\_name}] The name of the column in the data frame to be plotted.

\item[\code{n\_divisions}] The number of divisions for quantile breaks. Ignored if custom\_breaks is provided.

\item[\code{custom\_breaks}] A numeric vector of custom break points.

\item[\code{cutpoint\_inclusive}] Character. Either "lower" or "upper", specifying whether the cutpoint should be included in the lower or upper interval.

\item[\code{ties.method}] A character string specifying how ties should be handled. See ?quantile for details.

\item[\code{colour\_palette}] A vector of colors to use for the intervals. If NULL, uses the Okabe-Ito palette.

\item[\code{hist\_colour}] The color of the histogram borders.

\item[\code{line\_type}] The type of line to use for the histogram borders.

\item[\code{line\_width}] The width of the lines for the histogram borders.

\item[\code{title}] The title of the plot. If NULL, a default title is used.

\item[\code{subtitle}] The subtitle of the plot. If NULL, a default subtitle is used.

\item[\code{x\_lab}] The label for the x-axis. If NULL, the column name is used.

\item[\code{y\_lab}] The label for the y-axis. Default is "Count".

\item[\code{theme\_choice}] The ggplot2 theme to use. Default is theme\_classic().

\item[\code{text\_size}] The base text size for the plot.

\item[\code{axis\_text\_angle}] The angle of the x-axis text.

\item[\code{add\_density}] Logical. If TRUE, adds a density curve to the plot.

\item[\code{add\_rug}] Logical. If TRUE, adds a rug plot to the x-axis.

\item[\code{facet\_var}] Optional. The name of a variable to use for faceting.

\item[\code{x\_scale\_transform}] Optional. A transformation for the x-axis (e.g., "log10").

\item[\code{y\_scale\_transform}] Optional. A transformation for the y-axis (e.g., "log10").

\item[\code{additional\_layers}] A list of additional ggplot2 layers to add to the plot.

\item[\code{binwidth}] The width of the bins for the histogram. If NULL, calculated automatically.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the colored histogram.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
df <- data.frame(value = rnorm(1000))
coloured_histogram_quantiles(df, "value", n_divisions = 4)

# With custom breaks
coloured_histogram_quantiles(df, "value", custom_breaks = c(-2, -1, 0, 1, 2))

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_histogram}{Create a Histogram with Mean and Standard Deviation Highlights for Each Wave and Variable}{margot.Rul.plot.Rul.histogram}
%
\begin{Description}
This function generates a histogram plot for specified variables across different waves of data.
It highlights the mean and standard deviation for each variable in each wave.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_histogram(
  data,
  col_names,
  id_col = "id",
  wave_col = "wave",
  waves = NULL,
  binwidth = 0.5,
  title = NULL,
  x_label = NULL,
  y_label = "Count",
  save_path = NULL,
  width = 12,
  height = 8,
  facet_scales = "free",
  color_palette = NULL,
  add_timestamp = FALSE,
  file_prefix = "",
  mean_line_color = "black",
  sd_line_color = "black",
  vertical_facets = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables for the plot.

\item[\code{col\_names}] Vector of names of the columns to create histograms for.

\item[\code{id\_col}] Name of the column containing unique identifiers. Default is "id".

\item[\code{wave\_col}] Name of the column containing wave information. Default is "wave".

\item[\code{waves}] Vector of waves to include in the plot. If NULL, all waves are included.

\item[\code{binwidth}] Width of the bins for the histogram. Default is 0.5.

\item[\code{title}] An optional title for the plot. If NULL, an automatic title will be generated.

\item[\code{x\_label}] An optional label for the x-axis. If NULL, "Value" will be used.

\item[\code{y\_label}] An optional label for the y-axis. Default is "Count".

\item[\code{save\_path}] An optional path to save the plot. If NULL, the plot will not be saved.

\item[\code{width}] The width of the saved plot in inches. Default is 12.

\item[\code{height}] The height of the saved plot in inches. Default is 8.

\item[\code{facet\_scales}] Scales for facet. Either "fixed", "free\_x", "free\_y", or "free". Default is "free".

\item[\code{color\_palette}] An optional custom color palette for the plot.

\item[\code{add\_timestamp}] Logical. If TRUE, adds a timestamp to the saved filename. Default is FALSE.

\item[\code{file\_prefix}] An optional prefix to add to the beginning of the saved filename. Default is an empty string.

\item[\code{mean\_line\_color}] Color of the vertical line representing the mean. Default is "black".

\item[\code{sd\_line\_color}] Color of the dashed lines representing the standard deviation. Default is "black".

\item[\code{vertical\_facets}] Logical. If TRUE, facets are arranged vertically. If FALSE (default), facets are arranged horizontally.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the histogram with highlights.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# basic usage with default settings
margot_plot_histogram(
  data = your_data,
  col_names = c("variable1", "variable2"),
  id_col = "participant_id",
  wave_col = "survey_wave"
)

# specify waves and custom binwidth
margot_plot_histogram(
  data = your_data,
  col_names = c("score1", "score2"),
  waves = c(2018, 2020),
  binwidth = 1
)

# use custom labels and saving the plot with timestamp and prefix
margot_plot_histogram(
  data = your_data,
  col_names = c("attitude_measure"),
  title = "Distribution of Attitudes Over Time",
  x_label = "Attitude Score",
  save_path = "path/to/save/plot",
  add_timestamp = TRUE,
  file_prefix = "study1"
)

# use a custom color palette and custom line colors
custom_colors <- c("#FF9999", "#66B2FF")
margot_plot_histogram(
  data = your_data,
  col_names = c("var1", "var2"),
  color_palette = custom_colors,
  mean_line_color = "red",
  sd_line_color = "blue"
)

# use vertical faceting
margot_plot_histogram(
  data = your_data,
  col_names = c("var1", "var2"),
  vertical_facets = TRUE
)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_individual\_responses}{Create Individual Longitudinal Response Plots}{margot.Rul.plot.Rul.individual.Rul.responses}
%
\begin{Description}
This function creates a ggplot2 visualization of individual responses over time for one or more variables.
It allows for flexible data filtering, sampling, and customization of the plot appearance.
The function automatically handles missing data by removing rows with NA values in the specified variables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_individual_responses(
  data,
  y_vars,
  id_col = "id",
  wave_col = "wave",
  waves = NULL,
  data_fraction = 1,
  random_draws = 100,
  title = NULL,
  y_label = NULL,
  x_label = NULL,
  color_palette = NULL,
  theme = ggplot2::theme_classic(),
  include_timestamp = FALSE,
  save_path = NULL,
  width = 16,
  height = 8,
  seed = NULL,
  wave_label_angle = 45,
  full_response_scale = TRUE,
  scale_range = NULL,
  prefix = NULL,
  jitter_amount = 0.05,
  legend_position = "top"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables to be plotted.

\item[\code{y\_vars}] A character vector of column names in `data` to be plotted on the y-axis.

\item[\code{id\_col}] The name of the column in `data` that contains unique identifiers for individuals. Default is "id".

\item[\code{wave\_col}] The name of the column in `data` that contains the wave or time information. Default is "wave".

\item[\code{waves}] An optional vector of wave values to include in the plot. If NULL, all waves are included.

\item[\code{data\_fraction}] The fraction of data to use (between 0 and 1). Default is 1 (use all data).

\item[\code{random\_draws}] The number of random individuals to plot. If specified, overrides `data\_fraction`.

\item[\code{title}] An optional title for the plot.

\item[\code{y\_label}] An optional label for the y-axis.

\item[\code{x\_label}] An optional label for the x-axis.

\item[\code{color\_palette}] An optional vector of colors to use for the variables.

\item[\code{theme}] A ggplot2 theme to use for the plot. Default is theme\_classic().

\item[\code{include\_timestamp}] Logical, whether to include a timestamp in the saved file name. Default is FALSE.

\item[\code{save\_path}] An optional file path to save the plot.

\item[\code{width}] The width of the saved plot in inches. Default is 16.

\item[\code{height}] The height of the saved plot in inches. Default is 8.

\item[\code{seed}] An optional seed for reproducibility when sampling data.

\item[\code{wave\_label\_angle}] The angle of the x-axis labels in degrees. Default is 45.

\item[\code{full\_response\_scale}] Logical, whether to use the full response scale for the y-axis. Default is TRUE.

\item[\code{scale\_range}] An optional numeric vector of length 2 specifying the range for the y-axis. If NULL, the range is determined from the data.

\item[\code{prefix}] An optional prefix for the saved file name.

\item[\code{jitter\_amount}] Numeric, the amount of vertical jitter to apply to points and lines. Default is 0.05.

\item[\code{legend\_position}] The position of the legend. Default is "top".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the individual response plot.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Example 1: Basic usage with default settings
plot1 <- margot_plot_individual_responses(
  data = your_data,
  y_vars = c("variable1", "variable2"),
  id_col = "participant_id",
  wave_col = "year"
)

# Example 2: Plotting specific waves and using random draws
plot2 <- margot_plot_individual_responses(
  data = your_data,
  y_vars = c("score1", "score2", "score3"),
  waves = c(2020, 2021, 2022),
  random_draws = 50,
  title = "Individual Scores Over Time",
  y_label = "Score",
  x_label = "Year",
  seed = 123
)

# Example 3: Customizing plot appearance and saving
plot3 <- margot_plot_individual_responses(
  data = your_data,
  y_vars = c("measure1", "measure2"),
  full_response_scale = TRUE,
  scale_range = c(0, 10),
  theme = theme_minimal(),
  wave_label_angle = 90,
  jitter_amount = 0.03,
  legend_position = "bottom",
  save_path = "path/to/save",
  prefix = "custom_plot"
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_multi\_arm}{Create a Multi-arm Margot Plot with User-specified Contrast}{margot.Rul.plot.Rul.multi.Rul.arm}
%
\begin{Description}
This function is a wrapper for the `margot\_plot()` function, designed for multi-arm causal forest models.
It allows the user to specify the contrast of interest, and automatically passes it to the `margot\_plot()` function.
If the specified contrast is not found, an error is reported.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_multi_arm(
  model_table,
  contrast,
  original_df = NULL,
  options,
  label_mapping,
  save_output = FALSE,
  use_timestamp = FALSE,
  base_filename = "margot_plot_output",
  prefix = NULL,
  save_path = here::here("push_mods")
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_table}] A list or data frame containing the model output tables.

\item[\code{contrast}] A character string specifying the contrast to be used (e.g. `"(5.0,7.0] - [1.0,3.0]"`).

\item[\code{original\_df}] Optional data frame containing the original (non-transformed) data for back-transformation of results.

\item[\code{options}] A list of additional options for customising the plot, passed directly to `margot\_plot()`.

\item[\code{label\_mapping}] A named list for custom label mapping of the outcomes, also passed to `margot\_plot()`.

\item[\code{save\_output}] Logical. If TRUE, saves the complete output to a file. Default is FALSE.

\item[\code{use\_timestamp}] Logical. If TRUE, adds a timestamp to the saved filename. Default is FALSE.

\item[\code{base\_filename}] Character string. The base name for the saved file. Default is "margot\_plot\_output".

\item[\code{prefix}] Character string. An optional prefix for the saved filename. Default is NULL.

\item[\code{save\_path}] Character string. The directory path where the output will be saved. Default is here::here("push\_mods").
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The user must specify a contrast from the `model\_table`. If the contrast is not found, an error will be raised using `cli` reporting.

The `margot\_plot()` function provides various options for customising the plot, including the ability to save the plot,
modify labels, and adjust plot aesthetics. The full range of options available to `margot\_plot()` can be passed through the `options` argument.

If `original\_df` is provided, the function will use it to back-transform the results to the original scale.
\end{Details}
%
\begin{Value}
A list with the following elements:
\begin{itemize}

\item{} `plot`: A ggplot object of the Margot plot.
\item{} `interpretation`: A character string with the interpretation of the results.
\item{} `transformed\_table`: A data frame with transformed labels according to the options and label mappings.

\end{itemize}


If `save\_output` is TRUE, the complete output will be saved to a file using margot::here\_save\_qs().
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Example usage with multi-arm models
multi_results <- margot_plot_multi_arm(
  models_multi$combined_tables,
  contrast = "(5.0,7.0] - [1.0,3.0]",
  original_df = df_raw_outcomes,
  options = multi_options,
  label_mapping = label_mapping,
  save_output = TRUE,
  save_path = here::here("output", "margot_plots"),
  base_filename = "margot_plot_output",
  prefix = "test"
)
print(multi_results$plot)
cat(multi_results$interpretation)
print(multi_results$transformed_table)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_policy\_combo}{Create a Combined Decision Tree and Policy Relationship Graph}{margot.Rul.plot.Rul.policy.Rul.combo}
%
\begin{Description}
This function generates a combined plot consisting of a decision tree and a graph
showing relationships between variables in the recommended policy.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_policy_combo(
  result_object,
  model_name,
  max_depth = 2L,
  label_mapping = NULL,
  original_df = NULL,
  layout = list(heights = c(1, 2)),
  annotation = list(tag_levels = "A"),
  generate_policy_tree = TRUE,
  generate_decision_tree = TRUE,
  policy_tree_args = list(),
  decision_tree_args = list()
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{result\_object}] An object containing the results from a multi-arm causal forest model.

\item[\code{model\_name}] A character string specifying the name of the model.

\item[\code{max\_depth}] Integer, 1 or 2; which decision tree depth to plot. Default: 2.

\item[\code{label\_mapping}] Optional named list for custom label mappings.

\item[\code{original\_df}] Optional dataframe with untransformed variables.

\item[\code{layout}] A list specifying the layout of the combined plot when max\_depth==2. Default is
`list(heights = c(1, 2))`, which sets the relative heights of the two plots.

\item[\code{annotation}] A list specifying the annotation for the combined plot when max\_depth==2. Default is
`list(tag\_levels = "A")`, which adds alphabetic tags to the subplots.

\item[\code{generate\_policy\_tree}] Logical, whether to generate the policy tree plot. Default is TRUE.

\item[\code{generate\_decision\_tree}] Logical, whether to generate the decision tree plot. Default is TRUE.

\item[\code{policy\_tree\_args}] A list of arguments to pass to `margot\_plot\_policy\_tree`. Default is list().

\item[\code{decision\_tree\_args}] A list of arguments to pass to `margot\_plot\_decision\_tree`. Default is list().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing:
\begin{ldescription}
\item[\code{policy\_tree}] A ggplot object representing the policy tree (if generated)
\item[\code{decision\_tree}] A ggplot object representing the decision tree (if generated)
\item[\code{combined\_plot}] A ggplot object representing the combined plot (if both plots are generated)
\end{ldescription}
\end{Value}
\HeaderA{margot\_plot\_policy\_tree}{Plot a policy tree (depth-adaptive)}{margot.Rul.plot.Rul.policy.Rul.tree}
%
\begin{Description}
Visualise the first one or two splits of a `policytree` stored inside a
multi-arm causal-forest result.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_policy_tree(
  result_object,
  model_name,
  max_depth = 2L,
  original_df = NULL,
  shading = NULL,
  color_scale = NULL,
  point_alpha = 0.5,
  theme_function = ggplot2::theme_classic,
  label_mapping = NULL,
  label_options = list(remove_tx_prefix = TRUE, remove_z_suffix = TRUE,
    remove_underscores = TRUE, use_title_case = TRUE),
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{result\_object}] A list produced by `margot\_multiclass\_cf()` (or similar)
whose `results` slot holds `policy\_tree\_depth\_1`,
`policy\_tree\_depth\_2`, and `plot\_data` entries for `model\_name`.

\item[\code{model\_name}] Character scalar identifying the result inside
`result\_object\$results`.

\item[\code{max\_depth}] Integer, 1 or 2; which stored tree to visualise.

\item[\code{original\_df}] Optional data frame of raw-scale variables (only used
by the depth-2 plot for secondary-axis labels).

\item[\code{shading}] Logical â€“ draw shaded half-planes for non-decision regions
(depth-2 only).  If `NULL` (default) the function decides automatically.

\item[\code{color\_scale}] A pre-built `ggplot2` colour scale (rarely needed).

\item[\code{point\_alpha}] Alpha transparency for plotted points.

\item[\code{theme\_function}] A `ggplot2` theme function; default
`ggplot2::theme\_classic`.

\item[\code{label\_mapping}] Named list for explicit string replacements; passed
to `transform\_label()`.

\item[\code{label\_options}] List of logical flags understood by
`transform\_label()` (see that function for details).

\item[\code{...}] Extra arguments forwarded **only** to the depth-2 helper
(e.g. `title\_size`, `jitter\_width`, etc.).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
* **Depth 1** â€“ a oneâ€“dimensional jitter plot coloured by the predicted
action and annotated with the split point.
* **Depth 2** â€“ the existing two-panel scatter-plot (hand-off to
`margot\_plot\_policy\_tree\_depth2()`).
\end{Details}
%
\begin{Value}
A `ggplot` object (depth 1) or a patchwork object (depth 2).
\end{Value}
\HeaderA{margot\_plot\_qini}{Plot Qini Curves from margot\_multi\_arm\_causal\_forest Results}{margot.Rul.plot.Rul.qini}
%
\begin{Description}
This function creates a ggplot object displaying Qini curves based on the
results of a margot\_multi\_arm\_causal\_forest() model. It includes label
transformations and informative CLI messages.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_qini(
  mc_result,
  outcome_var,
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE,
  remove_underscores = TRUE,
  label_mapping = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{mc\_result}] A list containing the results from margot\_multi\_arm\_causal\_forest().

\item[\code{outcome\_var}] A character string specifying the name of the outcome variable
to plot. This should match one of the model names in mc\_result\$results.

\item[\code{remove\_tx\_prefix}] Logical value indicating whether to remove the "tx\_" prefix from labels. Default is TRUE.

\item[\code{remove\_z\_suffix}] Logical value indicating whether to remove the "\_z" suffix from labels. Default is TRUE.

\item[\code{use\_title\_case}] Logical value indicating whether to convert labels to title case. Default is TRUE.

\item[\code{remove\_underscores}] Logical value indicating whether to remove underscores from labels. Default is TRUE.

\item[\code{label\_mapping}] Optional named list for custom label mappings. Keys should be original variable names
(with or without "model\_" prefix), and values should be the desired display labels. Default is NULL.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object representing the Qini curves for the specified outcome variable.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming mc.test is the result of margot_multi_arm_causal_forest()
plot_qini_curves(mc.test, "model_t2_belong_z")

# Using custom label mapping
label_mapping <- list(
  "t2_env_not_env_efficacy_z" = "Deny Personal Environmental Efficacy",
  "t2_env_not_climate_chg_real_z" = "Deny Climate Change Real"
)
plot_qini_curves(mc.test, "model_t2_env_not_env_efficacy_z", label_mapping = label_mapping)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_rate}{Plot Rank Average Treatment Effect}{margot.Rul.plot.Rul.rate}
%
\begin{Description}
This function creates a ggplot visualisation of the Rank Average Treatment Effect.
It displays the estimate with a confidence interval, using a simple black line
and light gray shading by default.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_rate(
  x,
  outcome_var,
  title = NULL,
  subtitle = "(95% confidence interval shown as shaded area)",
  x_lab = "Treated fraction (q)",
  y_lab = "Estimate",
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE,
  remove_underscores = TRUE,
  label_mapping = NULL,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] An object of class "rank\_average\_treatment\_effect", typically the output
of the rank\_average\_treatment\_effect() function.

\item[\code{outcome\_var}] A character string specifying the name of the outcome variable
to plot. This is used for the plot title with proper label transformation.

\item[\code{title}] Character string for the plot title. If NULL (default), a title is
generated using the transformed outcome variable name.

\item[\code{subtitle}] Character string for the plot subtitle. Default explains the confidence interval.

\item[\code{x\_lab}] Character string for the x-axis label. Default is "Treated fraction (q)".

\item[\code{y\_lab}] Character string for the y-axis label. Default is "Estimate".

\item[\code{remove\_tx\_prefix}] Logical value indicating whether to remove the "tx\_" prefix from labels. Default is TRUE.

\item[\code{remove\_z\_suffix}] Logical value indicating whether to remove the "\_z" suffix from labels. Default is TRUE.

\item[\code{use\_title\_case}] Logical value indicating whether to convert labels to title case. Default is TRUE.

\item[\code{remove\_underscores}] Logical value indicating whether to remove underscores from labels. Default is TRUE.

\item[\code{label\_mapping}] Optional named list for custom label mappings. Keys should be original variable names
(with or without "model\_" prefix), and values should be the desired display labels. Default is NULL.

\item[\code{...}] Additional arguments passed to ggplot.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object that can be further customised or printed.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming rate_eval is your rank_average_treatment_effect object
p <- margot_plot_rate(rate_eval, "model_t2_belong_z")
print(p)

# Using custom label mapping
label_mapping <- list(
  "t2_env_not_env_efficacy_z" = "Deny Personal Environmental Efficacy",
  "t2_env_not_climate_chg_real_z" = "Deny Climate Change Real"
)
p <- margot_plot_rate(rate_eval, "model_t2_env_not_env_efficacy_z",
                      label_mapping = label_mapping)
print(p)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_rate\_batch}{Batch Process and Plot RATE Curves for Multiple Models}{margot.Rul.plot.Rul.rate.Rul.batch}
%
\begin{Description}
This function processes a subset of models (or all models by default), creates RATE (Rank Average Treatment Effect)
plots for each model using the margot package, and optionally saves the plots.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_rate_batch(
  models_binary,
  model_names = NULL,
  dpi = 300,
  width = 12,
  height = 8,
  save_plots = TRUE,
  output_dir = "rate_plots",
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE,
  remove_underscores = TRUE,
  label_mapping = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{models\_binary}] A list of model results, where each element contains a 'rate\_result' component.

\item[\code{model\_names}] Optional character vector of model names to process. Default NULL (all models).

\item[\code{dpi}] The resolution of the saved plots in dots per inch. Default is 300.

\item[\code{width}] The width of the saved plots in inches. Default is 12.

\item[\code{height}] The height of the saved plots in inches. Default is 8.

\item[\code{save\_plots}] Logical indicating whether to save the plots to disk. Default is TRUE.

\item[\code{output\_dir}] The directory where plots should be saved. Default is "rate\_plots".

\item[\code{remove\_tx\_prefix}] Logical value indicating whether to remove the "tx\_" prefix from labels. Default is TRUE.

\item[\code{remove\_z\_suffix}] Logical value indicating whether to remove the "\_z" suffix from labels. Default is TRUE.

\item[\code{use\_title\_case}] Logical value indicating whether to convert labels to title case. Default is TRUE.

\item[\code{remove\_underscores}] Logical value indicating whether to remove underscores from labels. Default is TRUE.

\item[\code{label\_mapping}] Optional named list for custom label mappings. Keys should be original variable names
(with or without "model\_" prefix), and values should be the desired display labels. Default is NULL.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the generated ggplot objects for each processed model.
\end{Value}
\HeaderA{margot\_plot\_response\_timeline}{Plot Panel Study Response Timeline}{margot.Rul.plot.Rul.response.Rul.timeline}
%
\begin{Description}
This function creates a ggplot2 visualization of a panel study response timeline.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_response_timeline(
  df_timeline,
  n_total_participants = NULL,
  save = FALSE,
  save_path = here::here("output"),
  width = 12,
  height = 8,
  base_filename = "timeline_histogram",
  title = "Panel Study Timeline",
  x_label = "Date",
  y_label = "Count of Responses",
  color_palette = NULL,
  save_png = FALSE,
  use_timestamp = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df\_timeline}] A data frame containing the processed timeline data, typically output from `prepare\_panel\_data()`.

\item[\code{n\_total\_participants}] The total number of unique participants. If NULL, it will be extracted from df\_timeline if available.

\item[\code{save}] Logical. If TRUE, saves the plot as a qs file. Default is FALSE.

\item[\code{save\_path}] The directory path to save the plot. Default is "output" in the current working directory.

\item[\code{width}] The width of the saved plot in inches. Default is 12.

\item[\code{height}] The height of the saved plot in inches. Default is 8.

\item[\code{base\_filename}] The base filename for saving the plot. Default is "timeline\_histogram".

\item[\code{title}] The main title for the plot. Default is "Panel Study Timeline".

\item[\code{x\_label}] The label for the x-axis. Default is "Date".

\item[\code{y\_label}] The label for the y-axis. Default is "Count of Responses".

\item[\code{color\_palette}] A vector of colors to use for the waves. If NULL, uses a default color-blind friendly palette.

\item[\code{save\_png}] Logical. If TRUE, saves the plot as a PNG file. Default is FALSE.

\item[\code{use\_timestamp}] Logical. If TRUE, includes a timestamp in the PNG filename. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the panel study response timeline.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Load required libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(here)

# Assume we have a data frame 'nzavs_data' with columns: id, wave, tscore

# Step 1: Define NZAVS-specific wave breaks
nzavs_wave_breaks <- list(
  "time 1" = c(as.Date("2009-08-30"), as.Date("2010-10-15")),
  "time 2" = c(as.Date("2010-10-15"), as.Date("2011-10-15")),
  "time 3" = c(as.Date("2011-10-15"), as.Date("2012-10-15")),
  "time 4" = c(as.Date("2012-10-15"), as.Date("2013-10-15")),
  "time 5" = c(as.Date("2013-10-15"), as.Date("2014-10-15")),
  "time 6" = c(as.Date("2014-10-15"), as.Date("2015-10-15")),
  "time 7" = c(as.Date("2015-10-15"), as.Date("2016-10-15")),
  "time 8" = c(as.Date("2016-10-15"), as.Date("2017-10-15")),
  "time 9" = c(as.Date("2017-10-15"), as.Date("2018-10-15")),
  "time 10" = c(as.Date("2018-10-15"), as.Date("2019-10-15")),
  "time 11" = c(as.Date("2019-10-15"), as.Date("2020-10-15")),
  "time 12" = c(as.Date("2020-10-15"), as.Date("2021-10-15")),
  "time 13" = c(as.Date("2021-10-15"), as.Date("2022-10-15")),
  "time 14" = c(as.Date("2022-10-15"), as.Date("2023-10-15"))
)

# Step 2: Prepare the NZAVS data
prepared_data <- prepare_panel_data(
  dat = nzavs_data,
  wave_col = "wave",
  tscore_col = "tscore",
  id_col = "id",
  base_date = as.Date("2009-06-30"),
  wave_breaks = nzavs_wave_breaks
)

# Step 3: Create the NZAVS timeline plot
nzavs_timeline <- margot_plot_response_timeline(
  df_timeline = prepared_data$df_timeline,
  n_total_participants = prepared_data$n_total_participants,
  save = TRUE,
  save_png = TRUE,
  use_timestamp = TRUE,
  save_path = here::here("output", "plots"),
  title = "New Zealand Attitudes and Values Study (panel)",
  x_label = paste("NZAVS years",  min(prepared_data$df_timeline$day, na.rm = TRUE),
                  "-", max(prepared_data$df_timeline$day, na.rm = TRUE),
                  "cohort: daily counts by condition"),
  y_label = "Count of Responses"
)

# Display the plot
print(nzavs_timeline)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_shift}{Visualise shifts in data distributions with highlighted ranges}{margot.Rul.plot.Rul.shift}
%
\begin{Description}
Visualise shifts in data distributions with highlighted ranges
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_shift(
  df,
  col_name,
  label_mapping = NULL,
  binwidth = 1,
  range_highlight = NULL,
  shift = "up",
  show_avg_line = TRUE,
  print_avg_value = TRUE,
  show_sd_line = TRUE,
  title = NULL,
  subtitle = NULL,
  x_lab = NULL,
  y_lab = "Count",
  save_path = NULL,
  width = 12,
  height = 8,
  include_timestamp = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{label\_mapping}] named vector; optional remapping of variable names for labels.
\end{ldescription}
\end{Arguments}
\HeaderA{margot\_plot\_slope}{Create a Slope Plot for Multiple Variables}{margot.Rul.plot.Rul.slope}
%
\begin{Description}
This function creates a ggplot2 visualization to show trends in multiple variables over time.
It's possible to add vertical lines at significant events. The function now also counts and
reports the number of unique participants and observations. It includes options for faceting
to avoid overplotting when dealing with multiple variables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_slope(
  data,
  y_vars,
  event_dates = NULL,
  event_names = NULL,
  start_date = NULL,
  end_date = NULL,
  title = NULL,
  y_label = NULL,
  x_label = NULL,
  data_fraction = 1,
  seed = NULL,
  plot_points = FALSE,
  point_alpha = 0.03,
  jitter_width = 1,
  base_date = as.Date("2009-06-30"),
  save_path = NULL,
  width = 12,
  height = 8,
  event_line_color = "darkred",
  event_line_alpha = 0.7,
  event_line_type = "dashed",
  event_line_width = 0.5,
  event_label_size = 3,
  event_label_color = "darkred",
  legend_position = "bottom",
  use_title_case = TRUE,
  remove_underscores = TRUE,
  y_limits = NULL,
  color_palette = NULL,
  use_facets = TRUE,
  facet_scales = "free_y",
  facet_ncol = NULL,
  facet_nrow = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables to be plotted.

\item[\code{y\_vars}] A list of variable names or a single variable name to be plotted on the y-axis.

\item[\code{event\_dates}] An optional vector of dates representing the events.

\item[\code{event\_names}] An optional vector of names for the events. If NULL, events will be labeled "Event 1", "Event 2", etc.

\item[\code{start\_date}] An optional start date for the x-axis.

\item[\code{end\_date}] An optional end date for the x-axis.

\item[\code{title}] An optional title for the plot. If NULL, an automatic title will be generated including the count of participants and observations.

\item[\code{y\_label}] An optional label for the y-axis.

\item[\code{x\_label}] An optional label for the x-axis.

\item[\code{data\_fraction}] The fraction of data to use. Default is 1 (use all data).

\item[\code{seed}] An optional seed for reproducibility when sampling data.

\item[\code{plot\_points}] Logical, whether to plot individual data points. Default is FALSE.

\item[\code{point\_alpha}] The alpha (transparency) of the data points. Default is 0.03.

\item[\code{jitter\_width}] The width of the jitter for the data points. Default is 1.

\item[\code{base\_date}] The base date for the timeline. Default is "2009-06-30".

\item[\code{save\_path}] An optional path to save the plot.

\item[\code{width}] The width of the saved plot in inches. Default is 12.

\item[\code{height}] The height of the saved plot in inches. Default is 8.

\item[\code{event\_line\_color}] The color of the event lines. Default is "darkred".

\item[\code{event\_line\_alpha}] The alpha of the event lines. Default is 0.7.

\item[\code{event\_line\_type}] The type of the event lines. Default is "dashed".

\item[\code{event\_line\_width}] The width of the event lines. Default is 0.5.

\item[\code{event\_label\_size}] The size of the event labels. Default is 3.

\item[\code{event\_label\_color}] The color of the event labels. Default is "darkred".

\item[\code{legend\_position}] The position of the legend. Default is "bottom".

\item[\code{use\_title\_case}] Logical, whether to use title case for labels. Default is TRUE.

\item[\code{remove\_underscores}] Logical, whether to remove underscores from labels. Default is TRUE.

\item[\code{y\_limits}] An optional vector of two numbers specifying the y-axis limits.

\item[\code{color\_palette}] An optional custom color palette. If NULL, a default palette will be used.

\item[\code{use\_facets}] Logical, whether to use faceting for multiple variables. Default is TRUE.

\item[\code{facet\_scales}] The scales parameter for facet\_wrap. Default is "free\_y".

\item[\code{facet\_ncol}] The number of columns for facet\_wrap. Default is NULL.

\item[\code{facet\_nrow}] The number of rows for facet\_wrap. Default is NULL.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the slope plot.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
library(dplyr)
library(ggplot2)
library(tidyr)
library(here)

# Basic usage with a single variable
single_var_plot <- margot_plot_slope(
  data = dat,
  y_vars = "warm_muslims",
  start_date = "2012-06-06",
  y_label = "Warmth",
  x_label = "NZAVS Time 4 - 14 Cohort (2012-2023)"
)

# Multiple variables with events and custom y-axis limits
multi_var_plot <- margot_plot_slope(
  data = dat,
  y_vars = list("warm_muslims", "warm_immigrants"),
  event_dates = c("2019-03-15", "2021-01-01"),
  event_names = c("Christchurch Attack", "COVID-19 Lockdown"),
  start_date = "2012-06-06",
  y_label = "Warmth",
  x_label = "NZAVS Time 4 - 14 Cohort (2012-2023)",
  y_limits = c(1, 7),
  use_facets = TRUE
)

# Plot with points, using a subset of data and custom facet layout
point_plot <- margot_plot_slope(
  data = dat,
  y_vars = list("warm_asians", "warm_pacific", "warm_immigrants"),
  plot_points = TRUE,
  point_alpha = 0.05,
  data_fraction = 0.1,
  seed = 123,
  y_label = "Warmth",
  use_facets = TRUE,
  facet_ncol = 2
)

# Save the plot
saved_plot <- margot_plot_slope(
  data = dat,
  y_vars = list("political_orientation", "social_dominance_orientation"),
  save_path = here::here("outputs", "plots"),
  width = 10,
  height = 6,
  use_facets = TRUE
)

# Custom styling and color palette
custom_plot <- margot_plot_slope(
  data = dat,
  y_vars = list("sat_government", "sat_nz_econ_conditions"),
  event_dates = "2017-10-26",
  event_names = "2017 Election",
  y_label = "Satisfaction Level (0-10)",
  y_limits = c(0, 10),
  event_line_color = "blue",
  event_label_color = "blue",
  legend_position = "top",
  color_palette = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd"),
  use_facets = TRUE
)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_slope\_covariate}{Create a Slope Plot using ggeffects}{margot.Rul.plot.Rul.slope.Rul.covariate}
%
\begin{Description}
This function creates a ggplot2 visualization using ggeffects to calculate
predicted responses from a model. It allows flexible specification of the model
and plotting options. The function automatically handles NA and infinite values,
and reports the number of unique participants and observations used in the analysis.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_slope_covariate(
  data,
  formula,
  terms,
  id_col = "id",
  title = NULL,
  y_label = NULL,
  x_label = NULL,
  y_limits = c(1, 7),
  color_label = NULL,
  include_title = TRUE,
  include_timestamp = FALSE,
  save_path = NULL,
  prefix = NULL,
  width = 12,
  height = 8,
  seed = NULL,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables for the model.

\item[\code{formula}] A formula specifying the model to be fit.

\item[\code{terms}] A character vector specifying the terms to be used in predict\_response.

\item[\code{id\_col}] Name of the column containing unique identifiers (default is "id").

\item[\code{title}] An optional title for the plot. If NULL, an automatic title will be generated.

\item[\code{y\_label}] An optional label for the y-axis. If NULL, the response variable name will be used.

\item[\code{x\_label}] An optional label for the x-axis. If NULL, the first term will be used.

\item[\code{y\_limits}] An optional vector of two numbers specifying the y-axis limits. Default is c(1, 7).

\item[\code{color\_label}] An optional label for the color legend. If NULL, the second term will be used.

\item[\code{include\_title}] Logical, whether to include the plot title (default is TRUE).

\item[\code{include\_timestamp}] Logical, whether to include timestamp in plot title and filename (default is FALSE).

\item[\code{save\_path}] An optional path to save the plot. If NULL, the plot will not be saved.

\item[\code{prefix}] Optional prefix for the saved file name (default is NULL).

\item[\code{width}] The width of the saved plot in inches. Default is 12.

\item[\code{height}] The height of the saved plot in inches. Default is 8.

\item[\code{seed}] An optional seed for reproducibility.

\item[\code{...}] Additional arguments to be passed to ggeffects::predict\_response.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the plot.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Example usage remains the same

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_slope\_covariate\_batch}{Create a Combined Slope Plot using ggeffects and patchwork}{margot.Rul.plot.Rul.slope.Rul.covariate.Rul.batch}
%
\begin{Description}
This function creates multiple ggplot2 visualizations using ggeffects to calculate
predicted responses from models for multiple outcome variables and combines them
into a single plot using the patchwork package. It allows flexible specification
of the models and plotting options, including layout and annotations.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_slope_covariate_batch(
  data,
  outcome_vars,
  exposure_formula,
  terms,
  label_mapping = NULL,
  x_label = NULL,
  color_label = NULL,
  save_path = NULL,
  file_prefix = "plot_slope_covariate_batch",
  ncol = NULL,
  nrow = NULL,
  guides = "collect",
  patchwork_params = list(),
  plot_annotation_params = list(),
  caption_size = 10,
  include_individual_titles = FALSE,
  width = 10,
  height = 8,
  dpi = 400,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables for the models.

\item[\code{outcome\_vars}] A character vector specifying the outcome variables to be modeled.

\item[\code{exposure\_formula}] A formula specifying the exposure variables (right-hand side of the model).

\item[\code{terms}] A character vector specifying the terms to be used in predict\_response.

\item[\code{label\_mapping}] An optional named list mapping outcome variables to custom y-axis labels.

\item[\code{x\_label}] An optional label for the x-axis. If NULL, the first term will be used.

\item[\code{color\_label}] An optional label for the color legend. If NULL, the second term will be used.

\item[\code{save\_path}] An optional path to save the combined plot. If NULL, the plot will not be saved.

\item[\code{file\_prefix}] Optional prefix for the saved file name (default is "plot\_slope\_covariate\_batch").

\item[\code{ncol}] Number of columns in the combined plot layout.

\item[\code{nrow}] Number of rows in the combined plot layout.

\item[\code{guides}] How to combine legends. Default is "collect".

\item[\code{patchwork\_params}] A list of additional parameters to be passed to patchwork::plot\_layout().

\item[\code{plot\_annotation\_params}] A list of parameters to be passed to patchwork::plot\_annotation().

\item[\code{include\_individual\_titles}] Logical, whether to include titles in individual plots (default is FALSE).

\item[\code{width}] Width of the combined plot in inches. Default is 12.

\item[\code{height}] Height of the combined plot in inches. Default is 8.

\item[\code{dpi}] Resolution of the saved plot. Default is 400.

\item[\code{...}] Additional arguments to be passed to margot\_plot\_slope\_covariate().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the combined plot.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Define outcome variables and label mapping
outcome_vars <- c("var1", "var2", "var3")
label_mapping <- list("var1" = "Variable 1", "var2" = "Variable 2", "var3" = "Variable 3")

# Create combined plot
combined_plot <- margot_plot_slope_covariate_batch(
  data = dat,
  outcome_vars = outcome_vars,
  exposure_formula = "~ wave:covariate",
  terms = c("wave", "covariate"),
  label_mapping = label_mapping,
  x_label = "Time",
  color_label = "Covariate",
  ncol = 2,
  plot_annotation_params = list(
    title = "Combined Slope Plots",
    subtitle = "Subtitle for the combined plot"
  ),
  save_path = "path/to/save/directory",
  width = 14,
  height = 10
)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_slope\_covariate\_combo}{Create a Combined Slope Plot using ggeffects and patchwork}{margot.Rul.plot.Rul.slope.Rul.covariate.Rul.combo}
%
\begin{Description}
This function creates multiple ggplot2 visualizations using ggeffects to calculate
predicted responses from models for multiple outcome variables and combines them
into a single plot using the patchwork package. It allows flexible specification
of the models and plotting options, including layout and annotations.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_slope_covariate_combo(
  data,
  outcome_vars,
  exposure_formula,
  terms,
  label_mapping = NULL,
  x_label = NULL,
  color_label = NULL,
  save_path = NULL,
  file_prefix = "plot_slope_covariate_batch",
  ncol = NULL,
  nrow = NULL,
  guides = "collect",
  patchwork_params = list(),
  plot_annotation_params = list(),
  caption_size = 10,
  include_individual_titles = FALSE,
  width = 12,
  height = 8,
  dpi = 400,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables for the models.

\item[\code{outcome\_vars}] A character vector specifying the outcome variables to be modeled.

\item[\code{exposure\_formula}] A formula specifying the exposure variables (right-hand side of the model).

\item[\code{terms}] A character vector specifying the terms to be used in predict\_response.

\item[\code{label\_mapping}] An optional named list mapping outcome variables to custom y-axis labels.

\item[\code{x\_label}] An optional label for the x-axis. If NULL, the first term will be used.

\item[\code{color\_label}] An optional label for the color legend. If NULL, the second term will be used.

\item[\code{save\_path}] An optional path to save the combined plot. If NULL, the plot will not be saved.

\item[\code{file\_prefix}] Optional prefix for the saved file name (default is "plot\_slope\_covariate\_batch").

\item[\code{ncol}] Number of columns in the combined plot layout.

\item[\code{nrow}] Number of rows in the combined plot layout.

\item[\code{guides}] How to combine legends. Default is "collect".

\item[\code{patchwork\_params}] A list of additional parameters to be passed to patchwork::plot\_layout().

\item[\code{plot\_annotation\_params}] A list of parameters to be passed to patchwork::plot\_annotation().

\item[\code{include\_individual\_titles}] Logical, whether to include titles in individual plots (default is FALSE).

\item[\code{width}] Width of the combined plot in inches. Default is 12.

\item[\code{height}] Height of the combined plot in inches. Default is 8.

\item[\code{dpi}] Resolution of the saved plot. Default is 400.

\item[\code{...}] Additional arguments to be passed to margot\_plot\_slope\_covariate().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 object representing the combined plot.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Define outcome variables and label mapping
outcome_vars <- c("var1", "var2", "var3")
label_mapping <- list("var1" = "Variable 1", "var2" = "Variable 2", "var3" = "Variable 3")

# Create combined plot
combined_plot <- margot_plot_slope_covariate_combo(
  data = dat,
  outcome_vars = outcome_vars,
  exposure_formula = "~ wave:covariate",
  terms = c("wave", "covariate"),
  label_mapping = label_mapping,
  x_label = "Time",
  color_label = "Covariate",
  ncol = 2,
  plot_annotation_params = list(
    title = "Combined Slope Plots",
    subtitle = "Subtitle for the combined plot"
  ),
  save_path = "path/to/save/directory",
  width = 14,
  height = 10
)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_plot\_tau}{Create Faceted Tau Hat Distribution Plots}{margot.Rul.plot.Rul.tau}
%
\begin{Description}
creates a faceted grid of histograms showing the distribution of tau hat
(individual treatment effects) for multiple models. the range is standardised
across all facets to facilitate comparison. automatically detects and handles
models\_binary structures by extracting the \$results component.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_plot_tau(
  models_list,
  label_mapping = NULL,
  binwidth = 0.01,
  base_size = 14,
  ncol = NULL,
  title = NULL,
  subtitle = NULL,
  x_label = expression(tau[i]),
  show_zero_line = TRUE,
  fill_colour = "white",
  border_colour = NA,
  colour_by_sign = TRUE,
  colour_below = "#4f88c6",
  colour_above = "#d8a739",
  zero_line_colour = "red",
  zero_line_alpha = 0.5,
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE,
  remove_underscores = TRUE,
  free_scales = FALSE,
  theme = "void"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{models\_list}] list of models or results object containing models with
tau\_hat values. can be a nested list structure like
`models\_binary\$results`, or the parent `models\_binary` object itself
(in which case `\$results` will be extracted automatically).

\item[\code{label\_mapping}] optional named list for transforming model names to
display labels. if null, uses automatic label transformation.

\item[\code{binwidth}] numeric; width of histogram bins. default 0.01.

\item[\code{base\_size}] numeric; base font size for the plot. default 14.

\item[\code{ncol}] integer; number of columns in facet grid. if null, automatically
determined based on number of models.

\item[\code{title}] character; main title for the plot. default null.

\item[\code{subtitle}] character; subtitle for the plot. default null.

\item[\code{x\_label}] character; label for x-axis. default uses expression for tau.

\item[\code{show\_zero\_line}] logical; whether to show vertical line at zero. default true.

\item[\code{fill\_colour}] character; fill colour for histogram bars when colour\_by\_sign is false. default "white".

\item[\code{border\_colour}] character; border colour for histogram bars. default NA (no border).

\item[\code{colour\_by\_sign}] logical; whether to colour bars differently above/below zero. default true.

\item[\code{colour\_below}] character; colour for bars below zero when colour\_by\_sign is true. default "\#4f88c6".

\item[\code{colour\_above}] character; colour for bars above zero when colour\_by\_sign is true. default "\#d8a739".

\item[\code{zero\_line\_colour}] character; colour for zero line. default "red".

\item[\code{zero\_line\_alpha}] numeric; transparency for zero line. default 0.5.

\item[\code{remove\_tx\_prefix}] logical; remove time prefixes from model names. default true.

\item[\code{remove\_z\_suffix}] logical; remove \_z suffix from model names. default true.

\item[\code{use\_title\_case}] logical; convert labels to title case. default true.

\item[\code{remove\_underscores}] logical; replace underscores with spaces. default true.

\item[\code{free\_scales}] logical; whether to allow free scales in facets. default false
to maintain fixed range across all facets.

\item[\code{theme}] character; ggplot2 theme to use. options include "classic", "minimal",
"bw", "light", "dark", "void", "grey", "linedraw". default "void".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a ggplot object with faceted tau hat distributions
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# with label mapping - pass models_binary directly
label_map <- list(
  "model_t2_belong_z" = "Social Belonging",
  "model_t2_trust_z" = "Trust in Others",
  "model_t2_log_charity_donate_z" = "Charitable Donations",
  "model_t2_log_hours_charity_z" = "Volunteer Hours"
)

# method 1: pass the parent object (auto-extracts $results)
tau_plot <- margot_plot_tau(
  models_binary,
  label_mapping = label_map,
  title = "Individual Treatment Effects"
)

# method 2: pass $results directly (also works)
tau_plot <- margot_plot_tau(
  models_binary$results,
  label_mapping = label_map,
  title = "Individual Treatment Effects"
)

# with different theme
tau_plot <- margot_plot_tau(
  models_binary,
  label_mapping = label_map,
  title = "Individual Treatment Effects",
  theme = "minimal"
)

# without conditional colouring
tau_plot <- margot_plot_tau(
  models_binary,
  label_mapping = label_map,
  title = "Individual Treatment Effects",
  colour_by_sign = FALSE,
  fill_colour = "lightblue"
)

# with custom colours for above/below zero
tau_plot <- margot_plot_tau(
  models_binary,
  label_mapping = label_map,
  title = "Individual Treatment Effects",
  colour_below = "darkred",
  colour_above = "darkgreen"
)

# add borders if desired
tau_plot <- margot_plot_tau(
  models_binary,
  label_mapping = label_map,
  border_colour = "grey50"
)

# without label mapping (auto transform)
tau_plot <- margot_plot_tau(models_binary)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_policy}{Batch Processing of Policy Trees and Related Visualisations}{margot.Rul.policy}
%
\begin{Description}
Process a list of multi-arm causal forest results: generate policy-tree and
decision-tree plots, Qini curves, and difference-gain summaries.
Users can toggle which outputs to include via the `output\_objects` parameter.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_policy(
  result_outcomes,
  policy_tree_args = list(),
  decision_tree_args = list(),
  max_depth = 2L,
  dpi = 600,
  width = 12,
  height = 12,
  save_plots = TRUE,
  output_dir = here::here(push_mods),
  spend = c(0.2, 0.5),
  label_mapping = NULL,
  original_df = NULL,
  model_names = NULL,
  output_objects = c("policy_tree", "decision_tree", "combined_plot", "qini_plot",
    "diff_gain_summaries")
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{result\_outcomes}] List returned by \code{margot\_multi\_arm\_causal\_forest()}.

\item[\code{policy\_tree\_args}] List of args for \code{margot\_plot\_policy\_tree()}. Default: \code{list()}.

\item[\code{decision\_tree\_args}] List of args for \code{margot\_plot\_decision\_tree()}. Default: \code{list()}.

\item[\code{max\_depth}] Integer, 1 or 2; which decision tree depth to plot. Default: 2.

\item[\code{dpi}] Resolution (dpi) for saved plots. Default: 600.

\item[\code{width}] Width (in inches) for saved plots. Default: 12.

\item[\code{height}] Height (in inches) for saved plots. Default: 12.

\item[\code{save\_plots}] Logical; save plots to disk? Default: TRUE.

\item[\code{output\_dir}] Directory for saving plots. Default: \code{here::here(push\_mods)}.

\item[\code{spend}] Numeric vector of spend levels for difference-gain summaries. Default: \code{c(0.2, 0.5)}.

\item[\code{label\_mapping}] Named list mapping variable names to display labels. Default: NULL.

\item[\code{original\_df}] Optional data.frame of untransformed variables for axis annotations. Default: NULL.

\item[\code{model\_names}] Character vector of model names to process; NULL = all. Default: NULL.

\item[\code{output\_objects}] Character vector specifying which outputs to include.
Options: "policy\_tree", "decision\_tree", "combined\_plot", "qini\_plot", "diff\_gain\_summaries".
Default: all.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A named list; each element corresponds to a model and contains only
the requested outputs.
\end{Value}
\HeaderA{margot\_process\_binary\_vars}{Process Binary Variables in a Data Frame}{margot.Rul.process.Rul.binary.Rul.vars}
%
\begin{Description}
This function identifies binary variables (both factors and numeric),
converts them to 0/1 format, and renames them. It allows for exceptions
to be specified, and ignores variables whose names already end with the
specified suffix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_process_binary_vars(data, exceptions = character(0), suffix = "_binary")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame to process.

\item[\code{exceptions}] A character vector of column names to exclude from processing.

\item[\code{suffix}] A string to append to renamed binary variables. Default is "\_binary".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with processed binary variables.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
df <- data.frame(
  a = factor(c("yes", "no", "yes")),
  b = c(1, 0, 1),
  c = c("apple", "banana", "apple"),
  d = factor(c("true", "false", "true")),
  e_binary = c(0, 1, 0)
)
processed_df <- margot_process_binary_vars(df, exceptions = "c")

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_process\_longitudinal\_data}{process longitudinal data for three waves}{margot.Rul.process.Rul.longitudinal.Rul.data}
%
\begin{Description}
this function processes longitudinal data for exactly three waves (t0, t1, t2).
it handles attrition, scales baseline variables, and optionally encodes ordinal variables.
note: this function is currently implemented for three waves only.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_process_longitudinal_data(
  df_wide,
  ordinal_columns = NULL,
  continuous_columns_keep = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df\_wide}] a wide-format dataframe containing longitudinal data for three waves.

\item[\code{ordinal\_columns}] a character vector of column names to be treated as ordinal and dummy-coded.

\item[\code{continuous\_columns\_keep}] a character vector of continuous column names to keep without scaling.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
the function performs the following steps:
1. creates na conditions for t0 and t1 based on missingness in subsequent waves.
2. handles non-factor and factor columns, applying attrition logic.
3. scales numeric baseline (t0) variables.
4. selects and orders columns.
5. optionally encodes ordinal columns.
\end{Details}
%
\begin{Value}
a processed dataframe suitable for use in longitudinal analyses with three waves.
\end{Value}
%
\begin{Note}
this function is specifically designed for datasets with exactly three waves (t0, t1, t2).
it may not work correctly for datasets with fewer or more waves.
\end{Note}
%
\begin{Examples}
\begin{ExampleCode}
# assuming df_wide is your wide-format dataframe with three waves
processed_data <- margot_process_longitudinal_data(
  df_wide,
  ordinal_columns = c("education", "income_category"),
  continuous_columns_keep = c("age", "bmi")
)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_process\_longitudinal\_data\_wider}{Process longitudinal dyadic data in wide format with censoring by missing exposure and silent dummy-coding}{margot.Rul.process.Rul.longitudinal.Rul.data.Rul.wider}
%
\begin{Description}
this function processes longitudinal data (wide format) across multiple waves,
handling dyadic censoring and optional censoring when the exposure is missing.

if `censor\_if\_missing\_exposure = TRUE`, any record with a missing exposure at wave t+1
is considered lost at wave t and all subsequent wave data are set to `NA`. dyadic logic
ensures that if any member of a dyad is lost, the entire dyad is censored.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_process_longitudinal_data_wider(
  df_wide,
  relationship_id = "NULL",
  ordinal_columns = NULL,
  continuous_columns_keep = NULL,
  exposure_vars = NULL,
  scale_exposure = FALSE,
  scale_continuous = TRUE,
  censor_if_missing_exposure = TRUE,
  not_lost_in_following_wave = "not_lost_following_wave",
  lost_in_following_wave = NULL,
  remove_selected_columns = TRUE,
  time_point_prefixes = NULL,
  time_point_regex = NULL,
  save_observed_y = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df\_wide}] a wide-format `data.frame` containing time-prefixed columns (e.g., `t0\_x`).

\item[\code{relationship\_id}] column name identifying dyads; if present, dyadic censoring is applied. default: `"NULL"`.

\item[\code{ordinal\_columns}] character vector of ordinal column bases to dummy-encode after processing.

\item[\code{continuous\_columns\_keep}] character vector of continuous column names to retain without scaling.

\item[\code{exposure\_vars}] character vector of exposure base names (without time prefixes).

\item[\code{scale\_exposure}] logical; if `TRUE`, scales exposure variables. default: `FALSE`.

\item[\code{scale\_continuous}] logical; if `TRUE`, scales continuous variables. default: `TRUE`.

\item[\code{censor\_if\_missing\_exposure}] logical; if `TRUE`, missing exposures at next wave cause censoring. default: `TRUE`.

\item[\code{not\_lost\_in\_following\_wave}] suffix for the not-lost indicator. default: `"not\_lost\_following\_wave"`.

\item[\code{lost\_in\_following\_wave}] suffix for the lost indicator; if `NULL`, no lost indicator is added.

\item[\code{remove\_selected\_columns}] logical; if `TRUE`, removes original columns after dummy encoding. default: `TRUE`.

\item[\code{time\_point\_prefixes}] optional vector of time-point prefixes (e.g., `c("t0","t1")`). if `NULL`, inferred from data.

\item[\code{time\_point\_regex}] regex pattern to identify time-point prefixes. used if `time\_point\_prefixes` is `NULL`.

\item[\code{save\_observed\_y}] logical; if `TRUE`, retains observed outcomes in final wave even if censored. default: `FALSE`.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a `data.frame` with processed and optionally scaled and encoded columns, ready for longitudinal analysis.
\end{Value}
\HeaderA{margot\_propensity\_model\_and\_plots}{Create Propensity Score Model and Associated Plots}{margot.Rul.propensity.Rul.model.Rul.and.Rul.plots}
%
\begin{Description}
This function creates a propensity score model using the specified exposure variable
and baseline covariates. It also generates associated plots and diagnostics.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_propensity_model_and_plots(
  df_propensity,
  exposure_variable,
  baseline_vars,
  weights_var_name,
  estimand = "ATE",
  method = "ebal",
  focal = NULL,
  love_plot_options = list(),
  bal_tab_options = list(),
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df\_propensity}] A data frame containing the variables for the propensity score model.

\item[\code{exposure\_variable}] A character string specifying the name of the exposure variable in df\_propensity.

\item[\code{baseline\_vars}] A character vector specifying the names of the baseline covariates in df\_propensity.

\item[\code{weights\_var\_name}] A character string specifying the name of the weights variable in df\_propensity.

\item[\code{estimand}] A character string specifying the estimand. Default is "ATE" (Average Treatment Effect).

\item[\code{method}] A character string specifying the method for propensity score estimation. Default is "ebal".

\item[\code{focal}] For binary treatments, a value of the treatment to be considered "focal" (i.e., the intervention). Default is NULL.

\item[\code{love\_plot\_options}] A list of options to be passed to cobalt::love.plot(). Default is an empty list.

\item[\code{bal\_tab\_options}] A list of options to be passed to cobalt::bal.tab(). Default is an empty list.

\item[\code{verbose}] A logical value indicating whether to print progress messages. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the following elements:
\begin{itemize}

\item{} match\_propensity: The propensity score model object.
\item{} love\_plot: A love plot object created by cobalt::love.plot().
\item{} summary: A summary of the propensity score model.
\item{} summary\_plot: A plot of the propensity score model summary.
\item{} balance\_table: A balance table created by cobalt::bal.tab().
\item{} diagnostics: A list of additional diagnostic information.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming df_propensity is your dataset with appropriate variables
results <- margot_propensity_model_and_plots(
  df_propensity = df_propensity,
  exposure_variable = "treatment",
  baseline_vars = c("age", "sex", "bmi"),
  weights_var_name = "sample_weights",
  love_plot_options = list(
    thresholds = c(m = .05),
    size = 4
  ),
  bal_tab_options = list(
    thresholds = c(m = .1, v = 2.5)
  ),
  verbose = TRUE
)

# Access the results
print(results$summary)
plot(results$love_plot)
print(results$balance_table)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_prop\_missing}{Proportion of missing data at baseline}{margot.Rul.prop.Rul.missing}
%
\begin{Description}
This function calculates the proportion of missing data at a baseline wave.
If a wave column is present, it uses the lowest number or lowest factor level as the baseline.
If no wave column is found, it issues a warning and uses the entire dataset.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_prop_missing(data, wave_col = "wave")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the dataset.

\item[\code{wave\_col}] A character string. Name of the column that indicates the wave.
The default is "wave".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric value representing the proportion of missing data at the baseline wave.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example using a dataset with a wave column
# assume dat_long_amelia_log has a column called wave
margot_prop_missing(dat_long_amelia_log)

# Example using a dataset without a wave column
# assume some_data is a dataset with no wave column
margot_prop_missing(some_data)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_rate}{Assemble RATE tables (AUTOC and QINI)}{margot.Rul.rate}
%
\begin{Description}
Convenience wrapper around margot\_rate\_batch(). Returns two data frames,
both sorted by descending RATE Estimate, with reliable results highlighted in bold.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_rate(
  models,
  model_names = NULL,
  policy = c("treat_best", "withhold_best"),
  round_digits = 3,
  highlight_significant = TRUE,
  label_mapping = NULL,
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE,
  remove_underscores = TRUE,
  adjust = NULL,
  alpha = 0.05,
  apply_adjustment = !is.null(adjust),
  seed = 12345
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{models}] List from margot\_causal\_forest().

\item[\code{model\_names}] Optional character vector specifying which models to process.
Default NULL (all models).

\item[\code{policy}] Character; "treat\_best" (default) or "withhold\_best".

\item[\code{round\_digits}] Integer; decimal places (default 3).

\item[\code{highlight\_significant}] Logical; bold outcomes whose 95 percent CI excludes 0 (default TRUE).
Note: Only positive significant effects are bolded, negative effects are not bolded.

\item[\code{label\_mapping}] Named character vector for converting variable names to readable labels.

\item[\code{remove\_tx\_prefix}] Logical; remove treatment prefix from variable names (default TRUE).

\item[\code{remove\_z\_suffix}] Logical; remove z-score suffix from variable names (default TRUE).

\item[\code{use\_title\_case}] Logical; convert variable names to title case (default TRUE).

\item[\code{remove\_underscores}] Logical; replace underscores with spaces (default TRUE).

\item[\code{adjust}] Character; method for adjusting p-values, passed to stats::p.adjust().
Options include "holm", "hochberg", "hommel", "bonferroni", "BH", "BY", "fdr", "none".
Default is NULL (no adjustment).

\item[\code{alpha}] Numeric; significance threshold (default 0.05).

\item[\code{apply\_adjustment}] Logical; if TRUE, compute p-values and apply adjustment method.
If FALSE, just document the adjustment method without recomputing. Default is TRUE when
adjust parameter is provided, FALSE otherwise.

\item[\code{seed}] Integer; base seed for reproducible RATE computations (default 12345).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with elements:
* rate\_autoc: AUTOC RATE table
* rate\_qini: QINI RATE table
\end{Value}
\HeaderA{margot\_rate\_batch}{Batch-compute RATEs for each outcome in a margot\_causal\_forest result}{margot.Rul.rate.Rul.batch}
%
\begin{Description}
This replaces the legacy internal RATE code and is the only function that
actually talks to grf. It flips the CATE vector on the fly when
policy is "withhold\_best".
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_rate_batch(
  model_results,
  policy = c("treat_best", "withhold_best"),
  target = c("AUTOC", "QINI"),
  level = 0.95,
  round_digits = 3,
  model_prefix = "model_",
  verbose = TRUE,
  seed = 12345
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] List returned by margot\_causal\_forest(), containing
results and full\_models.

\item[\code{policy}] Character; either "treat\_best" (default) or "withhold\_best".

\item[\code{target}] Character; weighting scheme: "AUTOC" (default) or "QINI".

\item[\code{level}] Numeric; Wald confidence level (default 0.95).

\item[\code{round\_digits}] Integer; decimal places to keep (default 3).

\item[\code{model\_prefix}] Character; common prefix on model names (default "model\_").

\item[\code{verbose}] Logical; print progress with cli (default TRUE).

\item[\code{seed}] Integer; base seed for reproducible RATE computations (default 12345).
\end{ldescription}
\end{Arguments}
\HeaderA{margot\_rescue\_qini}{Post-process models to recover Qini curves via propensity trimming}{margot.Rul.rescue.Rul.qini}
%
\begin{Description}
for any model in a results list whose qini\_objects are null or empty,
this function applies an overlap restriction on the estimated\bsl{} n\#' propensity scores and recomputes inâ€‘sample Qini curves on the trimmed data,
without touching the original ATE or forest objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_rescue_qini(
  model_results,
  propensity_bounds = c(0.05, 0.95),
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] a list from margot\_causal\_forest() with
save\_models = TRUE and save\_data = TRUE

\item[\code{propensity\_bounds}] numeric lengthâ€‘2 vector of lower/upper
bounds for forest\$W.hat (default c(0.05, 0.95))

\item[\code{verbose}] logical; if TRUE prints progress messages (default TRUE)
\end{ldescription}
\end{Arguments}
%
\begin{Value}
modified model\_results with rescued qini\_data and qini\_objects
for any models that initially had empty gain
\end{Value}
\HeaderA{margot\_reversed\_labels}{Update label map by marking reversed outcomes}{margot.Rul.reversed.Rul.labels}
%
\begin{Description}
Helper to tag reversed outcomes by appending "(reversed)" to labels.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_reversed_labels(label_map, reversed)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{label\_map}] named list mapping variable names to human-readable labels

\item[\code{reversed}] character vector of variable names that have been flipped
\end{ldescription}
\end{Arguments}
%
\begin{Value}
named list of labels, with "(reversed)" appended to specified entries
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
label_mapping_all <- list(
  t2_log_hours_exercise_z        = "Hours of Exercise (log)",
  t2_hlth_fatigue_z              = "Fatigue",
  t2_kessler_latent_anxiety_z    = "Anxiety",
  t2_kessler_latent_depression_z = "Depression",
  t2_rumination_z                = "Rumination",
  t2_foregiveness_z              = "Forgiveness",
  t2_perfectionism_z             = "Perfectionism",
  t2_self_esteem_z               = "Self Esteem",
  t2_gratitude_z                 = "Gratitude",
  t2_lifesat_z                   = "Life Satisfaction",
  t2_meaning_purpose_z           = "Meaning: Purpose",
  t2_meaning_sense_z             = "Meaning: Sense",
  t2_pwi_z                       = "Personal Well-being Index",
  t2_belong_z                    = "Social Belonging",
  t2_neighbourhood_community_z   = "Neighbourhood Community",
  t2_support_z                   = "Social Support"
)

flip_outcomes <- c(
  "t2_kessler_latent_anxiety_z",
  "t2_kessler_latent_depression_z",
  "t2_rumination_z"
)

# update mapping
label_mapping_all <- mark_reversed_labels(label_mapping_all, flip_outcomes)

print(label_mapping_all)
#> $t2_log_hours_exercise_z
#> [1] "Hours of Exercise (log)"
#> ...
#> $t2_kessler_latent_anxiety_z
#> [1] "Anxiety (reversed)"
#> ...
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_save\_png}{Save Margot Plot as PNG}{margot.Rul.save.Rul.png}
%
\begin{Description}
This function takes the output of either `margot\_plot()` or `margot\_plot\_multi\_arm()`
and saves the plot as a PNG image using `ggsave()`.

This function saves a plot object as a PNG image, supporting multiple plotting systems
including ggplot2, base R plots, and other plotting libraries.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_save_png(
  plot_output,
  prefix = NULL,
  base_filename = "plot",
  save_path = here::here(push_mods),
  width = 12,
  height = 12,
  dpi = 500
)

margot_save_png(
  plot_output,
  prefix = NULL,
  base_filename = "plot",
  save_path = here::here(push_mods),
  width = 12,
  height = 12,
  dpi = 500
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{plot\_output}] Either a plot object directly, or a list containing a plot element.
Supported plot types include ggplot objects, base R plot objects, and others.

\item[\code{prefix}] Character string. A prefix to add to the filename. Default is NULL.

\item[\code{base\_filename}] Character string. The base name for the saved file. Default is "plot".

\item[\code{save\_path}] Character string. The directory path where the image will be saved.
Default is here::here("push\_mods").

\item[\code{width}] Numeric. The width of the saved image in inches. Default is 16.

\item[\code{height}] Numeric. The height of the saved image in inches. Default is 8.

\item[\code{dpi}] Numeric. The resolution of the saved image in dots per inch. Default is 500.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
This function uses `ggsave()` to save the Margot plot as a PNG image.
If the save\_path directory doesn't exist, it will be created.
The final filename will be constructed as: `prefix\_base\_filename.png`.

This function handles different types of plot objects and saves them as PNG images:
- For ggplot2 objects: Uses ggsave()
- For other plot types: Uses png() and dev.off()
The final filename will be constructed as: `prefix\_base\_filename.png`.
\end{Details}
%
\begin{Value}
Invisibly returns the path of the saved file.

Invisibly returns the path of the saved file.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming you have already run margot_plot() or margot_plot_multi_arm()
plot_result <- margot_plot(your_data, your_options)

# Save the plot as PNG
margot_save_png(
  plot_result,
  prefix = "study1",
  base_filename = "treatment_effects",
  save_path = here::here("output", "plots")
)

## End(Not run)

## Not run: 
# For a ggplot object
plot_result <- ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point()
margot_save_png(plot_result, prefix = "study1", base_filename = "scatter")

# For a base R plot
plot_result <- plot(mtcars$mpg, mtcars$wt)
margot_save_png(plot_result, prefix = "study1", base_filename = "scatter")

# For a list containing a plot
plot_list <- list(plot = ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point())
margot_save_png(plot_list, prefix = "study1", base_filename = "scatter")

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_size}{Calculate the size of an R object in megabytes}{margot.Rul.size}
%
\begin{Description}
This function takes an R object and returns its size in megabytes (MB).
It's useful for understanding the memory footprint of large data structures
or complex objects in your R environment. It now includes cli alerts for
improved user feedback.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_size(obj, name = "Object")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{obj}] An R object whose size you want to measure

\item[\code{name}] An optional name for the object (default is "Object")
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character string representing the size of the object in MB,
formatted to two decimal places
\end{Value}
%
\begin{Note}
KEY MESSAGE: Monitoring object sizes is crucial for efficient memory
management, especially when working with large datasets or complex
analyses. Use this function to keep track of memory usage and optimize
your R code for better performance.
\end{Note}
%
\begin{Examples}
\begin{ExampleCode}
big_matrix <- matrix(rnorm(1e6), nrow = 1000)
margot_size(big_matrix, "Big Matrix")

summary_tables <- list(table1 = data.frame(a = 1:1000, b = rnorm(1000)))
margot_size(summary_tables, "Summary Tables")

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_subset\_batch}{Batch Process Subset Models for Causal Forests}{margot.Rul.subset.Rul.batch}
%
\begin{Description}
We compared treatment effects across multiple subsets derived from causal forest models (using the grf package).
For each subset, the number of participants, the total sample size, and the corresponding percentage are computed,
describing the composition of each targeted subpopulation. In addition, for each subset the plot, interpretation, and
transformed table are generated to summarise the causal effect estimates. Note that exploratory comparisons in causal
forest analyses should be interpreted with caution, as subsetting targeted subpopulations may yield unstable treatment
effect estimates, typically indicated by large standard errors and wide confidence intervals.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_subset_batch(
  model_results,
  X,
  base_defaults = list(),
  title = "",
  label_mapping = NULL,
  subsets,
  debug = FALSE,
  original_df = NULL,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] Results from the causal forest model.

\item[\code{X}] Covariate matrix.

\item[\code{base\_defaults}] A list of default plotting options.

\item[\code{title}] A character string for the main title of the plots.

\item[\code{label\_mapping}] A named vector mapping variable names to labels.

\item[\code{subsets}] A named list of subset definitions. Each element should be a list containing:
\begin{description}

\item[var] A character string specifying the variable to subset on.
\item[value] The value used to define the subset.
\item[operator] (Optional) A comparison operator (default is \code{"=="}).
\item[subset\_condition] (Optional) A pre-computed logical vector defining the subset.
\item[description] (Optional) A character string describing the subset.
\item[label] (Optional) A user-friendly label for the subset. If missing, the list name is used.

\end{description}


\item[\code{debug}] Logical; if \code{TRUE}, prints debugging information.

\item[\code{original\_df}] Optional data frame containing the original (non-transformed) data for back-transformation.

\item[\code{...}] Additional arguments to be passed to margot\_plot().
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with the following elements:
\begin{description}

\item[results] A list of subset results, each containing the plot, interpretation, and transformed table along with sample statistics.
\item[summary] A data frame summarising the sample size, total sample size, and percentage for each subset.
\item[explanation] A character string providing a comprehensive explanation of the subsetting process and the results for each model.

\end{description}

\end{Value}
\HeaderA{margot\_subset\_model}{Subset Model Results for Binary and Categorical Exposures}{margot.Rul.subset.Rul.model}
%
\begin{Description}
This function extracts and combines evaluation tables for specified outcome variables
from model results, handling both binary and categorical exposure models. It provides
enhanced functionality for creating and analyzing subsets based on various conditions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_subset_model(
  model_results,
  outcome_vars = NULL,
  subset_condition = NULL,
  scale = "RD",
  contrast = NULL,
  X = NULL,
  subset_var = NULL,
  subset_value = 1,
  subset_operator = "==",
  subset_description = NULL,
  debug = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_results}] A list of model results from `model\_causal\_forest` or similar functions.

\item[\code{outcome\_vars}] Optional. A character vector of outcome variable names. If NULL,
the function will use all models in the input.

\item[\code{subset\_condition}] A logical vector indicating the subset of data to use. Default is NULL.
Can be generated using the subset\_var, subset\_value, and subset\_operator parameters.

\item[\code{scale}] A character string indicating the scale to use. Default is "RD".

\item[\code{contrast}] For categorical exposures, a single character string specifying the contrast to extract.

\item[\code{X}] Optional. The feature matrix used in the original models. Required if using subset\_var.

\item[\code{subset\_var}] Optional. The name of the variable to use for subsetting.

\item[\code{subset\_value}] Optional. The value to compare against for subsetting. Default is 1.

\item[\code{subset\_operator}] Optional. The operator to use for comparison ("==", ">", ">=", "<", "<=", "!="). Default is "==".

\item[\code{subset\_description}] Optional. A description of the subset for reporting. If NULL, one will be generated.

\item[\code{debug}] Logical. If TRUE, print debug information. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function can be used in two ways:
1. By providing a pre-computed logical vector as `subset\_condition`.
2. By providing a variable name (`subset\_var`), value (`subset\_value`), and operator (`subset\_operator`)
to generate the subset condition automatically.

For categorical exposures, specify the contrast parameter to extract specific comparisons.
\end{Details}
%
\begin{Value}
A list containing:
\begin{ldescription}
\item[\code{results}] A data frame combining all custom evaluation tables for the specified outcomes and contrast.
\item[\code{subset\_condition}] The logical vector used for subsetting.
\item[\code{subset\_description}] A description of the subset.
\item[\code{subset\_info}] Additional information about the subset, if available.
\end{ldescription}
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Example 1: Basic subsetting with a pre-computed condition
subset_condition_conservative <- X[, "t0_political_conservative_z"] > 1
model_subset_conservative <- margot_subset_model(
  model_results = models_binary,
  subset_condition = subset_condition_conservative,
  debug = FALSE
)

# Example 2: Using the built-in subsetting functionality
model_subset_religious <- margot_subset_model(
  model_results = models_binary,
  X = X,
  subset_var = "t0_religion_bigger_denominations_not_rel_binary",
  subset_value = 1,
  subset_description = "Effects among religious participants",
  debug = TRUE
)

# Example 3: For categorical exposures with specific contrast
model_subset_gen_z <- margot_subset_model(
  model_results = models_cat,
  X = X,
  subset_var = "t0_gen_cohort_gen_Z_binary",
  subset_value = 1,
  contrast = "[6.0,7.0] - [1.0,5.0)",
  scale = "RD",
  debug = FALSE
)

# Example 4: Multiple outcome variables
model_subset_boomers <- margot_subset_model(
  model_results = models_binary,
  outcome_vars = c("t2_wellbeing_z", "t2_depression_z"),
  X = X,
  subset_var = "t0_gen_cohort_gen_Boomers_binary",
  subset_value = 1,
  subset_operator = "==",
  subset_description = "Effects among Baby Boomer participants",
  debug = TRUE
)

# Example 5: More complex subsetting (multiple conditions)
# Define a complex condition directly
complex_condition <- X[, "t0_political_conservative_z"] > 1 & X[, "t0_age_z"] > -2
model_subset_complex <- margot_subset_model(
  model_results = models_binary,
  subset_condition = complex_condition,
  subset_description = "Conservative (>1 SD) and not very young (>-2 SD in age)",
  debug = FALSE
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_summary\_cate\_difference\_gain}{Compute Difference in Gains and Integrated Difference Between Reference and Comparison Curves}{margot.Rul.summary.Rul.cate.Rul.difference.Rul.gain}
%
\begin{Description}
This function computes the difference in average gains and the integrated difference
between a reference curve (maq object) and a comparison curve at a specified spend level.
It returns a list of formatted strings for easy use in Quarto markdown text.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_summary_cate_difference_gain(
  mc_result,
  outcome_var,
  reference_curve,
  comparison_curve,
  spend,
  digits = 3
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{mc\_result}] A list containing the results from margot\_multi\_arm\_causal\_forest().

\item[\code{outcome\_var}] A character string specifying the name of the outcome variable.

\item[\code{reference\_curve}] A character string specifying the name of the reference Qini curve (e.g., "baseline").

\item[\code{comparison\_curve}] A character string specifying the name of the comparison Qini curve (e.g., "arm2").

\item[\code{spend}] A numeric value specifying the spend level (between 0 and 1).

\item[\code{digits}] An integer specifying the number of decimal places to round the output. Default is 3.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing formatted strings for use in Quarto markdown text:
\begin{ldescription}
\item[\code{diff\_gain}] Formatted string for difference in gains
\item[\code{int\_diff}] Formatted string for integrated difference
\item[\code{summary}] A summary sentence of the comparison
\end{ldescription}
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming mc_result is the result of margot_multi_arm_causal_forest()
result <- margot_summary_cate_difference_gain(mc_result,
                                 outcome_var = "model_Y",
                                 reference_curve = "baseline",
                                 comparison_curve = "arm2",
                                 spend = 0.3)
# Use in text
glue::glue("The difference in gains is {result$diff_gain}. {result$summary}")

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_summary\_panel}{Generate Summary Panel for Margot Study}{margot.Rul.summary.Rul.panel}
%
\begin{Description}
This function creates summary tables for a panel study, including unique IDs by wave,
participant wave summary, and grouped participant wave summary.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_summary_panel(
  data,
  output_format = "html",
  group_waves_at = 3,
  id_col = "id",
  wave_col = "wave",
  year_measured_col = "year_measured"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the panel study data.

\item[\code{output\_format}] Character string specifying the output format: "html" (default) or "markdown".

\item[\code{group\_waves\_at}] Numeric value specifying at which number of waves to start grouping (default is 3).

\item[\code{id\_col}] Character string specifying the name of the ID column (default is "id").

\item[\code{wave\_col}] Character string specifying the name of the wave column (default is "wave").

\item[\code{year\_measured\_col}] Character string specifying the name of the year measured column (default is "year\_measured").
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing three elements: unique\_ids\_by\_wave, participant\_wave\_summary, and participant\_wave\_summary\_grouped.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Assuming 'dat' is your dataset
results <- margot_summary_panel(dat)

# Custom settings
custom_results <- margot_summary_panel(
  data = dat,
  output_format = "markdown",
  group_waves_at = 4,
  id_col = "participant_id",
  wave_col = "survey_wave",
  year_measured_col = "measurement_year"
)

# View results
results$unique_ids_by_wave
results$participant_wave_summary
results$participant_wave_summary_grouped

# For markdown output
cat(custom_results$unique_ids_by_wave)
cat(custom_results$participant_wave_summary)
cat(custom_results$participant_wave_summary_grouped)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_transition\_table}{Format a Transition Table with observedâ€‘indicator filtering}{margot.Rul.transition.Rul.table}
%
\begin{Description}
Reshapes a longâ€‘format transition frequency data frame into a
wideâ€‘format table with totals, and formats it using markdown.
Diagonal elements are bolded to highlight state stability.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_transition_table(
  data,
  state_var,
  id_var,
  wave_var,
  waves = NULL,
  state_names = NULL,
  observed_var = NULL,
  observed_val = 1,
  table_name = "transition_table"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] a data frame containing your id, state, wave, and (optionally) an observed indicator.

\item[\code{state\_var}] name of the column indicating the state at each wave.

\item[\code{id\_var}] name of the column identifying each participant.

\item[\code{wave\_var}] name of the column indicating the wave (numeric or factor).

\item[\code{waves}] optional numeric vector of waves to include (defaults to all present in data).

\item[\code{state\_names}] optional character vector of labels for each state.

\item[\code{observed\_var}] optional name of a column marking participants still observed.

\item[\code{observed\_val}] value in \code{observed\_var} that denotes â€œobservedâ€ (default 1).

\item[\code{table\_name}] name for the output object (default "transition\_table").
\end{ldescription}
\end{Arguments}
%
\begin{Value}
an object of class \code{margot\_transitions} with
\code{tables}, \code{explanation}, \code{wave\_info}, and a
\code{quarto\_code} helper.
\end{Value}
\HeaderA{margot\_trim\_sample\_weights}{Standardise and (optionally) trim sample weights at both ends}{margot.Rul.trim.Rul.sample.Rul.weights}
%
\begin{Description}
This function first trims any weights below a specified lowerâ€quantile threshold
and/or above a specified upperâ€quantile threshold, then standardises the
(possibly trimmed) weights to have a mean of 1.  Missing values are preserved.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_trim_sample_weights(
  weights_vec,
  lower_quantile = NULL,
  upper_quantile = 0.99
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{weights\_vec}] Numeric vector of sample weights.  Must be positive or NA.

\item[\code{lower\_quantile}] Numeric in (0,1); all weights below this quantile will
be raised (â€œwinsorised upâ€) to the lower quantile value.  If \code{NULL} or \code{<= 0},
lowerâ€end trimming is skipped.

\item[\code{upper\_quantile}] Numeric in (0,1); all weights above this quantile will
be lowered (â€œwinsorised downâ€) to the upper quantile value.  If \code{NULL} or \code{>= 1},
upperâ€end trimming is skipped.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Trimming both tails of inverse-probability weights can mitigate the influence
of implausibly small or large weights, trading a bit of bias for lower variance.
\end{Details}
%
\begin{Value}
A numeric vector the same length as \code{weights\_vec}, with extremes
winsorised and then rescaled to have mean 1.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
set.seed(42)
w <- c(rlnorm(90, 0, 0.5), runif(5, 5, 20), runif(5, 0, 0.01), NA)
summary(w)

# trim both lower 1% and upper 99%, then standardise
w_both <- margot_trim_sample_weights(
  w,
  lower_quantile = 0.01,
  upper_quantile = 0.99
)
summary(w_both)

# only upper trim at 95th percentile
w_up95 <- margot_trim_sample_weights(w, lower_quantile = NULL, upper_quantile = 0.95)
summary(w_up95)

# only lower trim at 5th percentile
w_low5 <- margot_trim_sample_weights(w, lower_quantile = 0.05, upper_quantile = NULL)
summary(w_low5)

# no trimming (both NULL), only standardise
w_std <- margot_trim_sample_weights(w, lower_quantile = NULL, upper_quantile = NULL)
summary(w_std)

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_wide\_impute\_baseline}{Transform longitudinal data to wide format and impute baseline (soft-deprecated)}{margot.Rul.wide.Rul.impute.Rul.baseline}
\keyword{internal}{margot\_wide\_impute\_baseline}
%
\begin{Description}
**Deprecated**: Use `margot\_wide\_machine(imputation\_method = "mice")` instead.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_wide_impute_baseline(.data, baseline_vars, exposure_var, outcome_vars)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{.data}] A data frame containing the longitudinal data in long format.

\item[\code{baseline\_vars}] A character vector of baseline variable names to include and impute.

\item[\code{exposure\_var}] A character vector specifying the names of exposure variables.

\item[\code{outcome\_vars}] A character vector specifying the names of outcome variables.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame in wide format with imputed baseline variables.
\end{Value}
%
\begin{Section}{Deprecated}

This function was soft-deprecated in version 1.0.38 and will be removed in a future major release.
\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
# Preferred: use margot_wide_machine with mice imputation
# wide_df <- margot_wide_machine(
#   df,
#   baseline_vars = c("age", "male"),
#   exposure_var = "forgiveness",
#   outcome_vars = "kessler_latent_anxiety",
#   imputation_method = "mice"
# )

\end{ExampleCode}
\end{Examples}
\HeaderA{margot\_wide\_machine}{Transform longitudinal data to wide format with baseline imputation and optional NA indicators}{margot.Rul.wide.Rul.machine}
%
\begin{Description}
This function transforms longitudinal data from long format to wide format,
ensuring that baseline measurements are correctly labeled and included.
It handles multiple observations per subject across an indefinite number of waves,
and allows for the specification of baseline variables, exposure variables,
outcome variables, and time-varying confounders.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
margot_wide_machine(
  .data,
  id = "id",
  wave = "wave",
  baseline_vars,
  exposure_var,
  outcome_vars,
  confounder_vars = NULL,
  imputation_method = c("median", "mice", "none"),
  include_exposure_var_baseline = TRUE,
  include_outcome_vars_baseline = TRUE,
  extend_baseline = FALSE,
  include_na_indicators = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{.data}] A data frame containing the longitudinal data in long format.

\item[\code{id}] The name of the ID column identifying subjects (default is "id").

\item[\code{wave}] The name of the wave/time column (default is "wave").

\item[\code{baseline\_vars}] A character vector of baseline variable names to be included at t0.

\item[\code{exposure\_var}] A character string specifying the name of the exposure variable to be tracked across time.

\item[\code{outcome\_vars}] A character vector of outcome variable names to be tracked across time.

\item[\code{confounder\_vars}] An optional character vector of time-varying confounder variable names to include without imputation (default is NULL).

\item[\code{imputation\_method}] A character string specifying the imputation method to use for baseline variables. Options are 'median' (default), 'mice', or 'none'.

\item[\code{include\_exposure\_var\_baseline}] Logical indicating whether to include the exposure variable at baseline (t0).

\item[\code{include\_outcome\_vars\_baseline}] Logical indicating whether to include outcome variables at baseline (t0).

\item[\code{extend\_baseline}] Logical indicating whether to include baseline\_vars in all subsequent waves (default FALSE).

\item[\code{include\_na\_indicators}] Logical indicating whether to generate NA indicator columns for baseline variables (default TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A wide-format data frame with each subject's observations across time points
represented in a single row. Baseline variables, exposure variables at baseline,
and outcome variables at baseline have missing values imputed as specified.
NA indicators are created for variables at baseline only if include\_na\_indicators is TRUE.
Exposure variables are tracked across waves but are not imputed beyond baseline.
Outcome variables are included only at the final wave unless include\_outcome\_vars\_baseline is TRUE.
Confounders (if any) are included without imputation.
\end{Value}
\HeaderA{prepare\_panel\_data}{Prepare Panel Data for Timeline Visualization}{prepare.Rul.panel.Rul.data}
%
\begin{Description}
This function prepares panel data for timeline visualization across multiple waves.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
prepare_panel_data(
  dat,
  wave_col = "wave",
  tscore_col = "tscore",
  id_col = "id",
  base_date = as.Date("1970-01-01"),
  wave_breaks = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{dat}] A data frame containing the panel data. Must include columns for wave, time score, and participant ID.

\item[\code{wave\_col}] The name of the column containing wave information. Default is "wave".

\item[\code{tscore\_col}] The name of the column containing time score information. Default is "tscore".

\item[\code{id\_col}] The name of the column containing participant IDs. Default is "id".

\item[\code{base\_date}] The base date for calculating the timeline. Default is "1970-01-01".

\item[\code{wave\_breaks}] A named list of date ranges for each wave. If NULL, waves will not be categorized.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing two elements:
\begin{ldescription}
\item[\code{df\_timeline}] A data frame with the processed timeline data
\item[\code{n\_total\_participants}] The total number of unique participants in the dataset
\end{ldescription}
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
dat <- read.csv("panel_data.csv")
wave_breaks <- list(
  "wave 1" = c(as.Date("2010-01-01"), as.Date("2010-12-31")),
  "wave 2" = c(as.Date("2011-01-01"), as.Date("2011-12-31"))
)
prepared_data <- prepare_panel_data(dat, wave_col = "Wave", tscore_col = "TimeScore",
                                    id_col = "ParticipantID", base_date = "2010-01-01",
                                    wave_breaks = wave_breaks)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{pretty\_number}{Format Numbers with Commas}{pretty.Rul.number}
%
\begin{Description}
This function takes a numeric vector and formats it by inserting commas as
thousands separators, making large numbers easier to read.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pretty_number(x)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A numeric vector that you want to format.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character vector where each number is formatted with commas
as thousands separators.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
numbers <- c(1000, 50000, 1234567)
pretty_number(numbers)

\end{ExampleCode}
\end{Examples}
\HeaderA{regress\_with\_covariates}{Generalized Linear Regression with Covariates}{regress.Rul.with.Rul.covariates}
%
\begin{Description}
This unitility function performs a generalized linear regression on a specified dataset using an outcome variable, an exposure variable, and a set of baseline covariates. By default, the function uses the Gaussian family (linear regression), but it allows for specifying other families for generalized linear models (GLM).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
regress_with_covariates(
  data,
  outcome,
  exposure,
  baseline_vars,
  family = gaussian(),
  sample_weights = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the variables for the analysis.

\item[\code{outcome}] A character string specifying the name of the outcome variable in the data frame.

\item[\code{exposure}] A character string specifying the name of the main exposure variable in the data frame.

\item[\code{baseline\_vars}] A character vector specifying the names of baseline covariates to include in the model in addition to the exposure variable.

\item[\code{family}] A family object or a character string naming the family (default is \code{gaussian()}, which performs linear regression). This parameter determines the error distribution and link function to be used in the model.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function constructs a model formula using the outcome, exposure, and baseline variables. It then fits a generalized linear model using this formula. The baseline variables are filtered to exclude the outcome and exposure variables before model fitting. The function prints the formula used for the regression analysis for verification.
\end{Details}
%
\begin{Value}
An object of class \code{glm} representing the fitted model, which includes coefficients, residuals, and other model diagnostics. This object can be further analyzed using standard methods for GLM objects, such as \code{summary()} for model summaries or \code{anova()} for analysis of variance.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# using `df_nz` is your data frame with "income" as the continuous outcome variable,
# "age" as an exposure variable, and other covariates
outcome_var <- "income"
exposure_var <- "age"
baseline_vars <- c("age", "education", "partner")
model <- regress_with_covariates(df_nz, outcome_var, exposure_var, baseline_vars, family =  gaussian())
summary(model)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{remove\_numeric\_attributes}{Remove Attributes from Numeric Columns in a Data Frame}{remove.Rul.numeric.Rul.attributes}
%
\begin{Description}
Iterates over each column in the provided data frame. If a column is numeric
and has attributes, this function removes those attributes by converting the column
to a basic numeric vector. This is particularly useful for cleaning data frames
after operations that may add undesired attributes to numeric columns, such as
aggregations or merges.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
remove_numeric_attributes(df)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A `data.frame` object from which attributes of numeric columns will be removed.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A `data.frame` with attributes removed from all numeric columns.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
df <- data.frame(a = I(1:3), b = c("x", "y", "z"), c = I(rnorm(3)))
cleaned_df <- remove_numeric_attributes(df)
str(cleaned_df)

\end{ExampleCode}
\end{Examples}
\HeaderA{select\_and\_rename\_cols}{Select and Rename Columns Based on Criteria}{select.Rul.and.Rul.rename.Rul.cols}
%
\begin{Description}
Selects columns from a base set that match specified baseline variables and renames
an outcome variable by changing its prefix. Useful for longitudinal data where
time-point prefixes need to be standardised or adjusted.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
select_and_rename_cols(
  names_base,
  baseline_vars,
  outcome,
  from_prefix = "t2",
  to_prefix = "t0"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{names\_base}] a character vector of column names from which to select.

\item[\code{baseline\_vars}] a character vector of baseline variables to match in `names\_base`.

\item[\code{outcome}] the name of the outcome variable whose prefix should be replaced.

\item[\code{from\_prefix}] the original prefix of the outcome variable to be replaced,
defaulting to "t2". the prefix should include any character immediately
preceding the numeric value and underscore, e.g., "t2\_".

\item[\code{to\_prefix}] the new prefix to replace the original prefix in the outcome
variable, defaulting to "t0". the prefix should be in the same format as
`from\_prefix`, including the character immediately preceding the numeric value
and underscore, e.g., "t0\_".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a character vector of selected column names with the outcome variable name
modified to reflect the new prefix.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
names_base <- c("t0_age", "t0_weight", "t0_height", "t0_outcome")
baseline_vars <- c("age", "weight")
outcome_var <- "t2_outcome"

final_columns <- select_and_rename_cols(names_base, baseline_vars, outcome_var, "t2", "t0")
print(final_columns)

\end{ExampleCode}
\end{Examples}
\HeaderA{strict\_exposure\_outcome\_censoring}{Strict All-or-Nothing Censoring for Longitudinal Data}{strict.Rul.exposure.Rul.outcome.Rul.censoring}
\keyword{internal}{strict\_exposure\_outcome\_censoring}
%
\begin{Description}
This function processes wide-format longitudinal data with multiple time points.
It is a wrapper around the internal function `.strict\_exposure\_outcome\_censoring`.
See the internal function documentation for details.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
strict_exposure_outcome_censoring(
  df_wide,
  exposure_vars,
  ordinal_columns = NULL,
  continuous_columns_keep = NULL,
  scale_exposure = FALSE,
  not_lost_in_following_wave = "not_lost_following_wave",
  lost_in_following_wave = "lost_following_wave",
  remove_selected_columns = TRUE,
  time_point_prefixes = NULL,
  time_point_regex = NULL,
  save_observed_y = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df\_wide}] A wide-format dataframe with columns like t0\_X, t1\_X, t2\_X, etc.

\item[\code{exposure\_vars}] Character vector of all exposure names (e.g. c("aaron\_antagonism", "aaron\_disinhibition", ...)).

\item[\code{ordinal\_columns}] Character vector of ordinal (factor) variables to be dummy-coded.

\item[\code{continuous\_columns\_keep}] Numeric columns you do NOT want to scale (e.g. if they must remain in original units).

\item[\code{scale\_exposure}] If FALSE, do not scale exposures; if TRUE, exposures are also scaled.

\item[\code{not\_lost\_in\_following\_wave}] Name for the "not lost" indicator (default "not\_lost\_following\_wave").

\item[\code{lost\_in\_following\_wave}] Name for the "lost" indicator (default "lost\_following\_wave").

\item[\code{remove\_selected\_columns}] If TRUE, remove original columns after dummy-coding ordinal columns.

\item[\code{time\_point\_prefixes}] Optional vector of wave prefixes (like c("t0","t1","t2")); if NULL, we auto-detect via regex.

\item[\code{time\_point\_regex}] Regex used to detect wave prefixes if `time\_point\_prefixes` is NULL.

\item[\code{save\_observed\_y}] If FALSE, set any missing final-wave outcomes to NA. If TRUE, keep partial final-wave outcomes.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A processed dataframe, with strict all-or-nothing censoring
\end{Value}
\HeaderA{tab\_engine\_marginal}{Tabulate Marginal Effects with E-Values}{tab.Rul.engine.Rul.marginal}
\keyword{internal}{tab\_engine\_marginal}
%
\begin{Description}
This function processes simulation results to tabulate marginal effects along with E-values,
providing a summary suited for reporting. It supports both risk difference (RD) and risk ratio (RR)
types of estimates and handles continuous and categorical treatment variables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tab_engine_marginal(
  x,
  new_name,
  delta = 1,
  sd = 1,
  type = c("RD", "RR"),
  continuous_X = FALSE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A data frame or matrix containing simulation results to be processed.

\item[\code{new\_name}] A new name to assign to the output row, typically describing the variable or model.

\item[\code{delta}] The assumed smallest worthwhile effect, used for E-value calculations.

\item[\code{sd}] The standard deviation of the effect estimate, used for E-value calculations.

\item[\code{type}] Character vector specifying the scale of effect size, either "RD" or "RR".
This parameter determines how the effects are calculated and presented.

\item[\code{continuous\_X}] Logical indicating whether the treatment variable X is continuous.
If TRUE, adjusts row names based on the type parameter.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with the specified new\_name as a row name. The data frame includes
effect estimates, confidence intervals, E-values, and other relevant statistics formatted
for easy reporting.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Assuming you have results from a simulation or model in `results_df`
tabulated_results <- tab_engine_marginal(x = results_df,
                                         new_name = "Treatment Effect",
                                         delta = 1,
                                         sd = 0.2,
                                         type = "RD")  # Corrected 'scale' to 'type'

\end{ExampleCode}
\end{Examples}
\HeaderA{transform\_var\_name}{Transform a variable name into a human-readable label, preserving acronyms}{transform.Rul.var.Rul.name}
\keyword{internal}{transform\_var\_name}
%
\begin{Description}
This function applies explicit mappings, strips numeric-range suffixes,
removes time-prefixes and z-suffixes, replaces underscores, and converts
to title case while preserving NZ, SDO, and RWA acronyms.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
transform_var_name(
  var_name,
  label_mapping = NULL,
  remove_tx_prefix = TRUE,
  remove_z_suffix = TRUE,
  use_title_case = TRUE,
  remove_underscores = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{var\_name}] Character; the original variable name

\item[\code{label\_mapping}] Optional named list for explicit mappings

\item[\code{remove\_tx\_prefix}] Logical; remove leading 't0\_' etc.

\item[\code{remove\_z\_suffix}] Logical; remove trailing '\_z'

\item[\code{use\_title\_case}] Logical; convert to title case

\item[\code{remove\_underscores}] Logical; replace underscores with spaces
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character scalar of the transformed label, or NA if input missing
\end{Value}
\HeaderA{transition\_table}{Transition Table}{transition.Rul.table}
\keyword{internal}{transition\_table}
%
\begin{Description}
Generates a transition table that describes movements and stability between states
from one observation to the next. It formats the output as a markdown table, highlighting
the number of entities remaining in the same state (diagonal) and those transitioning
to different states (off-diagonal).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
transition_table(
  trans_df,
  state_names = NULL,
  wave_info = NULL,
  table_name = "transition_table"
)

transition_table(
  trans_df,
  state_names = NULL,
  wave_info = NULL,
  table_name = "transition_table"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{state\_names}] Optional; a vector of state names to replace the default state labels.
If NULL, states will be labeled as "State 1", "State 2", etc., based on the unique values
in `from` and `to` columns.

\item[\code{data}] A data frame with columns `from` and `to` indicating the initial and subsequent
states of entities, respectively, and a `Freq` column indicating the frequency of transitions.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with two elements: `explanation`, a character string explaining the table,
and `table`, a markdown-formatted table of transitions. The diagonal entries (in bold)
represent the count of entities that remained in their initial state, while the off-diagonal
entries show the count of transitions between different states.
\end{Value}
%
\begin{Section}{Functions}
\begin{itemize}

\item{} \code{transition\_table()}: helper to format a single transition data frame


\end{itemize}
\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
df <- read.table(header=TRUE, text="
id wave year_measured religion_believe_god
3 0 1 0
3 1 1 1
4 0 1 0
4 1 1 1
5 0 1 1
5 1 1 0")

transition_matrix <- create_transition_matrix(df, "religion_believe_god", "id")
# Assuming `transition_matrix` is a table with the transition counts between states
# First, convert `transition_matrix` to a dataframe suitable for `transition_table`
df_transition <- as.data.frame.matrix(transition_matrix)
df_transition$from <- rownames(df_transition)
long_df_transition <- tidyr::pivot_longer(df_transition, cols = -from, names_to = "to", values_to = "Freq")

transition_table_data <- transition_table(long_df_transition)
cat(transition_table_data$explanation)
cat("\n")not
print(transition_table_data$table)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{\Rpercent{}>\Rpercent{}}{Pipe operator}{.Rpcent.>.Rpcent.}
\keyword{internal}{\Rpercent{}>\Rpercent{}}
%
\begin{Description}
See \code{magrittr::\LinkA{\Rpercent{}>\Rpercent{}}{.Rpcent.>.Rpcent.}} for details.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lhs %>% rhs
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{lhs}] A value or the magrittr placeholder.

\item[\code{rhs}] A function call using the magrittr semantics.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
The result of calling `rhs(lhs)`.
\end{Value}
\printindex{}
\end{document}
